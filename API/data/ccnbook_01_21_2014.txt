You are about to embark on one of the most fascinating scientific journeys possible: inside your own brain We start this journey by understanding what individual neurons in your neocortex do with the roughly 10,000 synaptic input signals that they receive from other neurons. The neocortex is the most evolutionarily recent part of the brain, which is also most enlarged in humans, and is where most of your thinking takes place. The numbers of neurons and synapses between neurons in the neocortex are astounding: roughly 20 billion neurons, each of which is interconnected with roughly 10,000 others. That is several times more neurons than people on earth. And each neuron is far more social than we are as people --estimates of the size of stable human social networks are only around 150-200 people, compared to the 10,000 for neurons. Weve got a lot going on under the hood. At these scales, the influence of any one neuron on any other is relatively small. Well see that these small influences can be shaped in powerful ways through learning mechanisms, to achieve complex and powerful forms of information processing. And this information processing prowess does not require much complexity from the individual neurons themselves --fairly simple forms of information integration both accurately describe the response properties of actual neocortical neurons, and enable sophisticated information processing at the level of aggregate neural networks.After developing an understanding of these basic neural information processing mechanisms in Part I of this book, we continue our journey in Part II by exploring many different aspects of human thought, including perception and attention, motor control and reinforcement learning, learning and memory, language, and executive function. Amazingly, all these seemingly different cognitive functions can be understood using a small set of common neural mechanisms. In effect, our neocortex is a fantastic form of silly putty, which can be molded by the learning process to take on many different cognitive tasks. For example, we will find striking similarities across different brain areas and cognitive functions --the development of primary visual cortex turns out to tell us a lot about the development of rich semantic knowledge of word meaningsHere is a list of some of the cognitive neuroscience phenomena well explore in Part II of the book: Vision: We can effortlessly recognize countless people, places, and things. Why is this so hard for robots We will explore this issue in a network that views natural scenes  Attention: Wheres Waldo Well see in a model how two visual processing pathways work together to help focus our attention in different locations in space, and why damage to one of these pathways leads people to ignore half of space. Dopamine and Reward: Why do we get bored with things so quickly Because our dopamine system is constantly adapting to everything we know, and only gives us rewards when something new or different occurs.Well see how this all happens through interacting brain systems that drive phasic dopamine release. Episodic memory: How can damage to a small part of our brain cause amnesia Well see how in a model that replicates the structure of the hippocampus. This model provides insight into why the rest of the brain isnt well-suited to take on the job of forming new episodic memories. Reading: What causes dyslexia, and why do people who have it vary so much in their struggles with readingWell explore these issues in a network that learns to read and pronounce nearly 3,000 English words, and generalizes to novel nonwords just like people do. Well see why damaging the network in different ways simulates various forms of dyslexia. Meaning: "A rose is a rose is a rose."  Task directed behavior: How do we stay focused on tasks that we need to get done or things that we need to pay attention to, in the face of an ever-growing number of distractions Well explore this issue through a network that simulates the "executive" part of the brain, the prefrontal cortex. We will see how this area is uniquely-suited to protect us from distraction, and how this can change with age.An important feature of our journey through the brain is that we use the vehicle of computer models to understand cognitive neuroscience. These computer models enrich the learning experience in important ways --we routinely hear from our students that they didnt really understand anything until they pulled up the computer model and played around with it for a few hours. Being able to manipulate and visualize the brain using a powerful 3D graphical interface brings abstract concepts to life, and enables many experiments to be conducted easily, cleanly, and safely in the comfort of your own laptop. This stuff is fun, like a video game --think "sim brain", as in the popular "sim city" game from a few years ago.At a more serious level, the use of computer models to understand how the brain works has been a critical contributor to scientific progress in this area over the past few decades. A key advantage of computer modeling is its ability to wrestle with complexity that often proves daunting to otherwise unaided human understanding. How could we possibly hope to understand how billions of neurons interacting with 10s of thousands of other neurons produce complex human cognition, just by talking in vague verbal terms, or simple paper diagrams Certainly, nobody questions the need to use computer models in climate modeling, to make accurate predictions and understand how the many complex factors interact with each other. The situation is only more dire in cognitive neuroscience.Nevertheless, in all fields where computer models are used, there is a fundamental distrust of the models. They are themselves complex, created by people, and have no necessary relationship to the real system in question. How do we know these models arent just completely made up fantasies The answer seems simple: the models must be constrained by data at as many levels as possible, and they must generate predictions that can then be tested empirically. In what follows, we discuss different approaches that people might take to this challenge --this is intended to give a sense of the scientific approach behind the work described in this book --as a student this is perhaps not so relevant, but it might help give some perspective on how science really works.In an ideal world, one might imagine that the neurons in the neural model would be mirror images of those in the actual brain, replicating as much detail as is possible given the technical limitations for obtaining the necessary details. They would be connected exactly as they are in the real brain. And they would produce detailed behaviors that replicate exactly how the organism in question behaves across a wide range of different situations. Then you would feel confident that your model is sufficiently "real" to trust some of its predictions.But even if this were technically feasible, you might wonder whether the resulting system would be any more comprehensible than the brain itself In other words, we would only have succeeded in transporting the fundamental mysteries from the brain into our model, without developing any actual understanding about how the thing really works. From this perspective, the most important thing is to develop the simplest possible model that captures the most possible data --this is basically the principle of Ockhams razor, which is widely regarded as a central principle for all scientific theorizing.In some cases, it is easy to apply this razor to cut away unnecessary detail. Certainly many biological properties of neurons are irrelevant for their core information processing function. But often it comes down to a judgment call about what phenomena you regard as being important, which will vary depending on the scientific questions being addressed with the model. The approach taken for the models in this book is to find some kind of happy middle ground between biological detail and cognitive functionality. This middle ground is unhappy to the extent that researchers concerned with either end of this continuum are dissatisfied with the level of the models. Biologists will worry that our neurons and networks are overly simplified. Cognitive psychologists will be concerned that our models are too biologically detailed, and they can make much simpler models that capture the same cognitive phenomena. We who relish this "golden middle" ground are happy when weve achieved important simplifications on the neural side, while still capturing important cognitive phenomena. This level of modeling explores how consideration of neural mechanisms inform the workings of the mind, and reciprocally, how cognitive and computational constraints afford a richer understanding of the problems these mechanisms evolved to solve. It can thus make predictions for how a cognitive phenomenon is affected by changes at the neural level. The model can then be tested, falsified and refined. In this sense, a model of cognitive neuroscience is just like any other theory, except that it is explicitly specified and formalized, forcing the modeler to be accountable for their theory ifwhen the data dont match up.Conversely, models can sometimes show that when an existing theory is faced with challenging data, the theory may hold up after all due to a particular dynamic that may not be considered from verbal theorizing.Ultimately, it comes down to aesthetic or personality-driven factors, which cause different people to prefer different overall strategies to computer modeling. Each of these different approaches has value, and science would not progress without them, so it is fortunate that people vary in their personalties so different people end up doing different things. Some people value simplicity, elegance, and cleanliness most highly --these people will tend to favor abstract mathematical cognitive models. Other people value biological detail above all else, and dont feel very comfortable straying beyond the most firmly established facts --they will prefer to make highly elaborated individual neuron models incorporating everything that is known. To live in the middle, you need to be willing to take some risks, and value most highly the process of emergence, where complex phenomena can be shown to emerge from simpler underlying mechanisms. The criteria for success here are a bit murkier and subjective --basically it boils down to whether the model is sufficiently simple to be comprehensible, but not so simple as to make its behavior trivial or otherwise so fully transparent that it doesnt seem to be doing you any good in the first place. One last note on this issue is that the different levels of models are not mutually exclusive. Each of the low level biophysical and high level cognitive models have made enormous contributions to understanding and analysis in their respective domains. In fact, much ground can be gained by attempts to understand one level of modeling in terms of the other. At the end of the day, linking from molecule to mind spans multiple levels of analysis, and like studying the laws of particle physics to planetary motion, require multiple formal tools.Emergent Phenomena What makes something a satisfying scientific explanation A satisfying answer is that you can explain a seemingly complex phenomenon in terms of simpler underlying mechanisms, that interact in specific ways. The classic scientific process of reductionism plays a critical role here, where the complex system is reduced to simpler parts. However, one also needs to go in the opposite, oft-neglected direction, reconstructionism, where the complex system is actually reconstructed from these simpler parts. Often the only way to practically achieve this reconstruction is through computational modeling. The result is an attempt to capture the essence of emergence.Emergence can be illustrated in a very simple physical system, two interacting gears, as shown in Taking this example into the domain of interest here, does this mean that we can switch out our biological neurons for artificial ones, and everything should still function the same, as long as we capture the essential interactions in the right way Some of us believe this to be the case, and that when we finally manage to put enough neurons in the right configuration into a big computer simulation, the resulting brain will support consciousness and everything else, just like the ones in our own heads. One interesting further question arises: how important are all the interactions between our physical bodies and the physical environment There is good reason to believe that this is critical. Thus, well have to put this brain in a robot. Or perhaps more challengingly, in a virtual environment in a virtual reality, still stuck inside the computer. It will be fascinating to ponder this question on your journey through the simulated brain...One of the things youll discover on this journey is that Computational Cognitive Neuroscience is hard. There is a lot of material at multiple levels to master. We get into details of ion channels in neurons, names of pathways in different parts of the brain, effects of lesions to different brain areas, and patterns of neural activity, on top of all the details about behavioral paradigms and reaction time patterns. Wouldnt it just be a lot simpler if we could ignore all these brain details, and just focus on what we really care about --how does cognition itself work By way of analogy, we dont need to know much of anything about how computer hardware works to program in Visual Basic or Python, for example. Vastly different kinds of hardware can all run the same programming languages and software. Cant we just focus on the software of the mind and ignore the hardwareExactly this argument has been promulgated in many different forms over the years, and indeed has a bit of a resurgence recently in the form of abstract Bayesian models of cognition. David Marr was perhaps the most influential in arguing that one can somewhat independently examine cognition at three different levels: Computational --what computations are being performed What information is being processed Algorithmic --how are these computations being performed, in terms of a sequence of information processing steps Implementational --how does the hardware actually implement these algorithmsThis way of dividing up the problem has been used to argue that one can safely ignore the implementation, and focus on the computational and algorithmic levels, because, like in a computer, the hardware really doesnt matter so much.However, the key oversight of this approach is that the reason hardware doesnt matter in standard computers is that they are all specifically designed to be functionally equivalent in the first place Sure, there are lots of different details, but they are all implementing a basic serial Von Neumann architecture. What if the brain has a vastly different architecture, which makes some algorithms and computations work extremely efficiently, while it cannot even support others Then the implementational level would matter a great deal.There is every reason to believe that this is the case. The brain is not at all like a general purpose computational device. Instead, it is really a custom piece of hardware that implements a very specific set of computations in massive parallelism across its 20 billion neurons. In this respect, it is much more like the specialized graphics processing units in modern computers, which are custom designed to efficiently carry out in massive parallelism the specific computations necessary to render complex 3D graphics. More generally, the field of computer science is discovering that parallel computation is exceptionally difficult to program, and one has to completely rethink the algorithms and computations to obtain efficient parallel computation. Thus, the hardware of the brain matters a huge amount, and provides many important clues as to what kind of algorithms and computations are being performed.Historically, the "ignore the brain" approaches have taken an interesting trajectory. In the 1960s through the early 1990s, the dominant approach was to assume that the brain actually operates much like a standard computer, and researchers tended to use concepts like logic and symbolic propositions in their cognitive models. Since then, a more statistical metaphor has become popular, with the Bayesian probabilistic framework being widely used in particular.This is an advance in many respects, as it emphasizes the graded nature of information processing in the brain, as contrasted with hard symbols and logic, which didnt seem to be a particularly good fit with the way that most of cognition actually operates. However, the actual mathematics of Bayesian probability computations are not a particularly good fit to how the brain operates at the neural level, and much of this research operates without much consideration for how the brain actually functions. Instead, a version of Marrs computational level is adopted, by assuming that whatever the brain is doing, it must be at least close to optimal, and Bayesian models can often tell us how to optimally combine uncertain pieces of information. Regardless of the validity of this optimality assumption, it is definitely useful to know what the optimal computations are for given problems, so this approach certainly has a lot of value in general. However, optimality is typically conditional on a number of assumptions, and it is often difficult to decide among these different assumptions.If you really want to know for sure how the brain is actually producing cognition, clearly you need to know how the brain actually functions. Yes, this is hard. But it is not impossible, and the state of neuroscience these days is such that there is a wealth of useful information to inform all manner of insights into how the brain actually works. It is like working on a jigsaw puzzle --the easiest puzzles are full of distinctive textures and junk everywhere, so you can really see when the pieces fit together. The rich tableau of neuroscience data provides all this distinctive junk to constrain the process of puzzling together cognition. In contrast, abstract, purely cognitive models are like a jigsaw puzzle with only a big featureless blue sky. You only have the logical constraints of the piece shapes, which are all highly similar and difficult to discriminate. It takes forever.A couple of the most satisfying instances of all the pieces coming together to complete a puzzle include: The detailed biology of the hippocampus, including high levels of inhibition and broad diffuse connectivity, fit together with its unique role in rapidly learning new episodic information, and the remarkable data from patient HM who had his hippocampus resected to prevent intractable epilepsy. Through computational models in the Memory Chapter, we can see that these biological details produce high levels of pattern separation which keep memories highly distinct, and thus enable rapid learning without creating catastrophic levels of interference. The detailed biology of the connections between dopamine, basal ganglia, and prefrontal cortex fit together with the computational requirements for making decisions based on prior reward history, and learning what information is important to hold on to, versus what can be ignored. Computational models in the Executive Function Chapter show that the dopamine system can exhibit a kind of time travel needed to translate later utility into an earlier decision of what information to maintain, and those in the Motor Chapter show that the effects of dopamine on the basal ganglia circuitry are just right to facilitate decision making based on both positive and negative outcomes. And the interaction between the basal ganglia and the prefrontal cortex enables basal ganglia decisions to influence what is maintained and acted upon in the prefrontal cortex. There are a lot of pieces here, but the fact that they all fit together so well into a functional model --and that many aspects of them have withstood the test of direct experimentation --makes it that much more likely that this is really what is going on.This book is intended to accommodate many different levels of background and interests. The main chapters are relatively short, and provide a high-level introduction to the major themes. There will be an increasing number of detailed subsections added over time, to support more advanced treatment of specific issues. The ability to support these multiple levels of readers is a major advantage of the wiki format. We also encourage usage of this material as an adjunct for other courses on related topics. The simulation models can be used by themselves in many different courses.Due to the complexity and interconnected nature of the material, it may be useful to revisit earlier chapters after having read later chapters. Also, we strongly recommend reading the Brain Areas chapter now, and then re-reading it in its regular sequence after having made it all the way through Part I. It provides a nice high-level summary of functional brain organization, that bridges the two parts of the book, and gives an overall roadmap of the content well be covering. Some of it wont make as much sense until after youve read Part I, but doing a quick first read now will provide a lot of useful perspective. Gary Cottrells solicited compilation of important computational modeling papers Figure 2.1: Trace of a simulated neuron spiking action potentials in response to an excitatory input --the yellow vm membrane potential increases until it reaches threshold, at which point a green act activation spike is triggered, which then resets the membrane potential back to its starting value and the process continues. The spike is communicated other neurons, and the overall rate of spiking is proportional to the level of excitatory net input. You can produce this graph and manipulate all the relevant parameters in the Neuron exploration for this chapter.One major reason the brain can be so plastic and learn to do so many different things, is that it is made up of a highly-sculptable form of silly putty: What is that function Fundamentally, it is about detection. Neurons receive thousands of different input signals from other neurons, looking for specific patterns that are "meaningful" to them. A very simple analogy is with a smoke detector, which samples the air and looks for telltale traces of smoke. When these exceed a specified threshold limit, the alarm goes off. Similarly, the neuron has a threshold and only sends an "alarm" signal to other neurons when it detects something significant enough to cross this threshold. The alarm is called an action potential or spike and it is the fundamental unit of communication between neurons.Our goal in this chapter is to understand how the neuron receives input signals from other neurons, integrates them into an overall signal strength that is compared against the threshold, and communicates the result to other neurons.We will see how these processes can be characterized mathematically in computer simulations This chapter and the Learning Mechanisms Chapter are the only two in the entire book with significant amounts of math. We have separated the conceptual from the mathematical content, and those with an aversion to math can get by without understanding all the details. So, dont be put off or overwhelmed by the math here Basic Biology of a Neuron as Detector  This is all you need to know about the neuron biology to understand the basic detector functionality: It just receives inputs, integrates them, and decides whether the integrated input is sufficiently strong to trigger an output signal.There are some additional biological properties regarding the nature of the input signals, which well see have various implications for neural function, including making the integration process better able to deal with large changes in overall input signal strength. There are at least three major sources of input signals to the neuron: Excitatory inputs --these are the "normal", most prevalent type of input from other neurons, which have the effect of exciting the receiving neuron. They are conveyed via a synaptic channel called AMPA, which is opened by the neurotransmitter glutamate. Inhibitory inputs --these are the other 15% of inputs, which have the opposite effect to the excitatory inputs --they cause the neuron to be less likely to fire, and serve to make the integration process much more robust by keeping the excitation in check. There are specialized neurons in the brain called inhibitory interneurons that generate this inhibitory input. This input comes in via GABA synaptic channels, driven by the neurotransmitter GABA. Leak inputs --these arent technically inputs, as they are always present and active, but they serve a similar function to the inhibitory inputs, by counteracting the excitation and keeping the neuron in balance overall.Biologically, leak channels are potassium channels.The inhibitory and excitatory inputs come from different neurons in the cortex: a given neuron can only send either excitatory or inhibitory outputs to other neurons, not both. We will see the multiple implications of this constraint throughout the text.Finally, we introduce the notion of the net synaptic efficacy or weight, which represents the total impact that a sending neuron activity signal can have on the receiving neuron, via its synaptic connection. The synaptic weight is one of the most important concepts in the entire field of computational cognitive neuroscience We will be exploring it in many different ways as we go along. Biologically, it represents the net ability of the sending neurons action potential to release neurotransmitter, and the ability of that neurotransmitter to open synaptic channels on the postsynaptic side. For the excitatory inputs, it is thus the amount of glutamate released by the sending neuron into the synapse, and the number and efficacy of AMPA channels on the receiving neurons side of the synapse. Computationally, the weights determine what a neuron is detecting. A strong weight value indicates that the neuron is very sensitive to that particular input neuron, while a low weight means that that input is relatively unimportant. The entire process of Learning amounts to changing these synaptic weights as a function of neural activity patterns in the sending and receiving neurons. InTo learn more about the biology of the neuron, see NeuronBiology. The integration process can be understood in terms of a tug-of-war To see how this works, lets just consider excitation versus inhibition. The key point is that the integration process reflects the relative strength of excitation versus inhibition --if excitation is stronger than inhibition, then the neurons electrical potential increases, perhaps to the point of getting over threshold and firing an output action potential. If inhibition is stronger, then the neurons electrical potential decreases, and thus moves further away from getting over the threshold for firing.Before we consider specific cases, lets introduce some obscure terminology that neuroscientists use to label the various actors in our tug-of-war drama  --the inhibitory conductance --this is the total strength of the inhibitory input, and plays a major role in determining how strong of an inhibitory current there is. This corresponds biologically to the proportion of inhibitory ion channels that are currently open and allowing inhibitory ions to flow. For electricity buffs, the conductance is the inverse of resistance --most people find conductance more intuitive than resistance, so well stick with it. --the inhibitory driving potential --in the tug-of-war metaphor, this just amounts to where the inhibitory guy happens to be standing relative to the electrical potential scale that operates within the neuron. Typically, this value is around -75mV where mV stands for millivolts --one thousandth of a volt. These are very small electrical potentials for very small neurons. --the action potential threshold --this is the electrical potential at which the neuron will fire an action potential output to signal other neurons. This is typically around -50mV. This is also called the firing threshold or the spiking threshold, because neurons are described as "firing a spike" when they get over this threshold. --the membrane potential of the neuron. This is the current electrical potential of the neuron relative to the extracellular space outside the neuron. It is called the membrane potential because it is the cell membrane that separates the inside and outside of the neuron, and that is where the electrical potential really happens. An electrical potential or voltage is a relative comparison between the amount of electric charge in one location versus another. It is called a "potential" because when there is a difference, there is the potential to make stuff happen. For example, when there is a big potential difference between the charge in a cloud and that on the ground, it creates the potential for lightning. Just like water, differences in charge always flow "downhill" to try to balance things out. So if you have a lot of charge in one location, it will flow until everything is all level. The cell membrane is effectively a dam against this flow, enabling the charge inside the cell to be different from that outside the cell. The ion channels in this context are like little tunnels in the dam wall that allow things to flow in a controlled manner.And when things flow, the membrane potential changes In the tug-of-war metaphor, think of the membrane potential as the flag attached to the rope that marks where the balance of tugging is at the current moment. --the excitatory driving potential --this is where the excitatory guy is standing in the electrical potential space. --the excitatory conductance --this is the total strength of the excitatory input, reflecting the proportion of excitatory ion channels that are open.  In the next case The last case The membrane potential Vm is not communicated directly to other neurons --instead it is subjected to a threshold and only the strongest levels of excitation are then communicated, resulting in a much more efficient and compact encoding of information in the brain. In human terms, neurons are sensitive to "TMI" constraints, also known as "Gricean Maxims" wikipedia link Actual neurons in the Neocortex compute discrete spikes or action potentials, which are very brief and trigger the release of neurotransmitter that then drives the excitation or inhibition of the neurons they are sending to.After the spike, the membrane potential Vm is reset back to a low value, and it must then climb back up again to the level of the threshold before another spike can occur. This process results in different rates of spiking associated with different levels of excitation --it is clear from eletrophysiological recordings of neurons all over the neocortex that this spike rate information is highly informative about behaviorally and cognitively relevant information. There remains considerable debate about the degree to which more precise differences in spike timing contain additional useful information.In our computer models, we can simulate discrete spiking behavior directly in a very straightforward way. However, we often use a rate code approximation instead, where the activation output of the neuron is a real valued number between 0-1 that corresponds to the overall rate of neural spiking. We typically think of this rate code as reflecting the net output of a small population of roughly 100 neurons that all respond to similar information --the neocortex is organized anatomically with microcolumns of roughly this number of neurons, where all of the neurons do indeed code for similar information. Use of this rate code activation enables smaller-scale models that converge on a stable interpretation of the input patterns rapidly, with an overall savings in computational time and model complexity. Nevertheless, there are tradeoffs in using these approximations, which we will discuss more in the Networks and other chapters. Getting the rate code to produce a good approximation to discrete spiking behavior has been somewhat challenging in the Leabra framework, and only recently has a truly satisfactory solution been developed, which is now the standard in the emergent software.Now youve got an intuitive understanding of how the neuron integrates excitation and inhibition. We can capture this dynamic in a set of mathematical equations that can be used to simulate neurons on the computer. The first set of equations focuses on the effects of inputs to a neuron. The second set focuses on generating outputs from the neuron.We will cover a fair amount of mathematical ground here. Dont worry if you dont follow all of the details. As long as you follow conceptually what the equations are doing, you should be able to build on this understanding when you get your hands on the actual equations themselves and explore how they behave with different inputs and parameters. You will see that despite all the math, the neurons behavior is indeed simple: the amount of excitatory input determines how excited it gets, in balance with the amount of inhibition and leak. And the resulting output signals behave pretty much as you would expect.We begin by formalizing the "strength" by which each side of the tug-of-war pulls, and then show how that causes the Vm "flag" to move as a result. This provides explicit equations for the tug-of-war dynamic integration process.Then, we show how to actually compute the conductance factors in this tug-of-war equation as a function of the inputs coming into the neuron, and the synaptic weights. Finally, we provide a summary equation for the tug-of-war which can tell you where the flag will end up in the end, to complement the dynamical equations which show you how it moves over time.The key idea behind these equations is that each guy in the tug-of-war pulls with a strength that is proportional to both its overall strength, and how far the "flag" is away from its position. Imagine that the tuggers are planted in their position, and their arms are fully contracted when the Vm flag gets to their position, and they cant re-grip the rope, such that they cant pull any more at this point.To put this idea into an equation, we can write the "force" or current that the excitatory guy exerts as:The excitatory current is, and it is the product of the conductance times how far the membrane potential is away from the excitatory driving potential. If then the excitatory guy has "won" the tug of war, and it no longer pulls anymore, and the current goes to zero. Interestingly, this also means that the excitatory guy pulls the strongest when the Vm "flag" is furthest away from it --i.e., when the neuron is at its resting potential. Thus, it is easiest to excite a neuron when its well rested.The same basic equation can be written for the inhibition guy, and also separately for the leak guy:  inhibitory current:.Next, we can add together these three different currents to get the net current, which represents the net flow of charged ions across the neurons membrane:So what good is a net current Recall that electricity is like water, and it flows to even itself out. When water flows from a place where there is a lot of water to a place where there is less, the result is that there is less water in the first place and more in the second. The same thing happens with our currents: the flow of current changes the membrane potential inside the neuron:  update of membrane potential due to net current:.The above two equations are the essence of what we need to be able to simulate a neuron on a computer It tells us how the membrane potential changes as a function of the inhibitory, leak and excitatory inputs --given specific numbers for these input conductances, and a starting Vm value, we can then iteratively compute the new Vm value according to the above equations, and this will accurately reflect how a real neuron would respond to similar such inputs To summarize, heres a single version of the above equations that does everything: For those of you who noticed the issue with the minus sign above, or are curious how all of this relates to Ohms law and the process of diffusion, please see Electrophysiology of the Neuron. If youre happy enough with where weve come, feel free to move along to finding out how we compute these input conductances, and what we then do with the Vm value to drive the output signal of the neuron.The excitatory and inhibitory input conductances represent the total number of ion channels of each type that are currently open and thus allowing ions to flow. In real neurons, these conductances are typically measured in nanosiemens, which is siemens. Typically, neuroscientists divide these conductances into two components: --a constant value that determines the maximum conductance that would occur if every ion channelwere to be open. --a dynamically changing variable that indicates at the present moment, what fraction of the total number of ion channels are currently open.Thus, the total conductances of interest are written as:  excitatory conductance:  inhibitory conductance:.This separation of terms makes it easier to compute the conductance, because all we need to focus on is computing the proportion or fraction of open ion channels of each type. This can be done by computing the average number of ion channels open at each synaptic input to the neuron: where is the activity of a particular sending neuron indexed by the subscript i, is the synaptic weight strength that connects sending neuron i to the receiving neuron, and n is the total number of channels of that type across all synaptic inputs to the cell. As noted above, the synaptic weight determines what patterns the receiving neuron is sensitive to, and is what adapts with learning --this equation shows how it enters mathematically into computing the total amount of excitatory conductance.The above equation suggests that the neuron performs a very simple function to determine how much input it is getting: it just adds it all up from all of its different sources. Each input source contributes in proportion to how active the sender is, multiplied by how much the receiving neuron cares about that information --the synaptic weight value. We also refer to this average total input as the net input.The same equation holds for inhibitory input conductances, which are computed in terms of the activations of inhibitory sending neurons, times the inhibitory weight values.There are some further complexities about how we integrate inputs from different categories of input sources , which we deal with in the optional subsection: Net Input Detail. But overall, this aspect of the computation is relatively simple and we can now move on to the next step, of comparing the membrane potential to the threshold and generating some output.Before finishing up the final step in the detection process, we will need to use the concept of the equilibrium membrane potential, which is the value of Vm that the neuron will settle into and stay at, given a fixed set of excitatory and inhibitory input conductances. This equilibrium value is interesting because it tells us more clearly how the tug-of-war process inside the neuron actually balances out in the end. Also, we will see in the next section that it is useful mathematically.To compute the equilibrium membrane potential, we can use an important mathematical technique: set the change in membrane potential to 0, and then solve the equation for the value of Vm under this condition. In other words, if we want to find out what the equilibrium state is, we simply compute what the numbers need to be such that Vm is no longer changing.Here are the mathematical steps that do this:  iterative Vm update equation:  just the change part:  set it to zero:We show the math here: Equilibrium Membrane Potential Derivation.In words, this says that the excitatory drive contributes to the overall Vm as a function of the proportion of the excitatory conductance relative to the sum of all the conductances. And the same for each of the others. This is just what we expect from the tug-of-war picture: if we ignore gl, then the Vm "flag"is positioned as a function of the relative balance between and --if they are equal, then is .5, which means that the Vm flag is half-way between and . So, all this math just to rediscover what we knew already intuitively. But well see that this math will come in handy next.Here is a version with the conductance terms explicitly broken out into the "g-bar" constants and the time-varying "g" parts: For those who really like math, the equilibrium membrane potential equation can be shown to be a Bayesian OptimalDetector.The output of the neuron can be simulated at two different levels: discrete spiking, or using a rate code approximation. We cover each in turn, and show how the rate code must be derived to match the behavior of the discrete spiking neuron, when averaged over time.To compute discrete action potential spiking behavior from the neural equations we have so far, we need to determine when the membrane potential gets above the firing threshold, and then emit a spike, and subsequently reset the membrane potential back down to a value, from which it can then climb back up and trigger another spike again, etc. This is actually best expressed as a kind of simple computer program: if then: y  1 Vm  Vmr else y  0 where y is the activation output value of the neuron, and Vmr is the reset potential that the membrane potential is reset to after a spike is triggered. Biologically, there are special potassium channels that bring the membrane potential back down after a spike.This simplest of spiking models is not quite sufficient to account for the detailed spiking behavior of actual cortical neurons. However, a slightly more complex model can account for actual spiking data with great accuracy. This model is known as the Adaptive Exponential or AdEx model --click on the link to read more about it. We typically use this AdEx model when simulating discrete spiking, although the simpler model described above is also still an option. The critical feature of the AdEx model is that the effective firing threshold adapts over time, as a function of the excitation coming into the cell, and its recent firing history. The net result is a phenomenon called spike rate adaptation,where the rate of spiking tends to decrease over time for otherwise static input levels. Otherwise, however, the AdEx model is identical to the one described above.  Instead, it turns out that the excitatory net input enables a good prediction of actual spiking rate, when it is compared to an appropriate threshold value   equilibrium Vm at threshold:, Now, we can say that our rate coded output activation value will be some function of the difference between the excitatory net input ge and this threshold value: should look like.There are three important properties that this function should have: threshold --it should be 0 when ge is less than its threshold value. saturation --when ge gets very strong relative to the threshold, the neuron cannot actually keep firing at increasingly high rates --there is an upper limit to how fast it can spike. Thus, our rate code function also needs to exhibit this leveling-off or saturation at the high end. smoothness --there shouldnt be any abrupt transitions to the function, so that the neurons behavior is smooth and continuous. with an extra gain factor , which just multiplies everything: So the full equation is: Which can also be written as: As you can see in For completeness sake, and strictly for the mathematically inclined, here is the equation for the convolution operation: where y is the XX1 function applied to the z-x input instead of just x. In practice, a finite kernel of width on either side of x is used in the numerical convolution.After convolution, the XX1 function There is just one last problem with the equations as written above. They dont evolve over time in a graded fashion.In contrast, the Vm value does evolve in a graded fashion by virtue of being iteratively computed, where it incrementally approaches the equilibrium value over a number of time steps of updating. Instead the activation produced by the above equations goes directly to its equilibrium value very quickly, because it is calculated based on excitatory conductance and does not take into account the sluggishness with which changes in conductance lead to changes in membrane potentials. As discussed in the Introduction, graded processing is very important, and we can see this very directly in this case, because the above equations do not work very well in many cases because they lack this gradual evolution over time.To introduce graded iterative dynamics into the activation function, we just use the activation value from the above equation as a driving force to an iterative temporally-extended update equation:This causes the actual final rate code activation output at the current time t, y to iteratively approach the driving value given by , with the same time constant that is used in updating the membrane potential. In practice this works extremely well, better than any prior activation function used with Leabra.  1. Compute the excitatory input conductance: 2. Update the membrane potential one time step at a time, as a function of input conductances: 3a. For discrete spiking, compare membrane potential to threshold and trigger a spike and reset Vm if above threshold: then: y  1 Vm  Vmr else y  0 3b. For rate code approximation, compute output activation as NXX1 function of ge and Vm:To get your hands dirty, run Neuron. As this is the first exploration youll be running, you may need to consult the overall page for information on installing the Emergent software etc: CCNBookSimsAll.Now that you can see how the individual neuron integrates a given excitatory signal relative to leakinhibition, it is important to put this into the larger perspective of the detection process. In this simulation, youll see how a neuron can pick out a particular input pattern from a set of inputs, and also how it can have different patterns of responding depending on its parameters.To run this exploration, go to Detector.Here are all the sub-topics within the Neuron chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: Neuron Biology --more detailed description of neuron biology. Neuron Electrophysiology --more detailed description of the electrophysiology of the neuron, and how the underlying concentration gradients of ions give rise to the electrical integration properties of the neuron. Net Input Detail --details on how net inputs are computed across multiple different input projections. Adaptive Exponential Spiking Model --the AdEx model has won multiple competitions for best fitting actual cortical neuron firing patterns, and is what we actually use in spiking mode output. Temporal Dynamics --longer time-scale temporal dynamics of neurons. Sigmoidal Unit Activation Function --a more abstract formalism for simulating the behavior of neurons, used in more abstract neural network models. Bayesian Optimal Detector --how the equilibrium membrane potential represents a Bayesian optimal way of integrating the different inputs to the neuron.Here are all the explorations covered in the main portion of the Neuron chapter: Neuron --Individual Neuron --spiking and rate code activation. Detector  NMDA, which is involved in learning and allows Calcium ions to flow --we will discuss these receptors in more detail in the Learning chapter. mGluR, which is also involved in learning and also possibly active Importantly, the biology shows that synapses in the cortex can either be excitatory or inhibitory, but not both. This has implications for our computational models as we explore in the Networks chapter.Back to CCN Book Main Page  Neuron ChapterThis optional section provides a full treatment of the electrophysiology of the neuron --how differential concentrations of individual ions lead to the electrical dynamics of the neuron.First, some basic facts of electricity. Electrons and protons, which together make up atoms, have electrical charge. An ion is an atom where these positive and negative charges are out of balance, so that it carries a net charge. Because the brain carries its own salt-water ocean around with it, the primary ions of interest are: sodium which has a net positive charge. chloride which has a net negative charge. potassium which has a net positive charge. calcium which has two net positive charges. Ohms law formalizes the situation mathematically:. represents. The key difference between the diffusion and electrical force is:  Diffusion operates individually on each ion, regardless of its charge compared to other ions etc --each ion is driven by the diffusion force to spread itself uniformly around. In contrast, electrical forces ignore the identity of the ion, and only care about the net electrical charge. From electricitys perspective, Na and K are effectively equivalent.It is this critical difference between diffusion and electrical forces that causes different ions to have different driving potentials, and thus exert different influences over the neuron.. These relative concentration differences give rise to the different driving potentials for different ions, and thus determine their net effect on the neuron. Similarly, the K ions are drawn into the cell by the extra negative charge within, creating an opposite concentration imbalance for the potassium ions.All of these concentration imbalances create a strong diffusion force, where these ions are trying to distribute themselves more uniformly. But this diffusion force is counteracted by the electrical force, and when the neuron is at rest, it achieves an equilibrium state where the electrical and diffusion forces exactly balance and cancel each other out. Another name for the diving potential for an ion is the equilibrium potential --the electrical potential at which the diffusion and electrical forces exactly balance.As shown in Mathematically, we can capture this phenomenon using the same equation we derived from the tug-of-war analogy: Notice that this is just a simple modification of Ohms law --the E value "corrects" Ohms law to take into account any concentration imbalances and the diffusion forces that they engender. If there are no concentration imbalances, then E  0, and you get Ohms law.If we plug an E value of -70mV into this equation, then we see that the current is 0 when V  -70mV. This is the definition of an equilibrium state. No net current.Now consider the Na ion. Both the negative potential inside the neuron, and the concentration imbalance, drive this ion to want to move into the cell. Thus, at the resting potential of -70mV, the current for this ion will be quite high if it is allowed to flow into the cell. Indeed, it will not stop coming into the cell until the membrane potential gets all the way up to 55mV or so. This equilibrium or driving potential for Na is positive, because it would take a significant positive potential to force the Na ions back out against their concentration difference.The bottom line of all this is that synaptic channels that allow Na ions to flow will cause Na to flow into the neuron, and thereby excite the receiving neuron. In effect, the sodium pump "winds up" the neuron by creating these concentration imbalances, and thus the potential for excitation to come into the cell against a default background of the negative resting potential.Finally, when excitatory inputs do cause the membrane potential to increase, this has the effect of drawing more Clions back into the cell, creating an inhibitory pull back to the -70mV resting value, and similarly it pushes K ions out of the cell, which also makes the inside of the cell more negative, and has a net inhibitory effect. The Cl-ions only flow when inhibitory GABA channels are open, and the K ions flow all the time through the always-open leak channels.In this chapter, we build upon the Neuron Chapter to understand how networks of detectors can produce emergent behavior that is more than the sum of their simple neural constituents. We focus on the networks of the neocortex, which is the evolutionarily most recent, outer portion of the brain where most of advanced cognitive functions take place. There are three major categories of emergent network phenomena: Categorization of diverse patterns of activity into relevant groups: For example, faces can look very different from one another in terms of their raw "pixel" inputs, but we can categorize these diverse inputs in many different ways, to treat some patterns as more similar than others: male vs. female, young vs. old, happy vs. sad, "my mother" vs. "someone other", etc. Forming these categories is essential for enabling us to make the appropriate behavioral and cognitive responses. Imagine trying to relate all the raw inputs of a visual image of a face to appropriate behavioral responses, without the benefit of such categories.The relationship between pixels and responses is just too complex. These intermediate, abstractcategories organize and simplify cognition, just like file folders organize and simplify documents on your computer. One can argue that much of intelligence amounts to developing and using these abstract categories in the right ways. Biologically, well see how successive layers of neural detectors, organized into a hierarchy, enable this kind of increasingly abstract categorization of the world. We will also see that many individual neural detectors at each stage of processing can work together to capture the subtlety and complexity necessary to encode complex conceptual categories, in the form of a distributed representation. These distributed representations are also critical for enabling multiple different ways of categorizing an input to be active at the same time --e.g., a given face can be simultaneously recognized as female, old, and happy. A great deal of the emergent intelligence of the human brain arises from multiple successive levels of cascading distributed representations, constituting the collective actions of billions of excitatory pyramidal neurons working together in the cortex. Bidirectional excitatory dynamics are produced by the pervasive bidirectional connectivity in the neocortex. The ability of information to flow in all directions throughout the brain is critical for understanding phenomena like our ability to focus on the task at hand and not get distracted by irrelevant incoming stimuli, and our ability to resolve ambiguity in inputs by bringing higher-level knowledge to bear on lower-level processing stages. For example, if you are trying to search for a companion in a big crowd of people, you can maintain an image of what you are looking for, which helps to boost the relevant processing in lower-level stages. The overall effects of bidirectional connectivity can be summarized in terms of an attractor dynamic or multiple constraint satisfaction, where the network can start off in a variety of different states of activity, and end up getting "sucked into" a common attractor state, representing a cleaned-up, stable interpretation of a noisy or ambiguous input pattern. Probably the best subjective experience of this attractor dynamic is when viewing an Autostereogram  Inhibitory competition, mediated by specialized inhibitory interneurons is important for providing dynamic regulation of overall network activity, which is especially important when there are positive feedback loops between neurons as in the case of bidirectional connectivity. The existence of epilepsy in the human neocortex indicates that achieving the right balance between inhibition and excitation is difficult --the brain obtains so many benefits from this bidirectional excitation that it apparently lives right on the edge of controlling it with inhibition. Inhibition gives rise to sparse distributed representations, which have numerous advantages over distributed representations that have many neurons active at a time. In addition, well see in the Learning Chapter that inhibition plays a key role in the learning process, analogous to the Darwinian "survival of the fittest" dynamic, as a result of the competitive dynamic produced by inhibition.We begin with a brief overview of the biology of neural networks in the neocortex.Biology of the Neocortex is clear that learning takes place in the synapses between these excitatory neurons. The inhibitory neurons can be understood as "cooling off" the excitatory heat generated by the pyramidal neurons, much like the cooling system in a car engine. Without these inhibitory interneurons, the system would overheat with excitation and lock up in epileptic seizures. There are, however, areas outside of the cortex where important information processing does take place via inhibitory neurons, and certainly some researchers will object to this stark division of labor even within cortex, but it is nevertheless a very useful simplification.Layered Structure    superficial layers 23, which contain many pyramidal neurons that are well positioned for performing this critical categorization function. responses. We will adopt this same basic structure for most of the models we explore. Hidden connections between areas, and Output layers provide an "external interface" to communicate these Hidden representations more broadly. The exception to this general idea would be in the motor output areas of cortex, where the Output layers may be doing something more independent.Each cortical area also has extensive lateral connectivity among neurons within the same area, and this follows the same general pattern as the feedback projection, except that it also terminates in layer 4. These lateral connections serve a very similar functional role as the feedback projections as well --essentially they represent "self feedback". The other significant aspect of cortical connectivity that will become quite important for our models, is that the connectivity is largely bidirectional  really exist in the real world, or is it just something that our brains construct for us to enable us to get by This issue has been contemplated since the dawn of philosophy, e.g., by Plato with his notion that we live in a cave perceiving only shadows on the wall of the true reality beyond the cave. It seems plausible that there is something "objective" about chairs that enables us to categorize them as such, but providing a rigorous, exact definition thereof seems to be a remarkably challenging endeavor. It doesnt seem like most of our concepts are likely to be true "natural kinds" that have a very precise basis in nature. Things like Newtons laws of physics, which would seem to have a strong objective basis, are probably dwarfed by everyday things like chairs that are not nearly so well defined.The messy ontological status of conceptual categories doesnt bother us very much. As we saw in the previous chapter, Neurons are very capable detectors that can integrate many thousands of different input signals, and can thereby deal with complex and amorphous categories. Furthermore, we will see that learning can shape these category representations to pick up on things that are behaviorally relevant, without requiring any formality or rigor in defining what these things might be. In short, our mental categories develop because they are useful to us in some way or another, and the outside world produces enough reliable signals for our detectors to pick up on these things.Importantly, a major driver for learning these categories is social and linguistic interaction, which enables very complex and obscure things to be learned and shared --the strangest things can be learned through social interactions. Thus, our cultural milieu plays a critical role in shaping our mental representations, and is clearly a major force in what enables us to be as intelligent as we are. If you want to dive deeper into the philosophical issues of truth and relativism that arise from this lax perspective on mental categories, see Philosophy of Categories.One intuitive way of understanding the importance of having the right categories comes from insight problems. These problems are often designed so that our normal default way of categorizing the situation leads us in the wrong direction, and it is necessary to re-represent the problem in a new way, to solve it. For example, consider this "conundrum" problem: "two men are dead in a cabin in the woods. what happened" --you then proceed to ask a bunch of truefalse questions and eventually realize that you need to select a different way of categorizing the word "cabin" in order to solve the puzzle. Here is a list of some of these kinds of conundrums: http:  www. angelfire. com oh abnorm .For computer programmers, one of the most important lessons one learns is that choosing the correct representation is the most important step in solving a given problem. As a simple example, using the notion of a "heap" enables a particularly elegant solution to the sorting problem. Binary trees are also a widely used form of representation that often greatly reduce the computational time of various problems. In general, you simply want to find a representation that makes it easy to do the things you need to do. This is exactly what the brain does.One prevalent example of the brains propensity to develop categorical encodings of things are stereotypes. A stereotype is really just a mental category applied to a group of people. The fact that everyone seems to have them is strong evidence that this is fundamentally how the brain works. We cannot help but think in terms of abstract categories like this, and as weve argued above, categories in general are essential for allowing us to deal with the world in an intelligent manner. But the obvious problems with stereotypical thinking also indicate that these categories can also be problematic, and limit our ability to accurately represent the details of any given individual or situation. As we discuss next, having many different categorical representations active at the same time can potentially help mitigate these problems. The ability to entertain multiple such potential categories at the same time may be an individual difference variable associated with things like political and religious beliefs. This stuff can get interesting Distributed Representations proportion to how similar they are to the thing that it responds most actively to. With such graded responses ubiquitous in cortex, it follows that any given input will activate many different neuron detectors. Reproduced from In addition to our mental categories being somewhat amorphous, they are also highly Chairs have seating surfaces, and sometimes have a backrest, and typically have a chair-like shape, but their shapes can also be highly variable and strange. They are often made of wood or plastic or metal, but can also be made of cardboard or even glass. All of these different factors can be captured by the whole population of neurons firing away to encode these and many other features.The same goes for the polymorphous nature of categories. One set of neurons may be detecting chair-like Some real-world data on distributed representations is shown in Another demonstration of distributed representations comes from a landmark study by Haxby and colleagues, using functional magnetic resonance imaging of the human brain, while viewing different visual stimuli The opposite of a distributed representation is a localist representation, where a single neuron is active to encode a given category of information. Although we do not think that localist representations are characteristic of the actual brain, they are nevertheless quite convenient to use for computational models, especially for input and output patterns to present to a network. It is often quite difficult to construct a suitable distributed pattern of activity to realistically capture the similarities between different inputs, so we often resort to a localist input pattern with a single input neuron active for each different type of input, and just let the network develop its own distributed representations from there.  Clearly, when an image of Halle Berry is viewed, a huge number of neurons at all levels of the cortex will respond, so the overall representation is still highly distributed. But it does appear that, amongst all the different ways of categorizing such inputs, there are a few highly selective "grandmother" neurons One other outstanding question is the extent to which these neurons actually do show graded responses to other inputs --there is some indication of this in the figure, and more data would be required to really test this more extensively.See Face Categorization for an exploration of how face images can be categorized in different ways, each of which emphasizes some aspect of the input stimuli and collapses across others.The feedforward flow of excitation through multiple layers of the neocortex can make us intelligent, but the feedback flow of excitation in the opposite direction is what makes us robust, flexible, and adaptive. Without this feedback pathway, the system can only respond on the basis of whatever happens to drive the system most strongly in the feedforward, bottom-up flow of information. But often our first impression is wrong, or at least incomplete. In the "searching for a friend" example from the introduction, we might not get sufficiently detailed information from scanning the crowd to drive the appropriate representation of the person. Top-down activation flow can help focus us on relevant perceptual information that we can spot. As this information interacts with the bottom-up information coming in as we scan the crowd, our brains suddenly converge on the right answer: Theres my friend, in the red coat  There are many different instances where bidirectional excitatory dynamics are evident:. Top-down ambiguity resolution --Many stimuli are ambiguous without further top-down constraints. For example, if youve never seen  Pattern completion --If I ask you "what did you have for dinner last night", this partial input cue can partially excite the appropriate memory representation in your brain, but you need a bidirectional excitatory dynamic to enable this partial excitation to reverberate through the memory circuits and fill in the missing parts of the full memory trace. This reverberatory process is just like the coin orbiting around the gravity well --different neurons get activated and inhibited as the system "orbits" around the correct memory trace, eventually converging on the full correct memory trace. Sometimes, in so-called tip of the tongue states, the memory youre trying to retrieve is just beyond grasp, and the system cannot quite converge into its attractor state. Man, that can be frustrating Usually you try everything to get into that final attractor. We dont like to be in an unresolved state for very long.There is a mathematical way to capture something like the vertical axis in the attractor figure See Face Categorization for an exploration of how top-down and bottom-up processing interact to produce imagery and help resolve ambiguous inputs. These additional simulations provide further elaboration of bidirectional computation: Cats and Dogs --fun example of attractor dynamics in a simple semantic network. Necker Cube --another fun example of attractor dynamics, showing also the important role of noise, and neural fatigue.Inhibitory competition plays a critical role in enabling us to focus on a few things at a time, which we can then process effectively without getting overloaded. Inhibition also ensures that those detectors that do get activated are the ones that are the most excited by a given input --in Darwinian evolutionary terms, these are the fittest detectors.Without inhibition, the bidirectional excitatory connectivity in the cortex would quickly cause every neuron to become highly excited, because there would be nothing to check the spread of activation. There are so many excitatory connections among neurons that it doesnt take long for every neuron to become activated. A good analogy is placing a microphone near a speaker that is playing the sound from that microphone --this is a bidirectional excitatory system, and it quickly leads to that familiar, very loud "feedback" squeal. If ones audio system had the equivalent of the inhibitory system in the cortex, it would actually be able to prevent this feedback by dynamically turning down the input gain on the microphone, andor the output volume of the speaker.Another helpful analogy is to an air conditioner, which has a thermostat control that determines when it kicks in. This kind of feedback control system allows the room to warm up to a given set point before it starts to counter the heat. Similarly, inhibition in the cortex is proportional to the amount of excitation, and it produces a similar set point behavior, where activity is prevent from getting too high:typically no more than roughly 15-25% of neurons in any given area are active at a time.The importance of inhibition goes well beyond this basic regulatory function, however. Inhibition gives rise to competition --only the most strongly excited neurons are capable of overcoming the inhibitory feedback signal to get activated and send action potentials to other neurons. This competitive dynamic has numerous benefits in processing and learning. For example, selective attention depends critically on inhibitory competition. In the visual domain, selective attention is evident when searching for a stimulus in a crowded scene. You cannot process all of the people in the crowd at once, so only a relatively few capture your attention, while the rest are ignored. In neural terms, we say that the detectors for the attended few were sufficiently excited to out-compete all the others, which remain below the firing threshold due to the high levels of inhibition. Both bottom-up and top-down factors can contribute to which neural detectors get over threshold or not, but without inhibition, there wouldnt be any ability to select only a few to focus on in the first place. Interestingly, people with Balints syndrome, who have bilateral damage to the parietal cortex, show reduced attentional effects and also are typically unable to process anything if a visual display contains more than one item. We will explore these phenomena in the PerceptionChapter.We will see in the Learning Chapter that inhibitory competition facilitates learning by providing this selection pressure, whereby only the most excited detectors get activated, which then gets reinforced through the learning process to make the most active detectors even better tuned for the current inputs, and thus more likely to respond to them again in the future. This kind of positive feedback loop over episodes of learning leads to the development of very good detectors for the kinds of things that tend to arise in the environment. Without the inhibitory competition, a large percentage of neurons would get trained up for each input, and there would be no specialization of detectors for specific categories in the environment. Every neuron would end up weakly detecting everything, and thus accomplish nothing. Thus, again we see that competition and limitations can actually be extremely beneficial.A summary term for the kinds of neural patterns of activity that develop in the presence of inhibitory competition is sparse distributed representations. These have relatively few neurons active at a time, and thus these neurons are more highly tuned for the current inputs than they would otherwise be in a fully distributed representation with much higher levels of overall activity. Thus, although technically inhibition does not contribute directly to the basic information processing functions like categorization, because inhibitory connectivity is strictly local within a given cortical area, inhibition does play a critical indirect role in shaping neural activity patterns at each level. Functionally, feedforward inhibition can anticipate how excited the excitatory neurons will become, whereas feedback accurately reflects the actual level of activation they achieve.Feedback inhibition is the most intuitive, so well start with it. Here, the inhibitory interneurons are driven by the same excitatory neurons that they then project back to and inhibit. This is the classical "feedback" circuit from the AC example. When a set of excitatory neurons starts to get active, they then communicate this activation to the inhibitory interneurons. This excitation of the inhibitory neurons then causes them to fire action potentials that come right back to the excitatory neurons, opening up their inhibitory ion channels via GABA release.The influx of Cl- ions from the inhibitory input channels on these excitatory neurons acts to drive them back down in the direction of the inhibitory driving potential. Thus, excitation begets inhibition which counteracts the excitation and keeps everything under control, just like a blast of cold air from the AC unit.Feedforward inhibition is perhaps a bit more subtle. It operates when the excitatory synaptic inputs to excitatory neurons in a given area also drive the inhibitory interneurons in that area, causing the interneurons to inhibit the excitatory neurons in proportion to the amount of excitatory input they are currently receiving. This would be like a thermostat reacting to the anticipated amount of heat, for example, by turning on the AC based on the outside temperature. Thus, the key difference between feedforward and feedback inhibition is that feedforward reflects the net excitatory input, whereas feedback reflects the actual activation output of a given set of excitatory neurons.As we will see in the exploration, the anticipatory function of feedforward inhibition is crucial for limiting the kinds of dramatic feedback oscillations that can develop in a purely feedback-driven system. However, too much feedforward inhibition makes the system very slow to respond, so there is an optimal balance of the two types that results in a very robust inhibitory dynamic. Inhibition --this simulation shows how feedforward and feedback inhibitory dynamics lead to the robust control of excitatory pyramidal neurons, even in the presence of bidirectional excitation.kWTA Approximation for Inhibition  For these reasons, we developed a simple set of approximations to the net effects of the inhibitory interneurons, known as the k-winners-take-all inhibitory functions. These equations directly compute a level of inhibitory conductance for all the neurons in a layer, with this value placed so as to keep k units above their firing threshold, with the remainder below threshold. Thus, these equations capture the set point nature of inhibitory feedback, analogous to setting a desired temperature for the AC unit --the kWTA function ensures that the "temperature" doesnt get too much above k.To see how the basic form of kWTA is computed, see The average-based version of the kWTA function introduces a bit more flexibility in the overall activity levels within a layer, by computing the inhibitory conductance for the layer in terms of the average inhibitory threshold values for the top k vs. remainder neurons. As shown in Typically, we use the average-based kWTA for hidden layers, where the extra flexibility is very useful in enabling the layer to develop appropriate representations for different inputs, which may require different numbers of overall active neurons. For output layers with a prescribed number of active neurons, the basic kWTA works well to ensure that this number of neurons are always active.Finally, we emphasize that although the actual computational mechanism for these kWTA functions is biologically implausible, they nevertheless provide a good approximation for the behavior of networks governed by inhibitory interneuron inhibition. Thus, the net result is the same, and the computational advantages of kWTA are critical for enabling models to be run quickly and efficiently. As with the use of the rate code approximation instead of discrete spiking, the best way to mitigate against the potential limitations of using the kWTA approximation is to compare models using each mechanism, and determine what if any systematic differences exist. More of this kind of exploration needs to be done, now that computer hardware is significantly faster and such models can be relatively easily run.Here are all the sub-topics within the Networks chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: Philosophy of Categories --philosophical issues about the truth value of mental categories. Energy and Harmony --mathematics of attractor dynamics in terms of Hopfield energy or Smolenskys Harmony. kWTA Equations --equations for the k-winners-take-all inhibition function.Here are all the explorations covered in the main portion of the Networks chapter: How do we learn to read, do math, and play sports Learning in a neural network amounts to the modification of synaptic weights, in response to the local activity patterns of the sending and receiving neurons. As emphasized in previous chapters, these synaptic weights are what determine what an individual neuron detects, and thus are the critical parameters for determining neuron and network behavior.In other words, everything you know is encoded in the patterns of your synaptic weights, and these have been shaped by every experience youve had. Many of those experiences dont leave a very strong mark, and in much of the brain, traces of individual experiences are all blended together, so it is difficult to remember them distinctly. But each experience nevertheless drives some level of learning, and our big challenge in this chapter is to figure out how the mere influences of patterns of activity among individual neurons can add up to enable us to learn big things., but the high-level story is fairly straightforward: the overall level of neural activity on both ends of the synapse drives the influx of calcium ions via NMDA channels, and synaptic weight changes are driven by the level of postsynaptic Ca in the dendritic spine associated with a given synapse. Low levels of Ca cause synapses to get weaker, and higher levels cause them to get stronger.Computationally, many different sets of equations have been developed that can drive synaptic weight changes to accomplish many different computational goals. Which of these correspond to what the biology is actually doingThat is the big question. While a definitive answer remains elusive, we nevertheless have a reasonable candidate that aligns well with the biological data, and also performs computationally very useful forms of learning, which can solve the most challenging of cognitive tasks.There are two primary types of learning: Self-organizing learning, which extracts longer time-scale statistics about the environment, and can thus be useful for developing an effective internal model of the outside world. Error-driven learning, which uses more rapid contrasts between expectations and outcomes to correct these expectations, and thus form more detailed, specific knowledge about contingencies in the world. For example, young children seem endlessly fascinated learning about what happens when they push stuff off their high chair trays: will it still fall to the ground and make a huge mess this time Once they develop a sufficiently accurate expectation about exactly what will happen, it starts to get a bit less interesting, and other more unpredictable things start to capture their interest. As we can see in this example, error-driven learning is likely intimately tied up with curiosity, surprise, and other such motivational factors. For this reason, we hypothesize that neuromodulators such as dopamine, norepinephrine and acetylcholine likely play an important role in modulating this form of learning, as they have been implicated in different versions of surprise, that is, when there is a discrepancy between expectations and outcomes.Interestingly, the main computational difference between these two forms of learning has to do with the time scale over which one of the critical variables is updated --self-organizing learning involves averaging over a long time scale, whereas error-driven learning is much quicker. This difference is emphasized in the above descriptions as well, and provides an important source of intuition about the differences between these types of learning.Self-organizing learning is what happens when you blur your eyes and just take stuff in over a period of time, whereas error-driven learning requires much more alert and rapid forms of neural activity. In the framework that we will use in the rest of the book, we combine these types of learning into a single set of learning equations, to explore how we come to perceive, remember, read, and plan.  1. The postsynaptic membrane potential must be elevated, as a result of all the excitatory synaptic inputs coming into the cell. The most important contributor to this Vm level is actually the backpropagating action potential --when a neuron fires an action potential, it not only goes forward out the axon, but also backward down the dendrites. Thus, the entire neuron gets to know when it fires --well see that this is incredibly useful computationally.The elevated Vm causes magnesium ions to be repelled out of the openings of NMDA channels, unblocking them.The presynaptic neuron fires an action potential, releasing glutamate neurotransmitter into the synaptic cleft.Glutamate binds to the NMDA receptor, opening it to allow Ca ions to flow into the postsynaptic cell. This only occurs if the NMDA is also unblocked. This dependence of NMDA on both pre and postsynaptic activity was one of the early important clues to the nature of learning, as we see later. Ca can also enter the postsynaptic cell via voltage gated calcium channelss which are calcium channels that only open when the membrane potential is elevated. Unlike NMDA, however, they are not sensitive to presynaptic neural activity --they only depend on postsynaptic activity. This has important computational implications, as we discuss later. VGCCs contribute less to Ca levels than NMDA, so NMDA is still the dominant player. --low levels drive LTD, while high levels produce LTP. This property will be critical for our computational model. Note that the delay in synaptic plasticity effects based on Ca levels means that the synapse doesnt always have to do LTD on its way up to LTP --there is time for the Ca to reach a high level to drive LTP before the weights start to change.The famous Canadian psychologist Donald O. Hebb predicted the nature of the NMDA channel many years in advance of its discovery, just by thinking about how learning should work at a functional level. Here is a key quote:This can be more concisely summarized as cells that fire together, wire together. The NMDA channel is essential for this process, because it requires both pre and postsynaptic activity to allow Ca to enter and drive learning. It can detect the coincidence of neural firing. Interestingly, Hebb is reputed to have said something to the effect of "big deal, I knew it had to be that way already" when someone told him that his learning principle had been discovered in the form of the NMDA receptor.Mathematically, we can summarize Hebbian learning as: where is the change in synaptic weight w, as a function of sending activity x and receiving activity y.Anytime you see this kind of pre-post product in a learning rule, it tends to be described as a form of Hebbian learning. For a more detailed treatment of Hebbian learning and various popular variants of it, see Hebbian Learning .As well elaborate below, this most basic form of Hebbian learning is very limited, because weights will only go up, and will do so without bound.Interestingly, Hebb himself only seemed to have contemplated LTP, not LTD, so perhaps this is fitting. But it wont do anything useful in a computational model. Before we get to the computational side of things, we cover one more important result in the biology.Spike Timing Dependent Plasticity  Nevertheless, the STDP data does provide a useful stringent test for computational models of synaptic plasticity. We base our learning equations on a detailed model using more basic, biologically-grounded synaptic plasticity mechanisms that does capture these STDP findings The learning function we adopt for the models in the rest of this text is called the The top-down approach leverages the key idea behind the BCM learning function, which is the use of a floating threshold for determining the amount of activity needed to elicit LTP vs LTD As well see below, this function contributes to useful self-organizing learning, where different neurons come to extract distinct aspects of statistical structure in a given environment. But purely self-organizing mechanisms are strongly limited in what they can learn --they are driven by statistical generalities, and are incapable of adapting more pragmatically to the functional demands that the organism faces. For example, some objects are more important to recognize than others.To achieve these more pragmatic goals, we need error-driven learning, where learning is focused specifically on correcting errors, not just categorizing statistical patterns. Fortunately, we can use the same floating threshold mechanism to achieve error-driven learning within the same overall mathematical framework, by adapting the threshold on a faster time scale. In this case, weights are increased if activity states are greater than their very recent levels, and conversely, weights decrease if the activity levels go down relative to prior states. Thus, we can think of the recent activity levels as reflecting expectations which are subsequently compared to actual outcomes, with the difference driving learning. Because both forms of learning are quite useful, and use the exact same mathematical framework, we integrate them both into a single set of equations with two thresholds reflecting integrated activity levels across different time scales.Next, we describe the XCAL dWt function, before describing how it captures both forms of learning, followed by their integration into a single unified framework.The XCAL dWt Function   where is the piecewise linear function shown in  where is a constant that determines the point where the function reverses direction --this reversal point occurs at , so that it adapts according to the dynamic value.As noted in the previous section, the dependence of the NMDA channel on activity of both sending and receiving neurons can be summarized with this simple Hebbian product, and the level of intracellular Ca is likely to reflect this value. Thus, the XCAL dWt function makes very good sense in these terms: it reflects the qualitative nature of weight changes as a function of Ca that has been established from empirical studies and postulated by other theoretical models for a long time. The Urakubo model simulates detailed effects of prepostsynaptic spike timing on Ca levels and associated LTPLTD, but what emerges from these effects at the level of firing rates is this much simpler fundamental function.As a learning function, this basic XCAL dWt function has some advantages over a plain Hebbian function, while sharing its basic nature due to the "pre  post" term at its core. For example, because of the shape of the dWt function, weights will go down as well as up, whereas the Hebbian function only causes weights to increase. But it still has the problem that weights will increase without bound. Well see in the next section that some other top-down computationally-motivated modifications can result in a more powerful form of learning while maintaining this basic form.    where xy is understood to be the short-term average synaptic activity, which could be more formally expressed as: , and is the long-term average activity of the postsynaptic neuron, which plays the role of the floating threshold value in the XCAL function. This long-term average is obtained by running average, with periods of increases in activity contributing more to the average than periods of silence. The extra x term multiplying the long-term average is necessary to achieve an approximation to the Hebbian-like functionality similar to the BCM algorithm. Both of these details are explained in the XCALDetails sub-topic. Thus, as in the brain, it is really the temporally extended patterns of activity over different time scales that drive learning in our models.Figure 4.7 shows the main qualitative behavior of this learning mechanism: when the long term average activity of the receiver is low, the threshold moves down, and thus it is more likely that the short term synaptic activity value will fall into the positive weight change territory. This will tend to increase synaptic weights overall, and thus make the neuron more likely to get active in the future, achieving the homeostatic objective. Conversely, when the long term average activity of the receiver is high, the threshold is also high, and thus the short term synaptic activity is more likely to drive weight decreases than increases. This will take these over-active neurons down a notch or two, so they dont end up dominating the activity of the network.This ability to spread the neural activity around in a more equitable fashion turns out to be critical for self-organizing learning, because it enables neurons to more efficiently and effectively cover the space of things to represent. To see why, here are the critical elements of the self-organizing learning dynamic: Inhibitory competition --only the most strongly driven neurons get over the inhibitory threshold, and can get active. These are the ones whose current synaptic weights best fit the current input pattern. Rich get richer positive feedback loop --due to the nature of the learning function, only those neurons that actually get active are capable of learning. Thus, the neurons that already detect the current input the best are the ones that get to further strengthen their ability to detect these inputs. This is the essential insight that Hebb had with why the Hebbian learning function should strengthen an "engram". homeostasis to balance the positive feedback loop --if left unchecked, the rich-get-richer dynamic ends up with a few units dominating everything, and as a result, all the inputs get categorized into one useless, overly-broad category. The homeostatic mechanism in BCM helps fight against this by raising the floating threshold for highly active neurons, causing their weights to decrease for all but their most preferred input patterns, and thus restoring a balance. Similarly, under-active neurons experience net weight increases that get them participating and competing more effectively, and hence they come to represent distinct features.The net result is the development of a set of neural detectors that relatively evenly cover the space of different inputs patterns, with systematic categories that encompass the statistical regularities. For example, cats like milk, and dogs like bones, and we can learn this just by observing the reliable co-occurrence of cats with milk and dogs with bones.This kind of reliable co-occurrence is what we mean by "statistical regularity". See Hebbian Learning for a very simple illustration of why Hebbian-style learning mechanisms capture patterns of co-occurrence. It is really just a variant on the basic maxim that "things that fire together, wire together".There is an important factor missing from the above equations, which is the learning rate --we typically use the greek epsilon to represent this parameter, which simply multiplies the rate with which the weights change: Thus, a bigger epsilon means larger weight changes, and thus quicker learning, and vice-versa for a smaller value. A typical starting value for the learning rate is .02, and we often have it decrease over time --this typically results in the fastest overall learning and best final performance.Many researchers have the potentially dangerous belief that a faster learning rate is better, and various drugs have been developed that effectively increase the learning rate, causing rats to learn some kind of standard task faster than normal, for example. However, we will see in the Learning and Memory Chapter that actually a slow learning rate has some very important advantages. Specifically, a slower learning rate enables the system to incorporate more statistics into learning --the learning rate determines the effective time window over which experiences are averaged together, and a slower learning rate gives a longer time window, which enables more information to be integrated. Thus, learning can be much smarter with a slower learning rate. But the tradeoff of course is that the results of this smarter learning take that much longer to impact actual behavior. Many have argued that humans are distinctive in our extremely protracted period of developmental learning, so we can learn a lot before we need to start earning a paycheck. This allows us to have a pretty slow learning rate, without too many negative consequences.The best way to see this dynamic is via the computational exploration. Open the Self Organizing simulation and follow the directions from there.Error-Driven Learning: Short Time Scale Floating Threshold  what you need to do to fix a problem. Raw signals are not nearly as informative --it is easy to become overwhelmed by the forest and lose sight of the trees. Well see more specific examples later, after first figuring out how we can get error-driven learning to work in the first place.  Although this expectation-outcome comparison is the fundamental requirement for error-driven learning, a weight change based on this difference by itself begs the question of how the neurons would ever know which phase they are in. In XCAL, we use a more plausible implementation, in which instead of having discrete phases, we compare the most recent synaptic activity to that integrated over the medium-time scale, which effectively includes both minus and plus phases. Because the XCAL learning function is linear, the association of the floating threshold with this synaptic activity over the medium time frame, to which the short-term outcome is compared, directly computes their difference: Intuitively, we can understand how this error-driven learning rule works by just thinking about different specific cases. The easiest case is when the expectation is equivalent to the outcome --the two terms above will be the same, and thus their subtraction is zero, and the weights remain the same. So once you obtain perfection, you stop learning. What if your expectation was higher than your outcome The difference will be a negative number, and the weights will thus decrease, so that you will lower your expectations next time around.Intuitively, this makes perfect sense --if you have an expectation that all movies by M. Night Shyamalan are going to be as cool as The Sixth Sense, you might end up having to reduce your weights to better align with actual outcomes. Conversely, if the expectation is lower than the outcome, the weight change will be positive, and thus increase the expectation. You might have thought this class was going to be deadly boring, but maybe you were amused by the above mention of M. Night Shyamalan, and now youll have to increase your weights just a bit. It should hopefully be intuitively clear that this form of learning will work to minimize the differences between expectations and outcomes over time. Note that while the example given here was cast in terms of deviations from expectations having value Because of its explicitly temporal nature, there are a few other interesting ways of thinking about what this learning rule does. To reiterate, the rule says that the outcome comes immediately after a preceding expectation --this is a direct consequence of making it learn toward the short-term average synaptic activity, compared to a slightly longer medium-term average that includes the time just before the immediate present.First, we can think of this learning in terms of the attractor dynamics discussed in the Networks Chapter.Specifically, the name Contrastive Attractor Learning reflects the idea that the network is settling into an attractor state, and it is the contrast between the final attractor state that the network settles into, versus the networks activation trajectory as it approaches the attractor, that drives learning The short-time scale average reflects the final attractor state, and the medium time-scale average reflects the entire trajectory during settling. When the pattern of activity associated with the expectation is far from the actual outcome, the difference between these two attractor states will be large, and learning will drive weight changes so that in future encounters, the expectation will more closely reflect the outcome. The X part of XCAL simply reflects the fact that the same objective is achieved without having to explicitly compare two attractors at discrete points in time, but instead by using a time-averaged activity state eXtended across the entire settling trajectory as the baseline comparison, which is more biologically realistic because such variables are readily accessible by local neuronal activity.Mathematically, this CAL learning rule represents a simpler version of the oscillating learning function developed byNorman and colleagues --see Oscillating Learning Function for more details.There are also more general reasons for later information to train earlier information. Typically, the longer one waits, the better quality the information is --at the start of a sentence, you might have some idea about what is coming next, but as it unfolds, the meaning becomes clearer and clearer. This later information can serve to train up the earlier expectations, so that you can more efficiently understand things next time around.Before continuing, you might be wondering about the biological basis of this error-driven form of the floating threshold. Unlike the BCM-style floating threshold, which has solid empirical data consistent with it, the idea that the threshold changes on this quicker time scale to reflect the medium time-scale average synaptic activity has not yet been tested empirically. Thus, it stands as an important prediction of this computational model. Because it is so easily computed, and results in such a powerful form of learning, it seems plausible that the brain would takeadvantage of just such a mechanism, but well have to see how it stands up to empirical testing. We note here that there is substantial evidence that transient changes in neuromodulation that occur during salient, unexpected events, are important for modifying synaptic plasticity --and may functionally contribute to this type of error-driven learning mechanism. Also, we discuss a little bit later another larger concern about the nature and origin of the expectation vs. outcome distinction, which is central to this form of error-driven learning.As noted above, error-driven learning is much more computationally powerful than self-organizing learning. For example, all computational models that perform well at the difficult challenge of learning to recognize objects based on their visual appearance utilize a form of error-driven learning. Many also use self-organizing learning, but this tends to play more of a supporting role, whereas the models would be entirely non-functional without error-driven learning. Error-driven learning ensures that the model makes the kinds of categorical discriminations that are relevant, while avoiding those that are irrelevant. For example, whether a side view of a car is facing left or right is not relevant for determining that this is a car. But the presence of wheels is very important for discriminating a car from a fish. A purely self-organizing model has no way of knowing that these differences, which may be quite statistically reliable and strong signals in the input, differ in their utility for the categories that people care about.Mathematically, the history of error-driven learning functions provides a fascinating window into the sociology of science, and how seemingly simple ideas can take a while to develop. In the Backpropagation subsection, we trace this history through the derivation of error-driven learning rules, from the delta rule The Pattern Associator simulation provides a nice demonstration of the limitations of self-organizing Hebbian-style learning, and how error-driven learning overcomes these limitations, in the context of a simple two-layer pattern associator that learns basic inputoutput mappings. Follow the directions in that simulation link to run the exploration.You should have seen that one of the inputoutput mapping tasks was impossible for even error-driven learning to solve, in the two-layer network. The next exploration, Error Driven Hidden shows that the addition of a hidden layer, combined with the powerful error-driven learning mechanism, enables even this "impossible" problem to now be solved. This demonstrates the computational power of the Backpropagation algorithm.Although scientists have a tendency to want to choose sides strongly and declare that either self-organizing learning or error-driven learning is the best way to go, there are actually many advantages to combining both forms of learning together. Each form of learning has complementary strengths and weaknesses: Self-organizing is more robust, because it only depends on local statistics of firing, whereas error-driven learning implicitly depends on error signals coming from potentially distant areas. Self-organizing can achieve something useful even when the error signals are remote or not yet very coherent. But self-organizing learning is also very myopic --it does not coordinate with learning in other layers, and thus tends to be "greedy". In contrast, error-driven learning achieves this coordination, and can learn to solve problems that require collective action of multiple units across multiple layers.One analogy that may prove useful is that error-driven learning is like left-wing politics --it requires all the different layers and units to be working together to achieve common goals, whereas self-organizing learning is like right-wing politics, emphasizing local, greedy actions that somehow also benefit society as a whole, without explicitly coordinating with others. The tradeoffs of these political approaches are similar to those of the respective forms of learning. Socialist approaches can leave individual people feeling not very motivated, as they are just a little cog in a huge faceless machine. Similarly, neurons that depend strictly on error-driven learning can end up not learning very much, as they only need to make a very small and somewhat "anonymous" contribution to solving the overall problem. Once the error signals have been eliminated, learning stops. We will see that networks that rely on pure error-driven learning often have very random-looking weights, reflecting this minimum of effort expended toward solving the overall problem. On the other side, more strongly right-wing capitalist approaches can end up with excessive positive feedback loops, and are typically not good at dealing with longer-term, larger-scale problems that require coordination and planning. Similarly, purely self-organizing models tend to end up with more uneven distributions of "representational wealth" and almost never end up solving challenging problems, preferring instead to just greedily encode whatever interesting statistics come their way. Interestingly, our models suggest that a balance of both approaches --a centrist approach --seems to work best Perhaps this lesson can be generalized back to the political arena.Colorful analogies aside, the actual mechanics of combining both forms of learning within the XCAL framework amounts to merging the two different definitions of the floating threshold value. We just compute a weighted average of the two, using a "lambda" parameter to weight the long-term receiver average relative to the medium-term synaptic co-product: By using the medium-term average value to multiply the long-term average activation , we obtain a simpler expression that shows that were really just taking something between the medium and long-term average receiving neuron activity --the more long term that contributes, the more it computes the BCM-like self-organizing learning, and the more the medium-term contributes, the more it computes error-driven learning. It is reasonable that this lambda parameter may differ according to brain area, and even that it may be dynamically regulated.The one last issue we need to address computationally is the problem of synaptic weights growing without bound. In LTP experiments, it is clear that there is a maximum synaptic weight value --you cannot continue to get LTP on the same synapse by driving it again and again. The weight value saturates. There is a natural bound on the lower end, for LTD, of zero. Mathematically, the simplest way to achieve this kind of weight bounding is through an exponential approach function, where weight changes become exponentially smaller as the bounds are approached.This function is most directly expressed in a programming language format, as it involves a conditional:if dwt  0 then wt  wt   dwt else wt  wt  wt  dwtIn words: if weights are supposed to increase, then multiply the rate of increase by 1-wt, where 1 is the upper bound, and otherwise, multiply by the weight value itself. As the weight approaches 1, the weight increases get smaller and smaller, and similarly as the weight value approaches 0. The exponential approach function works well at keeping weights bounded in a graded way, but it also creates a strong tendency for weights to hang out in the middle of the range, around .5. This creates problems because then neurons dont have sufficiently distinct responses to different input patterns, and then the inhibitory competition breaks down, which then interferes with the positive feedback loop that is essential for learning, etc. To counteract these problems, while maintaining the exponential bounding, we introduce a contrast enhancement function on the weights: As you can see in Biologically, we think of the plain weight value w, which is involved in the learning functions, as an internal variable that accurately tracks the statistics of the learning functions, while the contrast-enhanced weight value is the actual synaptic efficacy value that you measure and observe as the strength of interaction among neurons. Thus, the plain w value may correspond to the phosphorylation state of CAMKII or some other appropriate internal value that mediates synaptic plasticity.Finally, see Implementational Details for a few implementational details about the way that the time averages are computed, which dont affect anything conceptually, but if you really want to know exactly what is going on..When, Exactly, is there an Outcome that should Drive Learning  A leading hypothesis for something that "marks" the presence of an important outcome is a phasic burst of a neuromodulator like dopamine. It is well established that dopamine bursts occur when an unexpected outcome arises, at least in the context of expectations of reward or punishment . Furthermore, we know from a number of studies that dopamine plays a strong role in modulating synaptic plasticity. Under this hypothesis, the cortical network is always humming along doing standard BCM-like self-organizing learning at a relatively low learning rate, and then, when something unexpected occurs, a dopamine burst drives stronger error-driven learning, with the immediate short-term average "marked" by the dopamine burst as being associated with this important outcome. The XCAL learning will automatically contrast this immediate short-term average with the immediately available medium-term average, which presumably reflects an important contribution from the prior expectation state that was just violated by the outcome.There are many other possible ideas for how the time for error-driven learning is marked, some of which involve local emergent dynamics in the network itself, and others that involve other neuromodulators, or networks with broad connectivity to broadcast an appropriate "learn now" signal. From everything we know about the brain, there are likely several such learning signals, each of which being useful in some particular subset of situations. This is an active area of ongoing research.Stepping up a level, we can consider the kinds of environmental situations that we think should drive learning. When are we likely to be confronted with outcomes that violate our expectations The Leabra Framework  computationally-motivated models.The biologically-based way of doing error-driven learning requires bidirectional connectivity, and the Leabra framework is relatively unique in its ability to learn complex computational tasks in the context of this pervasive bidirectional connectivity. Also, the kWTA inhibitory function is unique to the Leabra framework, and is also very important for its overall behavior, especially in managing the dynamics that arise with the bidirectional connectivity.The different elements of the Leabra framework are therefore synergistic with each other, and as we have discussed, highly compatible with the known biological features of the neocortex. Thus, the Leabra framework provides a solid foundation for the cognitive neuroscience models that we explore next in the second part of the text.For those already familiar with the Leabra framework, see Leabra Details for a brief review of how the XCAL version of the algorithm described here differs from the earlier form of Leabra described in the original CECN textbook Open the Family Trees simulation to explore how the combination of error-driven and self-organizing learning in the Leabra algorithm can produce better learning than either alone. This simulation is also very interesting for showing how networks can create their own similarity structure based on functional relationships, refuting the common misconception that networks are driven purely by input similarity structure.Here are all the sub-topics within the Learning chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: Detailed Biology of Learning --more in-depth treatment of postsynaptic signaling cascades that mediate LTP and LTD, described in context of the  Hebbian Learning --extensive treatment of computational properties of Hebbian learning --starts with a simple manual simulation of Hebbian learning showing exactly how and why it captures patterns of co-occurrence. STDP --more details on spike timing dependent plasticity Backpropagation --history and mathematical derivation of error-driven learning functions --strongly recommended to obtain greater insight into the computational nature of error-driven learning. Oscillating Learning Function --Norman et al learning rule based on rapidly changing synaptic plasticity combined with oscillations in inhibitory strength --produces an interesting hybrid of error-driven and self-organizing learning, and is mathematically equivalent to the CAL learning function. Implementational Details --misc implementational details about how time averaged activations are computed, etc. Leabra Details --describes how the current version of Leabra based on XCAL differs from the original one from the CECN textbook Here are all the explorations covered in the main portion of the Learning chapter: Self Organizing --Self organizing learning using BCM-like dynamic of XCAL  Pattern Associator --Basic two-layer network learning simple inputoutput mapping tasks withHebbian and Error-driven mechanisms. Error Driven Hidden --Full error-driven learning with a hidden layer, can solve any input output mapping. Part II --Cognitive NeuroscienceIn Part I of this book, we have developed a toolkit of basic neural mechanisms, going from the activation dynamics of individual neurons, to networks of neurons, and the learning mechanisms that configure them in both self-organizing and error-driven ways. At this start of Part II, we begin the transition to exploring a wide range of cognitive phenomena. As an important foundational step along this path, this chapter attempts to provide a big picture view of the overall functional organization of the brain, in a relatively non-controversial way that is roughly meant to correspond to what is generally agreed upon in the literature. This should help you understand at a broad level how different brain areas work together to perform different cognitive functions, and situate the more specific models in the subsequent chapters into a larger overall framework.We proceed in the same sequence as the subsequent chapters, which roughly follows the evolutionary trajectory of the brain itself, starting with basic perceptual and motor systems, and then proceeding to explore different forms of learning and memory. Building upon these core capacities, we then examine language and executive function, which build upon and extend the functionality of these basic cognitive systems.As usual, we begin with a basic foundation in biology: the gross anatomy of the brain.Navigating the Functional Anatomy of the Brain   Hippocampus --this brain area is actually an "ancient" form of cortex called "archicortex", and well see in Learning and Memory how it plays  Learning. Basal Ganglia --this is a collection of subcortical areas that plays a critical role in Motor Control and Reinforcement Learning, and also in Executive Function. It helps to make the final "Go" call on whether to execute particular actions that the cortex proposes, and whether or not to update cognitive plans in the prefrontal cortex. Its policy for making these choices is learned based on their prior history of reinforcementpunishment.  Temporal lobe --departing from the occipital lobe, the what pathway of visual processing dives down into inferotemporal cortex, where visual objects are recognized. Meanwhile, superior temporal cortex contains primary auditory cortex, and associated higher-level auditory and language-processing areas. Thus, the temporal lobes are where the visual appearance of objects gets translated into verbal labels  Parietal lobe --in contrast to temporal lobe, the parietal lobe is much murkier and subconscious. It is important for encoding spatial locations, and damage to certain parts of parietal gives rise to the phenomenon of hemispatial neglect --people just forget about an entire half of space But its functionality goes well beyond mere spatial locations. It is important for encoding number, mathematics, abstract relationships, and many other "smart" things. At a more down-to-earth level, parietal cortex provides the major pathway where visual information can guide motor actions, leading it to be characterized as the how pathway. It also contains the primary somatosensory cortex, which is important for guiding and informing motor actions as well. In some parts of parietal cortex, neurons serve to translate between different frames of reference, for example converting spatial locations on the body to visual coordinates. And visual information can be encoded in terms of the patterns of activity on the retinal, or head, body, or environment-based reference frames. One broad characterization of parietal cortex is that it is specialized for processing metrical information --things that vary along a continuum, in direct contrast with the discrete, categorical nature of temporal lobe. A similar distinction is popularly discussed in terms of left vs. right sides of the brain, but the evidence for this in terms of temporal vs.parietal is stronger overall.. The biggest mystery about the frontal lobe is how to understand how it does all of these amazing things, without using terms like "executive", because were pretty sure you dont have a little guy in a pinstripe suit sitting in there. It is all just neuronsArea Reward Error Self Org Separator Integrator Attractor  The hippocampus does introduce one critical innovation beyond what is present in the basal ganglia and cerebellum:it has attractor dynamics. Specifically the recurrent connections between CA3 neurons are important for retrieving previously-encoded memories, via pattern completion as we explored in the Networks Chapter. The price for this innovation is that the balance between excitation and inhibition must be precisely maintained, to prevent epileptic activity dynamics. Indeed, the hippocampus is the single most prevalent source of epileptic activity, in people at least.Against this backdrop of evolutionarily older systems, the neocortex represents a few important innovations. In terms of activation dynamics, it builds upon the attractor dynamic innovation from the hippocampus , and adds to this a strong ability to develop representations that integrate across experiences to extract generalities, instead of always keeping everything separate all the time. The cost for this integration ability is that the system can now form the wrong kinds of generalizations, which might lead to bad overall behavior. But the advantages apparently outweigh the risks, by giving the system a strong ability to apply previous learning to novel situations. In terms of learning mechanisms, the neocortex employs a solid blend of all three major forms of learning, integrating the best of all the available learning signals into one system.   V2 --secondary visual cortex, which encodes combinations of edge detectors to develop a vocabulary of intersections and junctions, along with many other basic visual features, that provide the foundation for detecting more complex shapes. These V2 neurons also encode these features in a broader range of locations, starting a process that ends up with IT neurons being able to recognize an object regardless of where it appears in the visual field. V4 --detects more complex shape features, over an even larger range of locations. Carrying the parietal how pathway forward, visual information going along the dorsal pathway through the parietal cortex heads directly into the frontal cortex, where it can drive motor neurons in primary motor cortex, which can directly drive the muscles to produce overt motor actions. This completes the critical sensory-motor loop that lies at the core of all behavior. Motor control also critically involves many subcortical brain areas, including the basal ganglia and cerebellum. The rough division of labor between these areas is: Neocortex --does high-level metrical processing of sensory information, integrating multiple modalities and translating between different reference frames as necessary, to arrive at a range of possible responses to the current sensory environment. Basal Ganglia --receives both sensory inputs and the potential responses being "considered" in frontal cortex, and can then trigger a disinhibitory Go signal that enables the best of the possible actions to get over threshold and actually drive behavior. This process of action selection is shaped by reinforcement learning --the basal ganglia are bathed in dopamine, which drives learning in response to rewards and punishments, and also influences the speed of the selection process itself. Thus, the basal ganglia selects the action that is most likely to result in reward, and least likely to result in punishment. The amygdala plays a key role in driving these dopamine signals in response to sensory cues associated with reward and punishment. Cerebellum --is richly interconnected with the parietal and motor cortex, and it is capable of using a simple yet powerful form of error-driven learning to acquire high-resolution metrical maps between sensory inputs and motor outputs. Thus, it is critical for generating smooth, coordinated motor movements that properly integrate sensory and motor feedback information to move in an efficient and controlled manner. It also likely serves to teach the parietal and motor cortex what it has learned.In Motor Control and Reinforcement Learning, we will see how dopamine signals shape basal ganglia learning and performance in a basic action selection task. Then, well explore a fascinating model of cerebellar motor learning in a virtual robot that performs coordinated eye and head movements to fixate objects --this model shows how the error signals needed for cerebellar learning can arise naturally.Interestingly, all of these "low level" motor control systems end up being co-opted by "higher level" executive function systems, so although some dont think of motor control as a particularly cognitive domain, it actually provides a solid foundation for understanding some of the highest levels of cognitive functionWhen you think of memory, probably things like "what did I have for dinner last night" and "how can I remember peoples names better" tend to come to mind. These represent just one category of memory, however. Indeed, memory is ubiquitous in neural networks --every synapse has the capacity for storing memory, and any given  When you need to remember the name associated with a person you recently met, youre relying on this rapid learning ability of the hippocampus.Well see that the neural properties of the hippocampal system are ideally suited to producing this rapid learning ability. One key neural property is the use of extremely sparse representations, which produce a phenomenon called pattern separation, where the neural activity pattern associated with one memory is highly distinct from that associated with other similar memories. This is what minimizes interference with prior learning --interference arises as a function of overlap. Well see how this pattern separation process is complemented by a pattern completion process for recovering memories during retrieval from partial information.Well also see how the learning rate plays a crucial role in learning. Obviously, to learn rapidly, you need a fast learning rate. But what happens with a slow learning rate Turns out this enables you to integrate across many different experiences, to produce wisdom and semantic knowledge. This slower learning rate is characteristic of most of the neocortex. Interestingly, even with a slow learning rate, neocortex can exhibit measurable effects of a single trial of learning, in the form of priming effects and familiarity signals that can drive recognition memory. This form of recognition memory seems to depend on medial temporal lobe areas including perirhinal cortex. Another form of one trial behavioral learning involves mechanisms that support active maintenance of memories in an attractor state. This form of memory does not require a weight change at all, but can nevertheless rapidly influence behavioral performance from one instance to the next.Language taps the coordinated function of many of the brain areas discussed above. Language requires highly sophisticated perceptual abilities, to be able to discriminate different speech sounds and different letters and combinations thereof. Likewise, sophisticated motor output abilities are required to produce the sounds and write the letters and words of language. In between, language requires some of the most demanding forms of cognitive processing, to keep track of the grammatical and semantic information streaming past you, often at high speed. This requires sophisticated executive function and working memory abilities, in addition to powerful distributed posterior-cortical semantic representations to integrate all the semantic information.Our exploration of language starts with a small-scale model of reading, that interconnects orthographic, phonological, and semantic representations of individual words, to form a distributed lexicon --there isnt one place where all word information is stored --instead it is distributed across brain areas that are specialized for processing the relevant perceptual, motor, and semantic information. Interestingly, we can simulate various forms of acquired dyslexia by damaging specific pathways in this model,providing an important way of establishing neural correlates of language function in humans, where invasive experiments are not possible.We then zoom in on the orthography to phonology pathway to explore issues with regularities and exceptions in this spelling-to-sound mapping, which has been the topic of considerable debate. We show that the object recognition model from the perception chapter has an important blend of features that support both regular and exception mappings, and this model pronounces nonword probe inputs much like people do, demonstrating that it has extracted similar underlying knowledge about the English mapping structure.Next, we zoom in on the semantics pathway, exploring how a self-organizing network can learn to encode statistical regularities in word co-occurance, that give rise to semantic representations that are remarkably effective in capturing the similarity structure of words. We train this network on an early draft of the first edition of this text, so you should be familiar with the relevant semantics Finally, we tackle the interactions between syntax and semantics in the context of processing the meaning of sentences, using the notion of a sentence gestalt representation, that uses coarse-coded distributed representations to encode the overall meaning of the sentence, integrating both syntactic and semantic cues. This is a distinctly neural approach to syntax, as contrasted with the symbolic, highly structured approaches often employed in linguistic theories.Executive Function: Prefrontal Cortex and Basal Ganglia shows an overall schematic for how this occurs. It also illustrates how the lateral surface is more associated with "cold" cognitive function, while the medial surface is more involved in "hot" emotional and motivational processing.Well see how the PFC can provide top-down cognitive control over processing in the posterior cortex, with the classic example being the Stroop task.Then well explore how PFC and BG can interact to produce a dynamically gated working memory system that allows the system to hold multiple pieces of information in mind, and to separately update some pieces of information while continuing to maintain some existing information. The role of the BG in this system builds on the more established role of the BG in motor control, by interacting in very similar circuits with PFC instead of motor cortex. In both cases, the BG provide a gating signal for determining whether or not a given frontal cortical actionshould be executed or not. Its just that PFC actions are more cognitive than motor cortex, and include things like updating of working memory states, or of goals, plans, etc. Once updated, these PFC representations can then provide that top-down cognitive control mentioned above, and hence can shape action selection in BG-motor circuits, but also influence attention to task-relevant features in sensory cortex. Interestingly, the mechanisms for reinforcing which cognitive actions to execute seem to depend on very similar dopaminergic reinforcement learning mechanisms that are so central to motor control. This framework also provides a link between motivation and cognition which is very similar to the well established link between motivation and action.Perception is at once obvious and mysterious. It is so effortless to us that we have little appreciation for all the amazing computation that goes on under the hood. And yet we often use terms like "vision" as a metaphor for higher-level concepts --perhaps this actually reflects a deep truth: that much of our higher-level cognitive abilities depend upon our perceptual processing systems for doing a lot of the hard work. Perception is not the mere act of seeing, but is leveraged whenever we imagine new ideas, solutions to hard problems, etc. Many of our most innovative scientists used visual reasoning processes to come up with their greatest insights. Einstein tried to visualize catching up to a speeding ray of light, and one of Feynmans major contributions was a means of visually diagramming complex mathematical operations in quantum physics.Pedagogically, perception serves as the foundation for our entry into cognitive phenomena. It is the most well-studied and biologically grounded of the cognitive domains. As a result, we will cover only a small fraction of the many fascinating phenomena of perception, focusing mostly on vision. But we do focus on a core set of issues that capture many of the general principles behind other perceptual phenomena.We begin with a computational model of primary visual cortex, which shows how self-organizing learning principles can explain the origin of oriented edge detectors, which capture the dominant statistical regularities present in natural images. This model also shows how excitatory lateral connections can result in the development of topography in V1 --neighboring neurons tend to encode similar features, because they have a tendency to activate each other, and learning is determined by activity.Building on the features learned in V1, we explore how higher levels of the ventral what pathway can learn to recognize objects regardless of considerable variability in the superficial appearance of these objects as they project onto the retina. Object recognition is the paradigmatic example of how a hierarchically-organized sequence of feature category detectors can incrementally solve a very difficult overall problem. Computational models based on this principle can exhibit high levels of object recognition performance on realistic visual images, and thus provide a compelling suggestion that this is likely how the brain solves this problem as well.Next, we consider the role of the dorsal where pathway in spatial attention. Spatial attention is important for many things, including object recognition when there are multiple objects in view --it helps focus processing on one of the objects, while degrading the activity of features associated with the other objects, reducing potential confusion. Our computational model of this interaction between what and where processing streams can account for the effects of brain damage to the where pathway, giving rise to hemispatial neglect for damage to only one side of the brain, and a phenomenon called Balints syndrome with bilateral damage. This ability to account for both neurologically intact and brain damaged behavior is a powerful advantage of using neurally-based models.As usual, we begin with a review of the biological systems involved in perception.Biology of Perception  This will serve to situate the models that come later, which provide a much more complete picture of each step of information processing.  Organization of information in a topographic fashion --for example, the left vs. right visual fields are organized into the contralateral hemispheres of cortex LGN cells will fire due to the differential excitation vs.inhibition they receive --as the figure shows, signals from the left part of visual space are routed to the right hemisphere, and vice-versa. Information within LGN and V1 is also organized topographically in various ways. This organization generally allows similar information to be contrasted, producing an enhanced signal, and also grouped together to simplify processing at higher levels. Extracting relevant signals, while filtering irrelevant ones --  Newer reverse correlation V1 receptive field mapping: http:  www. youtube. com watchvn31XBMSSSpI  V4 --detects more complex shape features, over an even larger range of locations. IT-posterior --detects entire object shapes, over a wide range of locations, sizes, and angles. For example, there is an area near the fusiform gyrus on the bottom surface of the temporal lobe, called the fusiform face area, that appears especially responsive to faces. As we saw in the Networks Chapter, however, objects are encoded in distributed representations over a broad range of areas in IT. IT-anterior --this is where visual information becomes extremely abstract and semantic in nature --it can encode all manner of important information about different people, places and things.In contrast, the where aspect of visual processing going up in a dorsal directly through the parietal cortex contains areas that are important for processing motion, depth, and other spatial features.  images that typically fall on our retinas. These are the most obvious The situation here is essentially equivalent to the self organizing learning model explored in the Learning Chapter, which was exposed to horizontal and vertical lines, and learned to represent these strong statistical regularities in the environment.However, that earlier simulation did nothing to address the topography of the V1 neurons --why do neighbors tend to encode similar information The answer we explore in the following simulation is that neighborhood-level connectivity can cause nearby neurons to tend to activate together, and because activity drives learning, this then causes them to tend to learn similar things.Open V1Rf to explore the development of oriented edge detectors in V1. This model gets exposed to a set of natural images, and learns to encode oriented edges, because they are the statistical regularity present in these images. Hemispatial Neglect   Notice that neglect appears to operate at two different spatial scales here: for the entire set of lines, and within each individual line.The Posner Spatial Cueing Task invalid ones, suggesting that spatial attention was drawn to that side of space. Patients with hemispatial neglect exhibit slowing for targets that appear in the neglected side of space, particularly when invalidly cued. We explore an alternative account here, based on bidirectional interactions between spatial and object  processing pathways Importantly, these models make distinct predictions regarding the effects of bilateral parietal damage. Patients with this condition are known to suffer from Balints syndrome, which is characterized by a profound inability to recognize objects when more than one is present in the visual field. This is suggestive of the important role that spatial attention plays in facilitating object recognition in crowded visual scenes. According to Posners disengage model, bilateral damage should result in difficulty disengaging from both sides of space, producing slowing in invalid trials for both sides of space. In contrast, the competition-based model makes the opposite prediction: the lesions serve to reduce competition on both sides of space, such that there should be reduced attentional effects on both sides. That is, the effect of the invalid cue actually decreases in magnitude.The data is consistent with the competition model, and not Posners model.Open AttnSimple to explore a model with spatial and object pathways interacting in the context of multiple spatial attention tasks, including perceiving multiple objects, and the Posner spatial cueing task. It reproduces the behavioral data shown above, and correctly demonstrates the observed pattern of reduced attentional effects for Balints patients.Here are all the sub-topics within the Perception chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read:Here are all the explorations covered in the main portion of the Perception and Attention chapter: V1Rf --V1 receptive fields from Hebbian learning. Objrec --invariant object recognition. AttnSimple --simple attention model.Area Reward Error Self Org Separator Integrator Attractor  In this chapter, we complete the loop that started in the previous chapter on Perception and Attention, by covering a few of the most important motor output and control systems, and the learning mechanisms that govern their behavior. At the subcortical level, the cerebellum and basal ganglia are the two major motor control areas, each of which has specially adapted learning mechanisms that differ from the general-purpose cortical learning mechanisms described in the Learning Mechanisms chapter There is a nice division of labor here, where the basal ganglia help to select one out of many possible actions to perform, and the cerebellum then makes sure that the selected action is performed well. Consistent with this rather clean division of labor, there are no direct connections between the basal ganglia and cerebellum --instead, each operates in interaction with various areas in the cortex, where the action plans are formulated and coordinated. Both basal ganglia and cerebellum are densely interconnected with the frontal cortex, including motor control areas in posterior frontal cortex, and the prefrontal cortex anterior to those. Also, as discussed in the prior chapter, the parietal cortex is important for mapping sensory information to motor outputs, by way of computing things like spatial maps, and relative spatial relationships among objects in the environment. Thus, parietal representations drive motor action execution as coordinated by the cerebellum, and cerebellum is also densely interconnected with parietal cortex. In contrast, the basal ganglia are driven to a much greater extent by the ventral pathway "what" information, which indicates the kinds of rewarding objects that might be present in the environment. They do also receive some input from parietal, but just not to the great extent that the cerebellum does.Both the cerebellum and basal ganglia have a complex disinhibitory output dynamic, which produces a gating-like effect on the brain areas they control. For example, the basal ganglia can disinhibit neurons in specific nuclei of the thalamus, which have bidirectional excitatory circuits through frontal and prefrontal cortical areas. The net effect of this disinhibition is to enable an action to proceed, without needing to specify any of the details for how to perform that action. This is what is meant by a gate --something that broadly modulates the flow of other forms of activation.The cerebellum similarly disinhibits parietal and frontal neurons to effect its form of precise control over the shape of motor actions. It also projects directly to motor outputs in the brain stem, something that is not true of most basal ganglia areas.We begin the chapter with the basal ganglia system, including the reinforcement learning mechanisms. Then we introduce the cerebellar system, and its unique form of error-driven learning. Each section starts with a review of the relevant neurobiology of each system.  Interestingly, the additional inputs that converge into the basal ganglia for a given area all make good sense. Motor  The striatum, which is the major input region, consisting of the caudate and putamen subdivisions The matrix clusters contain direct and indirect pathway medium spiny neurons, which together make up 95% of striatal cells, both of which receive excitatory inputs from all over the cortex but are inhibitory on their downstream targets in the globus pallidus as described next. The patch cells project to the dopaminergic system, and thus appear to play a more indirect role in modulating learning signals. There are also a relatively few widely spaced tonically active neurons, which release acetylcholine as a neurotransmitter and appear to play a modulatory role, and inhibitory interneurons, which likely perform the same kind of dynamic gain control that they play in the cortex. The globus pallidus, internal segment, which is a much smaller structure than the striatum, and contains neurons that are constantly active even with no additional input. These neurons send inhibition to specific nuclei in the thalamus. When the directGo pathway striatum neurons fire, they inhibit these GPi neurons, and thus disinhibit the thalamus, resulting ultimately in the initiation of a specific motor or cognitive action. Note that in other fronto-basal ganglia circuits, the role of the GPi is taken up by the substantia nigra pars reticulata, which is situated identically to the GPi anatomically, but receives from other areas of striatum and projects to outputs regulating other actions. The globus pallidus, external segment, which is also small, and contains tonically active neurons that send focused inhibitory projections to corresponding GPi neurons. When the indirectNoGo pathway neurons in the striatum fire, they inhibit the GPe neurons, and thus disinhibit the GPi neurons, causing them to provide even greater inhibition onto the thalamus. This blocks the initiation of specific actions coded by the population of active NoGo neurons. The thalamus, specifically the medial dorsal, ventral anterior, and ventrolateral nuclei  The substantia nigra pars compacta has neurons that release the neuromodulator dopamine, and specifically innervate the striatum. Interestingly, there are two different kinds of dopamine receptors in the striatum. D1 receptors are prevalent in Go pathway neurons, and dopamine has an excitatory effect on neurons with D1 receptors. In contrast, D2 receptors are prevalent in NoGo pathway neurons, and dopamine has an inhibitory effect via the D2 receptors. Thus, when a burst of dopamine hits the striatum, it further excites active Go units and inhibits NoGo units. This change in activity results in activity-dependent plasticity, and thus leads to an increased propensity for initiating motor and cognitive actions. In contrast, when a dip in dopamine firing occurs, Go neurons are less excited, while NoGo neurons are disinhibited, and thus those NoGo neurons receiving excitatory input from cortex will become more excited due to the dopamine dip.Again, this change in activity results in potentiation of synapses, such that this specific population of NoGo neurons will be more likely to become active in future encounters of this sensory state and candidate motor action. Both of these effects of dopamine bursts and dips make perfect sense: dopamine bursts are associated with positive reward prediction errors, and thus reinforce selection of actions that lead to good results. Conversely, dopamine dips are associated with negative reward prediction errors and thus lead to avoidance of those actions that tend to result in these bad results. Also, tonic levels of dopamine can influence the relative balance of activity of these pathways, so that even if learning has already occurred, changes in dopamine can affect whether action selection is influenced primarily by learned Go vs learned NoGo values --roughly speaking, the higher the dopamine, the more risky the choices. The subthalamic nucleus is also a major component of the basal ganglia, which acts as the third hyperdirect pathway, so named because it receives input directly from frontal cortex and sends excitatory projections directly to BG output, bypassing the striatum altogether. These STN-GPi projections are diffuse, meaning that a single STN neuron projects broadly to many GPi neurons, and as such the STN is thought to provide a global NoGo function that prevents gating of any motor or cognitive action. This area has been shown in models and empirical data to become more active with increasing demands for response inhibition or when there is conflict between alternative cortical action plans, so that the STN buys more time for striatal gating to settle on the best action Open BG for an exploration of a basic model of go vs. nogo action selection, gating, and reinforcement learning dynamics in the basal ganglia. This model also allows you to investigate the effects of Parkinsons disease and dopaminergic medications. Computationally, the simplest model of reward prediction error is the Rescorla-Wagner conditioning model, which is mathematically identical to the delta rule as discussed in the Learning Mechanisms chapter, and is simply the difference between the actual reward and the expected reward:where is the reward prediction error, r is the amount of reward actually received, and is the amount of reward expected, which is computed as a weighted sum over input stimuli x with weights w. The weights adapt to try to accurately predict the actual reward values, and in fact this delta value specifies the direction in which the weights should change: This is identical to the delta learning rule, including the important dependence on the stimulus activity x --you only want to change the weights for stimuli that are actually present.When the reward prediction is correct, then the actual reward value is cancelled out by the prediction, as shown in the second panel in What the Rescorla-Wagner model fails to capture is the firing of dopamine to the onset of the CS in the second panel in  where f represents the future rewards, and now the reward expectation has to try to anticipate both the current reward r and this future reward f. In a simple conditioning task, where the CS reliably predicts a subsequent reward, the onset of the CS results in an increase in this f value, because once the CS arrives, there is a high probability of reward in the near future. Furthermore, this f itself is not predictable, because the onset of the CS is not predicted by any earlier cue.Therefore, the r-hat expectation cannot cancel out the f value, and a dopamine burst ensues. Although this f value explains CS-onset dopamine firing, it raises the question of how can the system know what kind of rewards are coming in the future Like anything having to do with the future, you fundamentally just have to guess, using the past as your guide as best as possible. TD does this by trying to enforce consistency in reward estimates over time. In effect, the estimate at time t is used to train the estimate at time t-1, and so on, to keep everything as consistent as possible across time, and consistent with the actual rewards that are received over time.This can all be derived in a very satisfying way by specifying something known as a value function, V that is a sum of all present and future rewards, with the future rewards discounted by a "gamma" factor, which captures the intuitive notion that rewards further in the future are worth less than those that will occur sooner. As the Wimpy character says in Popeye, "Ill gladly pay you Tuesday for a hamburger today." Here is that value function, which is an infinite sum going into the future: We can get rid of the infinity by writing this equation recursively: And because we dont know anything for certain, all of these value terms are really estimates, denoted by the little "hats" above them: So this equation tells us what our estimate at the current time t should be, in terms of the future estimate at time t1.Next, we subtract V-hat from both sides, which gives us an expression that is another way of expressing the above equality --that the difference between these terms should be equal to zero: This is mathematically stating the point that TD tries to keep the estimates consistent over time --their difference should be zero. But as we are learning our V-hat estimates, this difference will not be zero, and in fact, the extent to which it is not zero is the extent to which there is a reward prediction error: If you compare this to the equation with f in it above, you can see that: and otherwise everything else is the same, except weve clarified the time dependence of all the variables, and our reward expectation is now a "value expectation" instead. Also, as with Rescorla-Wagner, the delta value here drives learning of the value expectations.The TD learning rule can be used to explain a large number of different conditioning phenomena, and its fit with the firing of dopamine neurons in the brain has led to a large amount of research progress. It represents a real triumph of the computational modeling approach for understanding brain function.Open RL for an exploration of TD-based reinforcement learning in simple conditioning paradigms. This exploration should help solidify your understanding of reinforcement learning, reward prediction error, and simple classical conditioning.The Actor-Critic Architecture for Motor Learning where it is assumed that rewards result at least in part from correct performance by the actor. The basal ganglia is the actor in this case, and the dopamine signal is the output of the critic, which then serves as a training signal for the actor. The reward prediction error signal produced by the dopamine system is a good training signal because it drives stronger learning early in a skill acquisition process, when rewards are more unpredictable, and reduces learning as the skill is perfected, and rewards are thus more predictable. If the system instead learned directly on the basis of external rewards, it would continue to learn about skills that have long been mastered, and this would likely lead to a number of bad consequences.Furthermore, the sign of the reward prediction error is appropriate for the effects of dopamine on the Go and NoGo pathways in the striatum, as we saw in the BG model project above. Positive reward prediction errors, when unexpected rewards are received, indicate that the selected action was better than expected, and thus Go firing for that action should be increased in the future. The increased activation produced by dopamine on these Go neurons will have this effect, assuming learning is driven by these activation levels. Conversely, negative reward prediction errors will facilitate NoGo firing, causing the system to avoid that action in the future.Finally, the ability of the dopamine signal to propagate backward in time is critical for spanning the inevitable delays between motor actions and subsequent rewards. Specifically, the dopamine response should move from the time of the reward to the time of the action that reliably predicts reward, in the same way that it moves in time to the onset of the CS in a classical conditioning paradigm.The PVLV Model of DA Biology  The dopamine signal in PVLV for primary values, which is in effect at the time external rewards are delivered or expected, is identical to Rescorla-Wagner, just using different labels for the variables:Where excitatory and inhibitory subscripts denote the two components of the primary value system, and the sign of their influence on dopamine firing.The dopamine signal for learned values applies whenever PV does not, and it has a similar form: Where LVe is the excitatory drive on dopamine from the CNA, which learns to respond to CSs. LVi is a counteracting inhibitory drive, again thought to be associated with the patch-like neurons of the ventral striatum. It learns much more slowly than the LVe system, and will eventually learn to cancel out CS-associated dopamine responses, once these CSs become highly highly familiar.The There are a number of interesting properties of the learning constraints in the PVLV system. First, the CS must still be active at the time of the external reward in order for the LVe system to learn about it, since LV only learns at the time of external reward. If the CS itself goes off, then some memory of it must be sustained. This fits well with known constraints on CS learning in conditioning paradigms. Second, the dopamine burst at the time of CS onset cannot influence learning in the LV system itself --otherwise there would be an unchecked positive feedback loop.One implication of this is that the LV system cannot support second-order conditioning, where a first CS predicts a second CS which then predicts reward. Consistent with this constraint, the CNA appears to only be involved in first order conditioning, while the basolateral nucleus of the amygdala is necessary for second-order conditioning. Furthermore, there does not appear to be much of any evidence for third or higher orders of conditioning. Finally, there is a wealth of specific data on differences in CS vs. US associated learning that are consistent with the PVLV framework.In short, the PVLV system can explain how the different biological systems are involved in generating phasic dopamine responses as a function of reward associations, in a way that seems to fit with otherwise somewhat peculiar constraints on the system. Also, we will see in the Executive Function Chapter that PVLV provides a cleaner learning signal for controlling the basal ganglias role in the prefrontal cortex working memory system.  how the parietal cortex can learn to do all the complex things it does. However, at this point both the parietal cortex and cerebellum are much better understood from a motor standpoint than a cognitive one.The cerebellum has a very well-defined anatomy Putting all these pieces together, David Marr and James Albus argued that the cerebellum is a system for error-driven learning, with the error signal coming from the climbing fibers. It is clear that it has the machinery to associate stimulus inputs with motor output commands, under the command of the climbing fiber inputs. One important principle of cerebellar function is the projection of inputs into a very high-dimensional space over the granule cells --computationally this achieves the separation form of learning, where each combination of inputs activates a unique pattern of granule cell neurons. This unique pattern can then be associated with a different output signal from the cerebellum, producing something approximating a lookup table of inputoutput values Exploration of Cerebellum  TODO: cerebellum circuit model learns by supervised signal trained by output of BG circuit, which learns by RL.Here are all the sub-topics within the Motor Control and Reinforcement Learning chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: PVLV Learning --full set of equations governing the learning in the PVLV model of phasic dopamine.Here are all the explorations covered in the main portion of the Motor Control and Reinforcement Learning chapter: When you think of memory, you probably think of episodic memory --memory for specific episodes or events.Maybe you can remember some special times from childhood, or some traumatic times. Probably you can remember what you had for dinner last night, and who you ate with Although this aspect of memory is the most salient for us, it is just one of many different types of memory.One broad division in memory in mechanistic, computational terms is between weight-based and activation-based forms of memory. Weight based memory is a result of synaptic plasticity, and is generally relatively long lasting. Activation-based memory is supported by ongoing neural activity, and is thus much more transient and fleeting, but also more flexible. Because weight-based memory exists at every modifiable synapse in the brain, it can manifest in innumerable ways. In this chapter, we focus on some of the most prominent types of memory studied by psychologists, starting with episodic memory, then looking at familiarity-based recognition memory, followed by weight-based priming, and activation-based priming. Well look at more robust forms of activation-based memory, including working memory, in the Executive Function chapter.Probably most people have heard of the hippocampus and its critical role in episodic memory --the movie Memento for example does a great job of portraying what it is like to not have a functional hippocampus. Well find out through our computational models why the hippocampus is so good at episodic memory --it has highly sparse patterns of neural activity, which allows even relatively similar memories to have very different, non-overlapping neural representations. These distinct neural patterns dramatically reduce interference, which is the primary nemesis of memory. Indeed, the highly distributed, overlapping representations in the neocortex --while useful for reasons outlined in the first half of this book --by themselves produce catastrophic interference when they are driven to learn too rapidly. But it is this rapid one-shot learning that is required for episodic memory Instead, it seems that the brain leverages two specialized, complementary learning systems --the hippocampus for rapid encoding of new episodic memories, and the neocortex for slow acquisition of rich webs of semantic knowledge, which benefit considerably from the overlapping distributed learning and slower learning rates, as well see.Countering the seemingly ever-present urge to oversimplify and modularize the brain, it is critical to appreciate that memory is a highly distributed phenomena, with billions of synapses throughout the brain being tweaked by any given experience. Several studies have shown preserved learning of new memories of relatively specific information in people with significant hippocampal damage --but it is critical to consider how these memories are cued. This is an essential aspect to remember about memory in general: whether a given memory can actually be retrieved depends critically on how the system is probed. Weve probably all had the experience of a flood of memories coming back as a result of visiting an old haunt --the myriad of cues available enable recall of memories that otherwise are not quite strong enough to rise to the surface. The memories encoded without the benefit of the hippocampus are weaker and more vague, but they do exist.In addition to being highly distributed, memory in the brain is also highly interactive. Information that is initially encoded in one part of the brain can appear to "spread" to other parts of the brain, if those memories are reactivated and these other brain areas get further opportunities to learn them. A classic example is that episodic memories initially encoded in the hippocampus can be strengthened in the surrounding neocortical areas through repeated retrieval of those memories. This can even happen while we are sleeping, when patterns of memories experienced during the day have shown to be re-activated Furthermore, influences of the prefrontal cortex Executive Function system, and affective states, can significantly influence the encoding and retrieval of memory. Thus, far from the static "hard drive" metaphor from computers, memory in the brain is a highly dynamic, constantly evolving process that reflects the complexity and interactions present across all the areas of the brain. However, a tiny percentage of otherwise seemingly "normal" people have Exceptional memory So what exactly makes the hippocampus such an exceptionally good episodic memory system Our investigation begins with failure. Specifically, the failure of a "generic" cortical neural network model of the sort weve been exploring in this textbook to exhibit any kind of useful episodic memory ability. This failure was first documentedby People are tested on their ability to recall the B associate for each A item, and training on the AB list ends when they achieve perfect recall. Then, they start learning the AC list, which involves new associates for the previous A items:After 1, 5, 10, and 20 iterations of learning this AC list, people are tested on their ability to recall the original AB items, without any additional training on those items. But well see that this kind of whole-sale abandonment of neural networks is unjustified. Indeed, in the following exploration we will see that there are certain network parameters that reduce the levels of interference. The most important manipulation required is to increase the level of inhibition so that fewer neurons are active, which reduces the overlap between the internal representation of the AB and AC list items, thus allowing the system to learn AC without overwriting the prior AB memories. Well then see that the hippocampal system exploits this trick to an extreme degree, making it an exceptionally good episodic memory system.Open the ABAC simulation and follow the directions there.  that they cry out for an explanation, and the vast repertoire of neural recording data from different hippocampal areas. We start with an overview of hippocampal anatomy, followed by the neural recording data and an understanding of how relatively sparse neural activity levels also results in pattern separation, which minimizes interference.The anatomy of the hippocampus proper and the areas that feed into it is shown in The basic episodic memory encoding story in terms of this anatomy goes like this. The high-level summary of everything in the brain is activated in EC, which then drives the DG and CA3 areas via the perforant pathway --the end result of this is a highly sparse, distinct pattern of neural firing in CA3, which represents the main "engram" of the hippocampus. The EC also drives activity in CA1, which has the critical feature of being able to then re-activate this same EC pattern all by itself. These patterns of activity then drive synaptic plasticity in all the interconnected synapses, with the most important being the synaptic connections among CA3 neurons, and the connections between CA3 and CA1. These plastic changes effectively "glue together" the different neurons in the CA3 engram, and associate them with the CA1 invertible pattern, so that subsequent retrieval of the CA3 engram can then activate the CA1, then EC, and back out to the cortex. Thus, the primary function of the hippocampus is to bind together all the disparate elements of an episode, and then be able to retrieve this conjunctive memory and reinstate it out into the cortex during recall. This is how a memory can come "flooding back" --it floods back from CA3 to CA1 to EC to cortex, reactivating something approximating the original brain pattern at the time the memory was encoded.As noted in the introduction, every attempt to simplify and modularize memory in this fashion is inaccurate, and in fact memory encoding is distributed among all the neurons that are active at the time of the episode. For example, learning in the perforant pathway is important for reactivating the CA3 engram from the EC inputs. In addition, learning all the way through the cortical pathways into and out of the hippocampus "greases" the retrieval process. Indeed, if a memory pattern is reactivated frequently, then these cortical connections can be strong enough to drive reactivation of the full memory, without the benefit of the hippocampus at all. We discuss this consolidation process in detail later. Finally, the retrieval process can be enhanced by controlled retrieval of memory using top-down strategies using the prefrontal cortex. We dont consider this aspect of controlled retrieval here, but it depends on a combination of activation and weight based memory analogous to some features we will explore in Executive Function .A representative picture of a critical difference between the hippocampus and cortex is shown in The connection between activity levels and pattern separation can also be observed within the hippocampus itself, by comparing the firing properties of DG vs. CA3 neurons, where DG neurons have the sparsest activity levels, even compared to the somewhat less sparse CA3. Another factor that contributes to effective pattern separation is the broad and diffuse connectivity from EC to DG and CA3, via the perforant pathway. This allows many different features in EC to be randomly combined in DG and CA3, enabling them to be sensitive to combinations or conjunctions of inputs. Because of the high inhibitory threshold associated with sparse activations, this means a given neuron in these areas must receive significant excitation from multiple of these diffuse input sources. In other words, these neurons have conjunctive representations.Pattern separation is important for enabling the hippocampus to rapidly encode novel episodes with a minimum of interference on prior learning, because the patterns of neurons involved overlap relatively little.While pattern separation is important for encoding new memories, this encoding would be useless unless these memories can be subsequently recalled. This recall process is also known as pattern completion, where a partial retrieval cue triggers the completion of the full original pattern associated with the memory. For example, if I cue you with the question: "did you go to summer camp as a kid" you can pattern complete from this to memories of summer camp, or not, as the case may be. The amazing thing about human memory is that it is content addressable memory --any sufficiently specific subset of information can serve as a retrieval cue, enabling recovery of previously-encoded episodic memories. In contrast, memory in a computer is accessed by a memory address or a variable pointer, which has no relationship to the actual content stored in that memory. The modern web search engines like Google demonstrate the importance of content addressability, and function much like the human memory system, taking search terms as retrieval cues to find relevant "memories" with related information. As you probably know from searching the web, the more specific you can make your query, the more likely you will retrieve relevant information --the same principle applies to human memory as well.In the hippocampus, pattern completion is facilitated by the recurrent connections among CA3 neurons, which glues them together during encoding, such that a subset of CA3 neurons can trigger recall of the remainder. In addition, the synaptic changes during encoding in the perforant pathway make it more likely that the original DG and CA3 neurons will become reactivated by a partial retrieval cue.Interestingly, there is a direct tension or tradeoff between pattern separation and pattern completion, and the detailed parameters of the hippocampal anatomy can be seen as optimizing this tradeoff Pattern separation makes it more likely that the system will treat the retrieval cue like a novel stimulus, and thus encode a new distinct engram pattern in CA3, instead of completing to the old one. Likewise, if the system is too good at pattern completion, it will reactivate old memories instead of encoding new pattern separated ones, for truly novel episodes. Although the anatomical parameters in our model do help to find a good balance between these different forces of completion and separation, it is also likely that the hippocampus benefits from strategic influences from other brain areas, e.g., prefrontal cortex executive control areas, to emphasize either completion or separation depending on whether the current demands require recall or encoding, respectively. We will explore this issue further in the Executive Function Chapter.Now, lets explore how the hippocampus encodes and recalls memories, using the AB-AC task. Just click on the following exploration and follow the instructions from there: may not be unique to neural networks --researchers often discount various theories of the mind, including Bayesian models for example, when they dont accord with some pattern of data. The trick is to identify when any given theory is fundamentally flawed given challenging data the devil is in the details, and oftentimes there are ways to reconcile or refine an existing theory without "throwing out the baby with the bathwater".Such musings aside, there are two possible solutions to the catastrophic interference problem. One would be to somehow improve the performance of a generic neural network model in episodic memory tasks, inevitably by reducing overlap in one way or another among the representations that form. The other would be to introduce a specialized episodic memory system, i.e., the hippocampus, which has parameters that are specifically optimized for low-interference rapid learning through pattern separation, while retaining the generic neural network functionality as a model of neocortical learning. The advantage of this latter perspective, known as the complementary learning systems framework Consistent with this basic tradeoff, people with exceptional episodic memory abilities often suffer from a commensurate difficulty with generalizing knowledge across episodes. Even more extreme, autistic memory savants, who can memorize all manner of detailed information on various topics, generally show an even more profound lack of common sense reasoning and general ability to get by in the real world. In these cases, it was speculated that the neocortex also functions much more like a hippocampus, with sparser activity patterns, resulting in overall greater capacity for memorizing specifics, but correspondingly poor abilities to generalize across experiences to produce common sense reasoning Having seen how the intact hippocampus functions, you may be wondering what goes wrong to produce amnesia.The hollywood version of amnesia involves getting hit on the head, followed by a complete forgetting of everything you know. Then of course another good whack restores those memories, but not before many zany hijinks have ensued. In reality, there are many different sources of amnesia, and memory researchers typically focus on the kind that is caused by direct damage to the hippocampus and related structures, More careful studies with HM showed that he could also learn new semantic information, but that this occurred relatively slowly, and the learned knowledge was more brittle in the way it could be accessed, compared to neurologically intact people. This further clarifies that the hippocampus is critical for episodic, but not semantic learning. However, for most people semantic information can be learned initially via the hippocampus, and then more slowly acquired by the neocortex over time. One indication that this process occurs is that HM lost his most recent memories prior to the surgery, more than older memories. Thus, the older memories had somehow become consolidated outside of the hippocampus, suggesting that this gradual process of the neocortex learning information that is initially encoded in the hippocampus, is actually taking place. We discuss this process in the next section.Certain drugs can cause a selective case of anterograde amnesia. For example, the benzodiazepines activate GABA inhibitory neurons throughout the brain, but benzodiazepene receptors are densely expressed in the hippocampus, and because of the high levels of inhibition, it is very sensitive to this. At the right dosage, this inhibition is sufficient to prevent synaptic plasticity from occurring within the hippocampus, to form new memories, but previously-learned memories can still be reactivated. This then gives rise to a more pure case of anterograde, without retrograde, amnesia. Experimentally, midazolam impairs hippocampal-dependent rapid memory encoding but spares other forms of integrative learning such as reinforcement learning Another source of amnesia comes from Korsakoffs syndrome, typically resulting from lack of vitamin B1 due to long-term alcoholism. This apparently affects parts of the thalamus and the mammillary bodies, which in turn influence the hippocampus via various neuromodulatory pathways, including GABA innervation from the medial septum, which can then influence learning and recall dynamics in the hippocampus.Why do we dream Is there something useful happening in our brains while we sleep, or is it just random noise and jumbled nonsensical associations Can you actually learn a foreign language while sleeping Our enduring fascination with the mysteries of sleep and dreaming may explain the excitement surrounding the idea that memories can somehow migrate from the hippocampus to the neocortex while we sleep. This process, known as memory consolidation, was initially motivated by the observation that more recent memories were more likely to be lost when people suffer from acquired amnesia, as in the case of H.M. discussed above. More recently, neural recordings in the hippocampus during wakefulness and sleep have revealed that patterns of activity that occur while a rat is running a maze seem to also be reactivated when the animal is then asleep. However, the measured levels of reactivation are relatively weak compared to the patterns that were active during the actual behavior, so it is not clear how strong of a learning signal could be generated from this. Furthermore, there is considerable controversy over the presence of the temporally-graded retrograde gradients in well-controlled animal studies, raising some doubts about the existence of the consolidation phenomenon in the first place. Nevertheless, on balance it seems safe to conclude that this process does occur at least to some extent, in at least some situations, even if not fully ubiquitous. In humans, slow wave oscillations during non-REM sleep are thought to be associated with memory consolidation.Indeed, one recent study showed that external induction of slow wave oscillations during sleep actually resulted in enhanced subsequent hippocampal-dependent memories for items encoded just prior to sleep A large amount of research on the hippocampus takes place in the rat, and spatial navigation is one of the most important behavioral functions for a rat. Thus, it is perhaps not too surprising that the rat hippocampus exhibits robust place cell firing, where individual DG, CA3 and CA1 neurons respond to a particular location in space. A given neuron will have a different place cell location in different environments, and there does not appear to be any kind of topography or other systematic organization to these place cells. This is consistent with the random, diffuse nature of the perforant pathway projections into these areas, and the effects of pattern separation.More recently, spatial coding in the entorhinal cortex has been discovered, in the form of grid cells. These grid cells form a regular hexagonal lattice or grid over space, and appear to depend on various forms of oscillations. These grid cells may then provide the raw spatial information that gets integrated into the place cells within the hippocampusproper. In addition, head direction cells have been found in a number of different areas that project into the hippocampus, and these cells provide a nice dead reckoning signal about where the rat is facing based on the accumulation of recent movements.The combination of all these cell types provides a solid basis for spatial navigation in the rat, and various computational models have been developed that show how these different signals can work together to support navigation behavior. An exploration model of this domain will be available in a future edition.Figure 8.7: Different areas of the hippocampal system fire out of phase with respect to the overall theta rhythm, producing dynamics that optimize encoding vs.retrieval. We consider the strength of the EC and CA3 inputs to CA1. When the EC input is strong and CA3 is weak, CA1 can learn to encode the EC inputs. This serves as a plus phase for an error-driven learning dynamic in the Leabra framework. When CA3 is strong and EC is weak, the system recalls information driven by prior CA3 - CA1 learning. This serves as a minus phase for Leabra error-driven learning, relative to the plus phase encoding state. information. This is an appealing idea, because as we discussed earlier, there can be a benefit by altering the hippocampal parameters to optimize encoding or retrieval based on various other kinds of demands.The emergent software now supports an extension to this basic theta encoding vs. retrieval idea that enables Leabra error-driven learning to shape two different pathways of learning in the hippocampus, all within one standard trial of processing. We refer to this as quad phase learning, because each pathway has an effective minus and plus phase activation state. The main pathway, trained on the standard minus to plus phase difference, involves CA3-driven recall of the corresponding CA1 activity pattern, which can then reactivate EC and so on out to cortex. The second pathway, trained using a special initial phase of settling within the minus phase, is the CA1 - EC invertible auto-encoder, which ensures that CA1 can actually reactivate the EC if it is correctly recalled. In our standard hippocampal model explored previously, this auto-encoder pathway is trained in advance on all possible sub-patterns within a single subgroup of EC and CA1 units. This new model suggests how this auto-encoder can instead be learned via the theta phase cycle.See Hippocampus Quad Phase for details on this quad phase version of the hippocampus, which is recommended to use for any computationally demanding hippocampal applications.Theta oscillations are also thought to play a critical role in the grid cell activations in the EC layers, and perhaps may also serve to encode temporal sequence information, because place field activity firing shows a theta phase procession, with different place fields firing at different points within the unfolding theta wave. We will cover these topics in greater detail in a subsequent revision.The subiculum is often neglected in theories of hippocampal function, and yet it likely plays various important roles.Anatomically, it is situated in a similar location as the entorhinal cortex relative to the other hippocampal areas, but instead of being interconnected with neocortical areas, it is interconnected more directly with subcortical areas Stepping back now from the specific memory contributions of the hippocampus, we consider a broader perspective of how the hippocampal system fits into the larger space of human memory capacities. One of the most important questions that researchers have focused on here is whether the neocortex can contribute anything at all to single trial episodic memory. Does a single exposure to a given stimulus leave a big enough trace anywhere in the cortex so as to influence overt behavior As noted previously, we feel confident that synapses throughout the brain are likely to be affected by every learning experience, but is neocortical learning simply too slow, and the representations too overlapping, to produce a behaviorally significant change from a single experienceA large body of data suggests that indeed the neocortex can support episodic memory traces, but that they have very different properties compared to those supported by the hippocampus. Specifically, it seems that the perirhinal cortex can produce a useful familiarity signal, that indicates in a very coarse manner whether a given stimulus was experienced recently or not. This familiarity signal can be contrasted with the recollective memory signal provided by the hippocampus: a full explicit recall of the details of the previous episode when the item was last experienced.The familiarity signal is instead more like a single graded value that varies in intensity depending on how strongly familiar the item is. One hypothesis about the neural basis for this signal is the sharpness of the representations in perirhinal cortex --single trials of learning in a generic cortical model leave measurable traces on the overall pattern of neural activity, such that the contrast between strongly active and more weakly active neurons is enhanced Interestingly, people can obtain subjective conscious access to this familiarity signal, and use it to make overt, conscious evaluations of how familiar they think an item is. The neural mechanism for this explicit readout of a sharpness signal has not been identified. This main challenge here is identifying why signals in perirhinal cortex are consciously accessible, while similar such signals in other neocortical areas do not appear to be accessible to consciousness.This combination of hippocampal recall and perirhinal familiarity memory systems is called a dual process model of recognition memory, and after many years of controversy, it is now widely accepted in the field. Some of the data consistent with this dual process model include preserved familiarity signals in people with substantial hippocampal lesions, and a variety of neuroimaging and behavioral studies that have been able to distinguish between these two memory signals in various ways.A subsequent revision will contain a dual-process memory model to explore at this point.Moving further afield from the hippocampus and surrounding cortical areas, can perceptual and other association cortex areas make useful memory contributions based on single or small numbers of exposures The answer here is also in the affirmative, but unlike the familiarity signal, these memory traces remain almost entirely below the radar of conscious awareness --scientists can measure memory effects in terms of various behavioral measures, but we are not subjectively aware of having these memories. The general term for this form of memory is priming, because the main behavioral manifestation is a speedup in reaction time, or an increased probability of making a particular behavioral response --as if the "pump is being primed" by these memory traces. Indeed, we think of the slow incremental neocortical learning effects as doing exactly this pump priming level of tweaking to the underlying neural representations. Only sustained changes over many experiences can truly reshape these more stable neural representations in more dramatic ways. And as we get older, it seems that perhaps the learning rate gets slower, making it even more difficult to fundamentally reshape the most basic neocortical representations.In addition to the subtle effects of slow learning changes, priming can also result from residual activation --neural firing that persists from previously processed information. Thus, we can distinguish between weight-based priming and activation-based priming. As might be expected, activation-based priming is very short-lived, disappearing as soon as the neural firing dissipates. By contrast, weight-based priming can be remarkably persistent, with some cases of priming lasting a year or more, from a single exposure This kind of behavioral result puts strong constraints on the stability of synaptic plasticity --various computational models introduce forms of synaptic weight decay, but this seems inconsistent with the extreme durability of priming, and of our long-term memories more generally.One behavioral paradigm used to reveal priming effects is called stem completion. Here, the first letters of a word are presented, and the participant is asked to complete the stem with the first word that comes to mind. For example, you might see stems like this:win let and respond with words like "window" or "winter", "letter" or "lettuce". The priming effect is revealed by first exposing people to one of the possible words for these stems, often in a fairly disguised, incidental manner, and then comparing how much this influences the subsequent likelihood of completing the stem with it. By randomizing which of the different words people are exposed to, you can isolate the effects of prior exposure relative to whatever baseline preferences people might otherwise have. We know that those priming effects are not due to learning in the hippocampus, because they remain intact in people with hippocampal lesions.Now we can explore both weight-based and activation-based priming on a simple stem-completion like task, using a very generic cortical learning model. WtPriming ActPrimingHere are all the sub-topics within the Memory chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: Hippo Quad Phase --advanced quad phase learning version of the hippocampus.Here are all the explorations covered in the main portion of the Memory chapter: Language involves almost every part of the brain, as covered in other chapters in the text: Perception and attention: language requires the perception of words from auditory sound waves, and written text.Attention is critical for pulling out individual words on the page, and individual speakers in a crowded room. In this chapter, we see how a version of the object recognition model from the perception chapter can perform written word recognition, in a way that specifically leverages the spatial invariance property of this model. Motor control: Language production obviously requires motor output in the form of speech, writing, etc. Fluent speech depends on an intact cerebellum, and the basal ganglia have been implicated in a number of linguistic phenomena. Learning and memory: early word learning likely depends on episodic memory in the hippocampus, while longer-term memory for word meaning depends on slow integrated learning in the cortex. Memory for recent topics of discourse and reading likely involves the hippocampus and sophisticated semantic representations in temporal cortex. Executive Function: language is a complex mental facility that depends critically on coordination and working memory from the prefrontal cortex and basal ganglia --for example encoding syntactic structures over time, pronoun binding, and other more transient forms of memory.One could conclude from this that language is not particularly special, and instead represents a natural specialization of domain general cognitive mechanisms. Of course, people have specialized articulatory apparatus for producing speech sounds, which are not shared by other primate species, but one could argue that everything on top of this is just language infecting pre-existing cognitive brain structures. Certainly reading and writing is too recent to have any evolutionary adaptations to support it.But language is fundamentally different from any other cognitive activity in a number of important ways: Symbols --language requires thought to be reduced to a sequence of symbols, transported across space and time, to be reconstructed in the receivers brain. Syntax --language obeys complex abstract regularities in the ordering of words and lettersphonemes. Temporal extent and complexity --language can unfold over a very long time frame, with a level of complexity and richness conveyed that far exceeds any naturally occurring experiences that might arise outside of the linguistic environment. If you ever find yourself watching a movie on an airplane without the sound, youll appreciate that visual imagery represents the lesser half of most movies content. Generativity --language is "infinite" in the sense that the number of different possible sentences that could be constructed is so large as to be effectively infinite. Language is routinely used to express new ideas. You may find some of those here. Culture --much of our intelligence is imparted through cultural transmission, conveyed through language. Thus, language shapes cognition in the brain in profound ways.The "special" nature of language, and its dependence on domain-general mechanisms, represent two poles in the continuum of approaches taken by different researchers. Within this broad span, there is plenty of room for controversy and contradictory opinions. Noam Chomsky famously and influentially theorized that we are all born with an innate universal grammar, with language learning amounting to discovering the specific parameters of that language instance. On the other extreme, connectionist language modelers such as Jay McClelland argue that completely unstructured, generic neural mechanisms are sufficient for explaining the special things about language.Our overall approach is clearly based in the domain-general approach, given that the same general-purpose neural mechanisms used to explore a wide range of other cognitive phenomena are brought to bear on language here.However, we also think that certain features of the PFC  basal ganglia system play a special role in symbolic, syntactic processing. At present, these special contributions are only briefly touched upon here, and elaborated just a bit more in the executive function chapter, but future plans call for further elaboration. One hint at these special contributions comes from mirror neurons discovered in the frontal cortex of monkeys, in an area thought to be analogous to Brocas area in humans --these neurons appear to encode the intentions of actions performed by other people, and thus may constitute a critical capacity to understand what other people are trying to communicate.We start as usual with a biological grounding to language, in terms of particularly important brain areas and the biology of speech. Then we look at the basic perceptual and motor pathways in the context of reading, including an account of how damage to different areas can give rise to distinctive patterns of acquired dyslexia. We explore a large-scale reading model, based on our object recognition model from the perception chapter, that is capable of pronouncing the roughly 3,000 monosyllabic words in English, and generalizing this knowledge to nonwords. Next, we consider the nature of semantic knowledge, and see how a self-organizing model can encode word meaning in terms of the statistics of word co-occurrence, as developed in the Latent Semantic Analysis model. Finally, we explore syntax at the level of sentences in the Sentence Gestalt model, where syntactic and semantic information are integrated over time to form a "gestalt" like understanding of sentence meaning.   "You know that smoodle pinkered and that I want to get him round and take care of him like you want before", which apparently was intended to mean: "The dog needs to go out so I will take him for a walk".In contrast, a person with damage to Brocas area has difficulty producing syntactically correct speech output, typically producing single content words with some effort, e.g., "dog....walk".The more modern term for Brocas aphasia is expressive aphasia, indicating a primary deficit in expressing speech. Comprehension is typically intact, although interestingly there can be deficits in understanding more syntactically complex sentences. Wernickes aphasia is known as receptive aphasia, indicating a deficit in comprehension, but also expression of meaning. their mouths and other articulatory systems, they cannot perform the complex sequencing of these motor commands that is necessary to produce fluid speech. Interestingly, these higher-order motor control areas also seem to be important for syntactic processing, even for comprehension. This is consistent with the idea that frontal cortex is important for temporally-extended patterning of behavior according to increasingly complex plans as one moves more anterior in frontal cortex.The location of Wernickes area in temporal cortex is sensible, given that we know that the temporal lobe represents the semantic meanings of objects and other things.There are still some controversies about the exact nature of damage required to produce each of these aphasias, but the basic distinction between these broad areas remains quite valid.The Articulatory Apparatus and Phonology   There are three major forms of acquired dyslexia that can be simulated with the model: Phonological --characterized by difficulty reading nonwords. This can be produced by damage to the direct pathway between orthography and phonology, such that people have difficulty mapping spelling to sound according to learned regularities that can be applied to nonwords. Well explore this phenomenon in greater detail in the next simulation. Deep --is a more severe form of phonological dyslexia, with the striking feature that people sometimes make semantic substitutions for words, pronouncing the word "orchestra" as "symphony" for example. There are also visual errors, so-named because they seem to reflect a misperception of the word inputs. Interestingly, well see how more significant damage to the direct pathway can give rise to this profile --the semantic errors occur due to everything going through the semantic layer, such that related semantic representations can be activated. In the normal intact brain, the direct pathway provides the relevant constraints to produce the actual written word, but absent this constraint, an entirely different but semantically related word can be output. Surface --here nonword reading is intact, but access to semantics is impaired, strongly implicating a lesion in the semantics pathway. Interestingly, pronunciation of exception words is impaired. This suggests that people typically rely on the semantic pathway to "memorize" how to pronounce odd words like yacht, and the direct pathway is used more for pronouncing regular words.That these different forms of dyslexia can be reliably observed in different patients, and fit so well with expected We now zoom in on the direct pathway between visual word inputs and verbal speech output, using a much larger set of words comprising most of the monosyllabic words in English. By learning on such a large selection of words, sampled according to their frequency of occurrence in English, the network has a chance to extract the "rules" that govern the mapping between spelling and sound in English, and thus be able to successfully pronounce nonwords.English is a particularly difficult language from a pronunciation perspective, as anyone knows who has tried to acquire it as a second language. There are very few absolute rules.Everything is more of a partial, context-dependent regularity, which is also called a subregularity. , t, w, and z.Another factor that determines how much context is required to pronounce a given letter is the preponderance of multi-letter groups like th, which have a particular regular pronunciation that differs from the individual letters separately. Other examples of these include:wh. One of the most context sensitive set of letters is the ough group, as in though, tough, cough, plough, through, nought, where the pronunciation varies widely.So English is a mess. The constructed word ghoti is a famous example of how crazy it can get. It is pronounced "fish", where the gh is an f sound as in tough, o is an i sound as in women, and ti is a sh sound as in nation.For any system to be able to have any chance of producing correct pronunciation of English, it must be capable of taking into account a range of context around a given letter in a word, all the way up to the entire word itself. An influential early approach to simulating spelling to sound in a neural network We take a different approach in our spelling-to-sound model One compelling demonstration of the importance of spatial invariance in reading comes from this example, which made the rounds in email a few years ago:I cnduot bvleiee taht I culod aulaclty uesdtannrd waht I was rdnaieg.Unisg the icndeblire pweor of the hmuan mnid, aocdcrnig to rseecrah at Cmabrigde Uinervtisy, it dsenot mttaer in waht oderr the lterets in a wrod are, the olny irpoamtnt tihng is taht the frsit and lsat ltteer be in the rhgit pclae. The rset can be a taotl mses and you can sitll raed it whoutit a pboerlm. Tihs is bucseae the huamn mnid deos not raed ervey ltteer by istlef, but the wrod as a wlohe. Aaznmig, huh Yaeh and I awlyas tghhuot slelinpg was ipmorantt See if yuor fdreins can raed tihs too.Clearly this is more effortful than properly spelled text, but the ability to read it at all indicates that just extracting individual letters in an invariant manner goes a long way. Taraban  McClelland 97.9 na 100.0 To test the performance of this object-recognition based approach, we ran it through a set of different standard sets of nonwords, several of which were also used to test the PMSP model. The results are shown in  Glushko regulars --nonwords constructed to match strong regularities, for example nust, which is completely regular. Glushko exceptions --nonwords that have similar English exceptions and conflicting regularities, such as bint. We score these items either according to the predominant regularity, or also including close exceptional cases. McCann  Besner ctrls --these are pseudo-homophones and matched controls, that sound like actual words, but are spelled in a novel way, for example choyce, and the matched control is phoyce. Taraban  McClelland --has frequency matched regular and exception nonwords, for example poes, and mose, like lower frequency pose or lose.The results indicate that the model does a remarkably good job of capturing the performance of peoples performance on these nonword reading sets. This suggests that the model is capable of learning the appropriate regularities and subregularities that are present in the statistics of English pronunciation. Open Spelling to Sound to explore the spelling-to-sound model, and test its performance on both word and nonword stimuli. However, there is also increasing evidence that the anterior tip or "pole" of the temporal lobe plays a particularly important role in representing semantic information, perhaps most importantly for more abstract words that lack a strong sensory or motor correlate. One theory is that this area acts as a central "hub" for coordinating the otherwise distributed semantic information How do we learn the meanings of these more abstract words in the first place Unlike the more concrete words shown in One compelling idea here is that words obtain their meaning in part from the company they keep --the statistics of word co-occurrence across the large volume of verbal input that we are exposed to can actually provide The key result of this SVDPCAHebbian process is to extract the strongest groupings or clusters of words that co-occur together, in a way that integrates over many partially-overlapping subsets of word groups. Thus, even though synonyms do not tend to occur with each other, they do co-occur with many of the same other sets of words, and this whole group of words represents a strong statistical grouping that will be pulled out by the dimensionality reduction  Hebbian self-organizing learning process.This process is exactly the same as what we saw with the V1 receptive field model in the Perception Chapter. In that model, Hebbian learning extracted the statistical regularity of oriented edges from a set of natural images. Any given image typically contains a noisy, partial version of an oriented edge, with perhaps several pixels occluded or blurry or otherwise distorted. However, as the self-organizing learning process integrates over many such inputs, these idiosyncrasies wash away, and the strongest statistical groupings of features emerge as oriented edges.Unlike the V1 model, however, the individual statistical clusters that emerge from the LSA model  do not have any clear interpretation equivalent to "oriented edges". As youll see in the exploration, you can typically make some sense of small subsets of the words, but no obvious overall meaning elements are apparent. But this is not a problem --what really matters is that the overall distributed pattern of activity across the semantic layer appropriately captures the meanings of words. And indeed this turns out to be the case. Open Semantics for the sem.proj exploration of semantic learning of word co-occurrences. The model here was trained on an early draft of the first edition of this textbook, and thus has relatively specialized knowledge, hopefully much of which is now shared by you the reader. Having covered some of the interesting properties of language at the level of individual words, we now take one step higher, to the level of sentences. This step brings us face-to-face with the thorny issue of syntax. The traditional approach to syntax assumes that people assemble something akin to those tree-like syntactic structures you learned in school The notion of a semantically-oriented gestalt representation of a sentence seems appealing, but until an implemented model actually shows that such a thing actually works, it is all just a nice story. The St. These include the roles: agent, experiencer, goal, instrument, patient, source, theme, beneficiary, companion, location, author, possession, subtype, property, if, because, while, and although. This thematic role binding approach is widely used in the natural language processing field for encoding semantics, but it moves away from the notion of an unstructured gestalt representation of semantic meaning. The sentence gestalt model uses a much simpler form of this thematic role training, which seems less controversial in this respect.The sentence gestalt model is trained on a very small toy world, consisting of the following elements: People: busdriver, teacher,, schoolgirl, pitcher. adult, child, someone also used. Actions: eat, drink, stir, spread, kiss, give, hit, throw, drive, rise. Objects: spot, steak, soup, ice  Locations: kitchen, living room, shed, park.The semantic roles used to probe the network during training are Also, as you can see, several of the words are ambiguous so that context must be used to disambiguate.The model is trained on randomly-generated sentences according to a semantic and syntactic grammar that specifies which words tend to co-occur etc. It is then tested on a set of key test sentences to probe its behavior in various ways: Active semantic: The schoolgirl stirred the kool-aid with a spoon. Active syntactic: The busdriver gave the rose to the teacher.. Passive semantic: The jelly was spread by the busdriver with the knife. Passive syntactic: The teacher was kissed by the busdriver. vs. The busdriver kissed the teacher.. Word ambiguity: The busdriver threw the ball in the park., The teacher threw the ball in the living room.. Conflict: The adult drank iced-tea in the kitchen..The model structure projecting up through an encoding hidden layer to the gestalt layer, which is where the distributed representation of sentence meaning develops. The memory for prior words and meaning interpretations of the sentence is encoded via a context layer, which is a copy of the gestalt layer activation state from the previous word input. This context layer is known as a simple recurrent network, and it is widely used in neural network models of temporally extended tasks. The network training comes from repeated probing of the network for the various semantic roles enumerated above. A role input unit is activated, and then the network is trained to activate the appropriate response in the filler output layer.  Open Sentence Gestalt to explore the sentence gestalt model.The primary function of language is to communicate. It is fundamentally about semantics. And semantics represents a major barrier to further progress in language modeling. The sentence gestalt model has very simplistic semantics, and the more advanced version of it developed by Rohde introduces more complex semantics, at the cost of injecting externally more of what the model should be developing on its own. Thus, the fundamental challenge for models of sentence-level or higher-level language is to develop a more naturalistic way of training the corresponding semantics. In an ideal case, a virtual humanoid robot would be wandering around a rich simulated naturalistic environment, and receiving and producing language in order to understand and survive in this environment. This would mimic the way in which people acquire and use language, and would undoubtedly provide considerable insight into the nature of language acquisition and higher-level semantic representations. But clearly this will require a lot of work.Here are all the sub-topics within the Language chapter, collected in one place for easy browsing. These may or may not be optional for a given course, depending on the instructors specifications of what to read: Dyslexia Details --detailed graphs of the effects of partial lesions to the different pathways within the triangle model.Here are all the explorations covered in the main portion of the Language chapter: Dyslexia --Normal and disordered reading and the distributed lexicon. Spelling to Sound --Orthography to Phonology mapping and regularity, frequency effects. Semantics --Semantic Representations from World Co-occurrences and Hebbian Learning. Sentence Gestalt --The Sentence Gestalt model.We have now reached the top of the cognitive neuroscience hierarchy: the "executive" level. In a business, an executive makes important decisions and plans, based on high-level information coming in from all the different divisions of the company, and with a strong consideration of "the bottom line." In a person, the executive level of processing, thought to occur primarily within the prefrontal cortex, similarly receives high-level information from posterior cortical association areas, and is also directly interconnected with motivational and emotional areas that convey "the bottom line" forces that ultimately guide behavior. Although many of us walk around with the impression that our actions are based on rational thought and planning, instead it is highly likely that basic biological motivations and affective signals play a critical role in shaping what we do. At least, this is what the underlying biology of the PFC and associated brain areas suggests. And yet, it is also clear that the PFC is critical for supporting more abstract reasoning and planning abilities, including the ability to ignore distraction and other influences in the pursuit of a given goal. We will try to unravel the mystery of this seemingly contradictory coexistence of abilities in the PFC in this chapter.Evidence for the importance of the PFC in higher-level cognitive control comes from the environmental dependency syndrome associated with damage to PFC. In one classic example, a patient with PFC damage visited a researchers home and, upon seeing the bed, proceeded to get undressed, got into bed, and prepared to sleep. The environmental cues overwhelmed any prior context about what one should do in the home of someone you dont know very well. In other words, without the PFC, behavior is much more reflexive and unthinking, driven by the affordances of the immediate sensory environment, instead of by some more abstract and considered plan or goals. You dont need actual PFC damage to experience this syndrome --certainly you have experienced yourself absent-mindedly doing something cued by the immediate sensory environment that you hadnt otherwise planned to do. We all experience lapses in attention --the classic stereotype of an absent-minded professor is not explained by lack of PFC in professors, but rather that the PFC is apparently working on something else and thus leaves the rest of the brain to fend for itself in an environmentally-dependent manner.Another great source of insight into the cognitive contributions of the PFC is available to each of us every night, in the form of our dreams. It turns out that the PFC is one of the brain areas most inactivated during dreaming phases of sleep. As a result, our dreams often lack continuity, and seem to jump from one disconnected scene to another, with only the most tangential thread connecting them. For example, one moment you might be reliving a tense social situation from high school, and the next youre trying to find out when the airplane is supposed to leave, with a feeling of general dread that youre hopelessly late for it.So what makes the PFC uniquely capable of serving as the brains executive Part of the answer is its connectivity, as alluded to above --it sits on top of the overall information processing hierarchy of the brain, and thus receives highly-processed "status reports" about everything important going on in your brain. In this sense it is similar to the hippocampus as we saw in the Memory Chapter, and indeed these areas appear to work together. However, the PFC is also especially well placed to exert control over our actions --the PFC is just in front of the frontal motor areas, and has extensive connectivity to drive overt motor behavior. Furthermore, the medial and ventral areas of PFC are directly interconnected with affective processing areas in subcortical regions such as the amygdala, thus enabling it to be driven by, and reciprocally, to amplify or override, motivational and affective signals.In addition to being in the right place, the PFC also has some special biological properties that enable it to hold onto information in the face of distraction, e.g., from incoming sensory signals. Thus, with an intact PFC, you can resist the idea of laying down in someone elses bed, and remain focused on the purpose of your visit. We refer to this ability as robust active maintenance because it depends on the ability to keep a population of neurons actively firing over the duration needed to maintain a goal or other relevant pieces of information. This ability is also referred to as working memory, but this latter term has been used in many different ways in the literature, so we are careful to define it as synonymous with robust active maintenance of information in the PFC, in this context. We will see later how active maintenance works together with a gating system that allows us to hold in mind more than one item at a time, to selectively update and manipulate some information while continuing to maintain others, in a way that makes the integrated system support more sophisticated forms of working memory.Recordings of neurons in the PFC of monkeys in the 1970s showed that they exhibit this robust active firing over delays. One of the most widely-used tasks is the oculomotor delayed response task, where a stimulus is flashed in a particular location of a video display, but the monkey is trained to maintain its eyes focused on a central fixation cross until that cross goes off, at which point it must then move its eyes to the previously flashed location in order to receive a juice reward. Neurons in the frontal eye fields show robust delay-period firing that is tuned to the location of the stimulus, and this activity terminates just after the monkey correctly moves its eyes after the delay. There are many other demonstrations of this robust active maintenance in the PFC of humans as well.The computational models we explore in this chapter show how these two factors of connectivity and robust active maintenance can combine to support a wide range of executive function abilities that have been attributed to the PFC. The goal is to provide a unifying model of executive function, as compared to a laundry list of cognitive abilities that it is thought to support.One of the most important executive function abilities is the ability to rapidly shift behavior or thought in a strategic manner. This dynamic gating function is identical to the role the BG plays in gating motor actions, as we saw in the MotorChapter. Furthermore, the BG learning process is also identical to that in the Motor chapter based on reinforcement learning principles. Specifically, dopamine shapes BG learning and thereby enables the gating mechanism to deal with the challenging problem of deciding what is important to maintain, vs. what can be ignored. These mechanisms embody the general notion that the PFC-BG cognitive system evolved by leveraging existing powerful mechanisms for gating motor behavior and learning. From The Basal Ganglia, which consists principally of the striatum, globus pallidus, and subthalamic nucleus, is densely interconnected with the PFC by way of specific nuclei of the thalamus The final major component of the executive control system consists of the substantia nigra pars compacta and several other associated brain areas that together drive phasic dopamine neuromodulation of the BG, resulting in reinforcement learning of its gating actions. This system, summarized computationally using the PVLV model as described in Motor Control and Reinforcement Learning, interacts with the active maintenance of information in PFC to be able to reinforce a gating signal in the BG that leads to subsequent good performance and reward later in time. This time-travel property of the phasic DA reinforcement learning is essential for training a system that maintains information over time.In the following subsections, we summarize the biological properties of each of these systems and their relevance to executive function. The prefrontal cortex basal ganglia working memory model then integrates all of these elements into a functioning computational model that can perform complex executive function tasks, as we explore in the remainder of the chapter. The ability of PFC neurons to exhibit sustained active firing over delays, as initially discovered by Functionally we can characterize the lateral areas as being important for "cold" cognitive control, while the medial areas are important for "hot" emotional and motivational processing The functional significance of the dorsal vs. ventral distinction has been considerably more controversial in the literature, but anatomically it is clear that dorsal PFC areas interconnect more with the dorsal pathway in the posterior cortex, while ventral PFC interconnects with the ventral posterior cortex pathway. As we saw in the Perception Chapter, the dorsal pathway in posterior cortex is specialized for perception-for-action:extracting perceptual signals to drive motor control, while the ventral pathway is specialized for perception-for-identification. This functional specialization in posterior cortex can be carried forward to the associated dorsal and ventral areas of PFC On the medial side, the dorsal medial PFC is also known as the anterior cingulate cortex, which has been shown to encode the affective aspects of motor control variables, which is consistent with a "hot how" functional specialization. Dorsomedial PFC areas also project to the subthalamic nucleus within the BG, and serve to delay motor responding to prevent impulsive choice under difficult response selection demands In PFC and other areas, neurons within a microcolumn tend to encode very similar information, and may be considered equivalent to a single rate-coded neuron of the sort that we typically use in our models. We can then consider an individual stripe as containing roughly 100 such rate-coded neuron-equivalents, which provides sufficient room to encode a reasonably large number of different things using sparse distributed representations across microcolumns.Functionally, we hypothesize in the PBWM model that each stripe can be independently updated by a corresponding stripe-wise loop of connectivity with an associated stripe of neurons through the BG system. This allows for very fine-grained control by the BG over the updating and maintenance of information in PFC, as we describe next.Basal Ganglia and Dynamic Gating has separate Go and NoGo neurons that project to a combined SNr and thalamus layer with a single neuron per stripe that fires if the Go pathway is sufficiently stronger than the NoGo. A SNrThal Go signal will update the PFC gc.h currents to reflect the current input activations, while a NoGo leaves the PFC alone to continue to maintain prior information. properly, the BG has to already fire Go to update new information into PFC, and this information must be maintained through to the time of reward. Learning can only reinforce the "good" actions that take place randomly initially. As the figure shows, if the BG does fire Go to a particular stimulus input, and thus the PFC encodes this new stimulus, and this PFC representation was previously associated with positive reward value by the LV component of PVLV, then the LV will drive a phasic dopamine burst that will reinforce the BG Go firing, causing it to be more likely to occur in the future. This positive feedback dynamic, operating over time with lots of trial and error, enables the system to learn reasonably complex executive function tasks, as well see.There is also a negative feedback dynamic for reinforcing BG NoGo firing for PFC representations that are associated with more negative reward values by the LV system The presence of multiple stripes is typically important for the PBWM model to learn rapidly, because it allows different gating strategies to be explored in parallel, instead of having a single stripe sequentially explore all the different such strategies. As long as one stripe can hit upon a useful gating strategy, the system can succeed, and it quickly learns to focus on that useful stripe while ignoring the others.. One interesting consequence of having these multiple stripes is that "superstitious" gating can occur in other stripes --if that gating happens to reliably enough coincide with the gating signals that are actually useful, it too will get reinforced. Perhaps this may shed light on our proclivity for being superstitiousAs we saw in For more PBWM details, including further considerations for output gating, how maintained information is cleared when no longer needed, and gating biases that can help improve learning, see PBWM detailsSubtopic, which also includes relevant equations and default parameters. We now turn to a series of computer simulations to explore various facets of executive function. We begin with perhaps the single most studied task used to test for executive function, the Stroop task, named after John Ridley Stroop, who first described the basic phenomenon back in 1935 In the Stroop paradigm Sometimes the word "red" appears in green ink, which represents the incongruent or conflict condition. The "Stroop effect" is that error rates and response times are larger for this incongruent condition, especially in the case of color naming The developmental process can provide important insights into various cognitive phenomena, often by making cognitive failures particularly stark. A great example of this is the A-not-B task developed by pioneering developmental researcher Jean Piaget The computational model we explore here To see how this all plays out, open the A Not B model and follow the directions from there.Having seen in the Stroop and A-not-B models how sustained PFC activity can influence behavior through top-down biasing, we now turn to the more complex aspects of PFC function, involving the dynamic gating of PFC representations by the basal ganglia, and the ability to rapidly update and robustly maintain information. As a first introduction to this functionality, captured by the PBWM model, we use the simple SIRtask. Here is a sample sequence of trials in this task: S -A --this means that the network should store the A stimulus for later recall --network responds A. I -C --ignore the C stimulus, but you still have to respond to it --network responds C. I -B --ignore the B stimulus --network responds B. R --recall the most recently stored stimulus --network responds A.The BG maintenance gating system has to learn to fire Go to drive updating of PFC on the Store trials to encode the associated stimulus for later recall. It also must learn to fire NoGo to the ignore stimuli, so they dont overwrite previously stored information. Finally, on recall trials, the output BG gating mechanism should drive output of the stored information from PFC. It is critical to appreciate that the network starts out knowing nothing about the semantics of these various inputs, and has to learn entirely through trial-and-error what to do with the different inputs.. More generally, we refer to the higher level rule as a "task-set" which contextualizes how to act in response to many different stimuli. Hierarchical PFC-BG networks can learn to create these PFC task-sets, and simultaneously, which actions to select in each task-set. They can enhance learning and generalization across contexts which share the same task-set, while also identifying when it may not be appropriate to re-use an existing task-set and instead to create a new one. To see this network in action, including demonstrations of enhanced transfer, see the Collins  Frank network linked here To put many of the elements explored above to their most important use, we explore how the coordinated interactions of various regions of the PFC, together with BG gating, enable the system to behave in a coherent, task-driven manner over multiple sequential steps of cognitive processing. This is really the hallmark of human intelligence: we can solve complex problems by performing a sequence of simpler cognitive steps, in a flexible, adaptive manner. More abstract cognitive models such as ACT-R provide a nice characterization of the functional properties of this level of cognition. The goal with the model we explore here is to understand how more detailed neural mechanisms can work together to produce this functionality.  Higher levels of PFC encode contextgoalsplans to organize sequence of cognitive actions, which are driven by more lower, more posterior PFC areas. Critically, these higher areas do not specify rigid sequences of actions, but rather encode the desired outcome states of the sequence of actions, and provide appropriate context so that appropriate lower-level steps will be selected.  Each step in a sequence of actions involves a consideration of the reward outcomes and effort costs of the action relative to other possible options.TODO: invent this model is important for encoding the affective value of motor actions and plans Here, we explore a model of these areas in the context of relatively simple conditioning tasks in animals.TODO: Whip model, BG-OFC. ACC-STN model in cognitive control.The main models explored above are intended to cover some of the most central and important aspects of executive function, but this is a very large space and there are many important phenomena that we unfortunately cannot cover.For many people, particularly in an academic setting, the first things that may come to mind if asked to name some higher-level cognitive functions might be things like: learning andor using formal mathematics or, perhaps, the use of careful logical reasoning to make a major decision. But, in addition to these highly formalized domains, there are many other day-to-day, but none the less important, mental activities that also involve a highly sophisticated level of processing, activities like: planning ones day or a work project, or resisting the temptation to have dessert when you are trying to lose ten pounds before bathing suit season, or counting cards in working memory while playing blackjack. All these kinds of mental activities are now known to rely upon the frontal cortex and related structures for their optimal expression. Here is a list of some major categories of distinctive executive functions: Highly structured cognitive activities, often involving formal symbol systems --Mental activities like learning andor using mathematics, formal logic, computer programming, creative andor non-fiction writing, and structured, rational decision-making. All of these require temporally-extended maintenance of task-relevant information, especially of a highly abstract, symbolic nature. The role of language in these and many other executive functions is a very important aspect --language provides a highly flexible mental currency for active maintenance and control over behavior --by remembering specific words or phrases, we can remind ourselves of what we want to achieve, or what we have derived in an initial processing step, etc. Control over encoding and retrieval of episodic information in the hippocampus --it is highly likely that the hippocampus and PFCBG systems interact significantly in many forms of executive function, with the rapid learning abilities of the hippocampus complementing the transient, flexible active maintenance properties of the PFC. If the PFC gets distracted, the information is typically gone forever, but the hippocampus can encode and retrieve information in terms of long-lasting synaptic changes. Often, it may be more efficient to use this hippocampal encoding and retrieval instead of persistent active maintenance of information in PFC. a visuospatial scratchpad for spatial information. Another highly influential theoretical approach came from Tim Motivated largely by the kinds of cognitive functions listed above, traditional AI has largely focused on a design-oriented approach using symbols that has focused on trying to figure out what it would take to solve a particular kind of problem, and then designing a model that does things that way. There is an irony in this approach in that researchers taking this approach are using the very higher-level cognitive functionality they are trying to explain in order to design a system that will reproduce it. A fundamental problem with this kind of approach is that it basically designs in the very functionality it aims to explain. This is not to say that these kinds of approaches are wholly without merit, only that they are fundamentally limited in what they can ultimately explain. Perhaps for obvious reasons, it has turned out that these kinds of models of cognitive function have been most successful in dealing with the kinds of cognitive function that we listed as being at the highest level -that is, in modeling systems able to do formal mathematics and logic. What they have done less well in has been in accounting for many of the kinds of things that might be considered less high-level, or even lower-level, things which we often take to be automatic. It is for these latter areas, that the biologically informed neural network approach has been most helpful.Thus, these two approaches can be nicely complementary and hybrid approaches are being pursued. For example, the Leabra approach is being hybridized with the ACT-R approach in an architecture called SAL.All of these approaches are not mutually exclusive, but instead share many common ideas and can be complementary in many ways. In particular, the traditional AI approach, by going straight to solving a high level problem e.g., arithmetic. On the other hand, the goal of the neural network approach we advocate is to provide a more bottom-up model that tries to provide a reductionist account for the emergence of control-like processing based on underlying automatic mechanisms. This is the approach we take with the PBWM framework.  The prefrontal cortex encodes information in an active state through sustained neural firing, which is more flexible and rapidly updatable than using synaptic weight changes.  The basal ganglia drives updating of PFC active memory states, enhancing flexibility.  Phasic dopamine signals from midbrain nuclei have the right properties for training BG gating, by transferring reward associations earlier in time to the onset of stimuli that predict subsequent rewards.  The PFC influences cognitive processing elsewhere in the brain via top-down excitatory biasing, as demonstrated in the Stroop model.  Developmental changes in active memory can be explained in terms of stronger PFC active maintenance abilities, as demonstrated in the A-not-B model.  BG dynamic gating can support flexible cognitive function by dynamically encoding some information while ignore other irrelevant information, and updating the contents of active memory. The SIR and n-back models demonstrate these abilities. convey affective information about stimuli and actions, respectively, and are important for properly evaluating potential actions to be taken.