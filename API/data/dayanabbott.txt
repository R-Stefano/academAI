Copyright ElsevierAll rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means without permission in writing from the publisher.MIT Press books may be purchased at special quantity discounts for business or sales promotional use. For information, please email special salesmitpress.mit.edu or write to Special Sales Department, The Theoretical analysis and computational modeling are important tools for characterizing what nervous systems do, determining how they function, and understanding why they operate in particular ways. Neuroscience encompasses approaches ranging from molecular and cellular studies to human psychophysics and psychology. Theoretical neuroscience encourages crosstalk among these subdisciplines by constructing compact representations of what has been learned, building bridges between different levels of description, and identifying unifying concepts and principles. In this book, we present the basic methods used for these purposes and discuss examples in which theoretical approaches have yielded insight into nervous system function.The questions what, how, and why are addressed by descriptive, mechanistic, and interpretive models, each of which we discuss in the following chapters. Descriptive models summarize large amounts of experimental descriptive models data compactly yet accurately, thereby characterizing what neurons and neural circuits do. These models may be based loosely on biophysical, anatomical, and physiological findings, but their primary purpose is to describe phenomena, not to explain them. Mechanistic models, on the mechanistic models other hand, address the question of how nervous systems operate on the basis of known anatomy, physiology, and circuitry. Such models often form a bridge between descriptive models couched at different levels. Interpretive models use computational and information-theoretic principles interpretive models to explore the behavioral and cognitive significance of various aspects of nervous system function, addressing the question of why nervous systems operate as they do.It is often difficult to identify the appropriate level of modeling for a particular problem. A frequent mistake is to assume that a more detailed model is necessarily superior. Because models act as bridges between levels of understanding, they must be detailed enough to make contact with the lower level yet simple enough to provide clear results at the higher level.This book is organized into three parts on the basis of general themes. Part I, Neural Encoding and Decoding, is devoted to the coding of information by action potentials and the representation of inxiv Preface formation by populations of neurons with selective responses. Modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics is presented in part II, Neurons and Neural Circuits. The role of plasticity in development and learning is discussed in part III, Adaptation and Learning. With the exception of chapters 5 and 6, which jointly cover neuronal modeling, the chapters are largely independent and can be selected and ordered in a variety of ways for a one-or two-semester course at either the undergraduate or the graduate level.Although we provide some background material, readers without previous exposure to neuroscience should refer to a neuroscience textbook such background as Theoretical neuroscience is based on the belief that methods of mathematics, physics, and computer science can provide important insights into nervous system function. Unfortunately, mathematics can sometimes seem more of an obstacle than an aid to understanding. We have not hesitated to employ the level of analysis needed to be precise and rigorous. At times, this may stretch the tolerance of some of our readers. We encourage such readers to consult the Mathematical Appendix, which provides a brief review of most of the mathematical methods used in the text, but also to persevere and attempt to understand the implications and consequences of a difficult derivation even if its steps are unclear.Theoretical neuroscience, like any skill, can be mastered only with practice. Exercises are provided for this purpose on the web site for this book, http:mitpress.mit.edudayan-abbott. We urge the reader to exercises do them. In addition, it will be highly instructive for the reader to construct the models discussed in the text and explore their properties beyond what we have been able to do in the available space.In order to maintain the flow of the text, we have kept citations within the chapters to a minimum. Each chapter ends with an annotated bibliography containing suggestions for further reading, information about works cited within the chapter, and references to related studies. We concentrate on introducing the basic tools of computational neuroscience and discussing applications that we think best help the reader to understand and appreciate them. This means that a number of systems where computational approaches have been applied with significant success are not discussed. References given in the annotated bibliographies lead the reader toward such applications. Many people have contributed significantly to the areas we cover. The books and review articles in the annotated bibliographies provide more comprehensive references to work that we have failed to cite.The link between stimulus and response can be studied from two opposite points of view. Neural encoding, the subject of chapters 1 and 2, refers to the map from stimulus to response. For example, we can catalog how neurons respond to a wide variety of stimuli, and then construct models that attempt to predict responses to other stimuli. Neural decoding refers to the reverse map, from response to stimulus, and the challenge is to reconstruct a stimulus, or certain aspects of that stimulus, from the spike sequences it evokes. Neural decoding is discussed in chapter 3. In chapter 4, we consider how the amount of information encoded by sequences of action potentials can be quantified and maximized. Before embarking on this tour of neural coding, we briefly review how neurons generate their responses and discuss how neural activity is recorded. The biophysical mechanisms underlying neural responses and action potential generation are treated in greater detail in chapters 5 and 6.Neurons are highly specialized for generating electrical signals in response to chemical and other inputs, and transmitting them to other cells. Some important morphological specializations, seen in figure 1.1, are the dendrites that receive inputs from other neurons and the axon that carries the neuronal output to other cells. The elaborate branching structure of the dendritic tree allows a neuron to receive inputs from many other neurons through synaptic connections. The cortical pyramidal neuron of figure 1.1A and the cortical interneuron of figure 1.1C each receive thousands of synaptic inputs, and for the cerebellar Purkinje cell of figure 1.1B the number is over 100,000. Along with these morphological features, neurons have physiological specializations. Most prominent among these are a wide variety of membrane-spanning ion channels that allow ions, predominantly sodium, potassium, calcium, and chloride, to move into ion channels and out of the cell. Ion channels control the flow of ions across the cell membrane by opening and closing in response to voltage changes and to both internal and external signals.The electrical signal of relevance to the nervous system is the difference in electrical potential between the interior of a neuron and the surrounding extracellular medium. Under resting conditions, the potential inside the cell membrane of a neuron is about -70 mV relative to that of the surrounding bath, and the cell is said to be polarized. Ion pumps located in the cell membrane maintain membrane potential concentration gradients that support this membrane potential difference. For example, Na  is much more concentrated outside a neuron than inside it, and the concentration of K  is significantly higher inside the neuron than in the extracellular medium. Ions thus flow into and out of a cell due to both voltage and concentration gradients. Current in the form of positively charged ions flowing out of the cell through open channels makes the membrane potential more negative, a process called hyperpolarization. Current flowing hyperpolarization and depolarization into the cell changes the membrane potential to less negative or even positive values. This is called depolarization.If a neuron is depolarized sufficiently to raise the membrane potential above a threshold level, a positive feedback process is initiated, and the neuron generates an action potential. An action potential is a roughly 100 action potential mV fluctuation in the electrical potential across the cell membrane that lasts for about 1 ms. Action potential generation also depends on the recent history of cell firing. For a few milliseconds just after an action potential has been fired, it may be virtually impossible to initiate another spike. This is called the absolute refractory period. For a longer interval known as the relative refractory period, lasting up to tens of milrefractory period liseconds after a spike, it is more difficult to evoke an action potential.  Axons terminate at synapses where the voltage transient of the action potential opens ion channels, producing an influx of Ca 2 that leads to synapse the release of a neurotransmitter. The neurotransmitter binds to receptors at the signal-receiving or postsynaptic side of the synapse, It is filled with synaptic vesicles containing the neurotransmitter that is released when an action potential arrives from the presynaptic neuron. Transmitter crosses the synaptic cleft and binds to receptors on the dendritic spine, a process roughly 1 m long that extends from the dendrite of the postsynaptic neuron. Excitatory synapses onto cortical pyramidal cells form on dendritic spines as shown here.Other synapses form directly on the dendrites, axon, or soma of the postsynaptic neuron. The middle trace in figure 1.3 illustrates an idealized, noise-free extracellular recording. Here an electrode is placed near a neuron but it does not penetrate the cell membrane. Such recordings can reveal the action potenextracellular electrodes tials fired by a neuron, but not its subthreshold membrane potentials. Extracellular recordings are typically used for in vivo experiments, especially those involving behaving animals. Intracellular recordings are sometimes made in vivo, but this is difficult to do. Intracellular recording is more commonly used for in vitro preparations, such as slices of neural tissue. The responses studied in this chapter are action potential sequences that can be recorded either intra-or extracellularly.Characterizing the relationship between stimulus and response is difficult because neuronal responses are complex and variable. Neurons typically respond by producing complex spike sequences that reflect both the intrinsic dynamics of the neuron and the temporal characteristics of the stimulus. Isolating features of the response that encode changes in the stimulus can be difficult, especially if the time scale for these changes is of the same order as the average interval between spikes. Neural responses can vary from trial to trial even when the same stimulus is presented repeatedly. There are many potential sources of this variability, including variable levels of arousal and attention, randomness associated with various biophysical processes that affect neuronal firing, and the effects of other cognitive processes taking place during a trial. The complexity and trial-to-trial variability of action potential sequences make it unlikely that we can describe and predict the timing of each spike deterministically. Instead, we seek a model that can account for the probabilities that different spike sequences are evoked by a specific stimulus.Typically, many neurons respond to a given stimulus, and stimulus features are therefore encoded by the activities of large neural populations. In studying population coding, we must examine not only the firing patterns of individual neurons but also the relationships of these firing patterns to each other across the population of responding cells.In this chapter, we introduce the firing rate and spike-train correlation functions, which are basic measures of spiking probability and statistics. We also discuss spike-triggered averaging, a method for relating action potentials to the stimulus that evoked them. Finally, we present basic stochastic descriptions of spike generation, the homogeneous and inhomogeneous Poisson models, and discuss a simple model of neural responses to which they lead. In chapter 2, we continue our discussion of neural encoding by showing how reverse-correlation methods are used to construct estimates of firing rates in response to time-varying stimuli. These methods have been applied extensively to neural responses in the retina, lateral geniculate nucleus of the thalamus, and primary visual cortex, and we review the resulting models.Action potentials convey information through their timing. Although action potentials can vary somewhat in duration, amplitude, and shape, they are typically treated as identical stereotyped events in neural encoding studies. If we ignore the brief duration of an action potential, an action potential sequence can be characterized simply by a list of the times when spikes occurred. For n spikes, we denote these times by t i with i  1, 2, . . . , n. The trial during which the spikes are recorded is taken to start at time 0 and end at time T, so 0  t i  T for all i. The spike sequence can also be represented as a sum of infinitesimally narrow, idealized spikes in the form of Dirac  functions,   n i1  .We call  the neural response function and use it to re-express sums neural response function  over spikes as integrals over time. For example, for any well-behaved function h, we can write Because the sequence of action potentials generated by a given stimulus varies from trial to trial, neuronal responses are typically treated statistically or probabilistically. For example, they may be characterized by firing rates, rather than as specific spike sequences. Unfortunately, the term "firing rate" is applied conventionally to a number of different quantities. The simplest of these is what we call the spike-count rate, which is obtained by counting the number of action potentials that appear during a trial and dividing by the duration of the trial. We denote the spike-count rate by r, where spike-count rate r r  n T  1 T T 0 d  .The second equality follows from the fact that d   n and indicates that the spike-count rate is the time average of the neural response function over the duration of the trial.The spike-count rate can be determined from a single trial, but at the expense of losing all temporal resolution about variations in the neural response during the course of the trial. A time-dependent firing rate can be defined by counting spikes over short time intervals, but this can no longer be computed from a single trial. For example, we can define the firing rate at time t during a trial by counting all the spikes that occurred between times t and t  t, for some small interval t, and dividing this count by t. However, for small t, which allows for high temporal resolution, the result of the spike count on any given trial is apt to be either 0 or 1, giving only two possible firing-rate values. The solution to this problem is to average over multiple trials. Thus, we define the time-dependent firing rate as the average number of spikes appearing during a short interval between times t and t  t, divided by the duration of the interval.The number of spikes occurring between times t and t  t on a single trial is the integral of the neural response function over that time interval. The average number of spikes during this interval is the integral of the trial-averaged neural response function. We use angle brackets, , to trial average denote averages over trials that use the same stimulus, so that z for any quantity z is the sum of the values of z obtained from many trials involving the same stimulus, divided by the number of trials. The trial-averaged neural response function is denoted by  , and the time-dependent firing rate is given by firing rate r r  1 t t t t d  .We use the notation r for this important quantity, and when we use the term "firing rate" without any modifiers, we mean r. Formally, the limit t  0 should be taken on the right side of this expression, but, in extracting a time-dependent firing rate from data, the value of t must be large enough so there are sufficient numbers of spikes within the interval defining r to obtain a reliable estimate of the average.For sufficiently small t, r t is the average number of spikes occurring between times t and t  t over multiple trials. The average number of spikes over a longer time interval is given by the integral of r over that interval. If t is small, there will never be more than one spike within the interval between t and t  t on any given trial. This means that r t is also the fraction of trials on which a spike occurred between those times. Equivalently, r t is the probability that a spike occurs during this time interval. This probabilistic interpretation provides a formal definition of spiking probability the time-dependent firing rate r t is the probability of a spike occurring during a short interval of duration t around the time t.In any integral expression such as equation 1.2, the neural response function generates a contribution whenever a spike occurs. If we use the trialaverage response function instead, as in equation 1.5, this generates contributions proportional to the fraction of trials on which a spike occurred. Because of the relationship between this fraction and the firing rate, we can replace the trial-averaged neural response function with the firing rate r within any well-behaved integral, for example, d h   d hr In the same way that the response function  can be averaged across trials to give the firing rate r, the spike-count firing rate can be averaged over trials, yielding a quantity that we refer to as the average firing rate. This is denoted by r and is given by average firing rate rThe first equality indicates that r is just the average number of spikes per trial divided by the trial duration. The third equality follows from the equivalence of the firing rate and the trial-averaged neural response function within integrals. The average firing rate is equal to both the time average of r and the trial average of the spike-count rate r. Of course, a spike-count rate and average firing rate can be defined by counting spikes over any time period, not necessarily the entire duration of a trial.. The difference between the fonts is rather subtle, but the context should make it clear which rate is being used.The firing rate r cannot be determined exactly from the limited data available from a finite number of trials. In addition, there is no unique way to approximate r. A discussion of the different methods allows us to introduce the concept of a linear filter and kernel that will be used extensively in the following chapters. We illustrate these methods by extracting firing rates from a single trial, but more accurate results could be obtained by averaging over multiple trials. Counting spikes in preassigned bins produces a firing-rate estimate that depends not only on the size of the time bins but also on their placement. To avoid the arbitrariness in the placement of bins, we can instead take a single bin or window of duration t and slide it along the spike train, counting the number of spikes within the window at each location. The jagged curve in figure 1.4C shows the result of sliding a 100 ms wide window along the spike train. The firing rate approximated in this way can be expressed as the sum of a window function over the times t i for i  1, 2, . . . , n when the n spikes in a particular sequence occurred, r approx  n i1 w , Use of a sliding window avoids the arbitrariness of bin placement and produces a rate that might appear to have a better temporal resolution. However, it must be remembered that the rates obtained at times separated by less than one bin width are correlated because they involve some of the same spikes.The sum in equation 1.8 can also be written as the integral of the window function times the neural response function:r approx    d w .The integral in equation 1.10 is called a linear filter, and the window funclinear filter and kernel tion w, also called the filter kernel, specifies how the neural response function evaluated at time t   contributes to the firing rate approximated at time t.The jagged appearance of the curve in figure 1.4C is caused by the discontinuous shape of the window function used. An approximate firing rate can be computed using virtually any window function w that goes to 0 outside a region near   0, provided that its time integral is equal to 1. For example, instead of the rectangular window function used in figure 1.4C, w can be a Gaussian:w .In this case,  w controls the temporal resolution of the resulting rate, playing a role analogous to t. A continuous window function like the Gaussian used in equation 1.8 generates a firing-rate estimate that is a smooth function of time.Both the rectangular and the Gaussian window functions approximate the firing rate at any time, using spikes fired both before and after that time.A postsynaptic neuron monitoring the spike train of a presynaptic cell has access only to spikes that have previously occurred. An approximation of the firing rate at time t that depends only on spikes fired before t can be calculated using a window function that vanishes when its argument is negative.  Neuronal responses typically depend on many different properties of a stimulus. In this chapter, we characterize responses of neurons as functions of just one of the stimulus attributes to which they may be sensitive.The value of this single attribute is denoted by s. In chapter 2, we consider more complete stimulus characterizations.A simple way of characterizing the response of a neuron is to count the number of action potentials fired during the presentation of a stimulus. This approach is most appropriate if the parameter s characterizing the stimulus is held constant over the trial. If we average the number of action potentials fired over trials and divide by the trial duration, we obtain the average firing rate, r , defined in equation 1.7. The average firing rate written as a function of s, r  f, is called the neural response tuning curve. The functional form of a tunresponse tuning curve f Response tuning curves can be used to characterize the selectivities of neurons in visual and other sensory areas to a variety of stimulus parameters. Tuning curves can also be measured for neurons in motor areas, in which case the average firing rate is expressed as a function of one or more parameters describing a motor action.  Retinal disparity is a difference in the retinal location of an image between the two eyes. Some neurons in area V1 are sensitive to disparity, representing an early stage in the representation of viewing distance. In Tuning curves allow us to predict the average firing rate, but they do not describe how the spike-count firing rate r varies about its mean value The image from the fixation point falls at the fovea in each eye, the small pit where the black lines meet the retina. The image from a nearer object falls to the left of the fovea in the left eye and to the right of the fovea in the right eye. For objects farther away than the fixation point, this would be reversed. The disparity angle s is indicated in the Response variability extends beyond the level of spike counts to the entire temporal pattern of action potentials. Later in this chapter, we discuss a model of the neuronal response that uses a stochastic spike generator to produce response variability. This approach takes a deterministic estimate of the firing rate, r est, and produces a stochastic spiking pattern from it. The spike generator produces variable numbers and patterns of action potentials, even if the same estimated firing rate is used on each trial. eraging the stimuli that produce a given response. To average stimuli in this way, we need to specify what fixed response we will use to "trigger" the average. The most obvious choice is the firing of an action potential. Thus, we ask, "What, on average, did the stimulus do before an action potential was fired" The resulting quantity, called the spike-triggered average stimulus, provides a useful way of characterizing neuronal selectivity. Spike-triggered averages are computed using stimuli characterized by a parameter s that varies over time. Before beginning our discussion of spike triggering, we describe some features of such stimuli.Neurons responding to sensory stimuli face the difficult task of encoding parameters that can vary over an enormous dynamic range. For example, photoreceptors in the retina can respond to single photons or can operate in bright light with an influx of millions of photons per second. To deal with such wide-ranging stimuli, sensory neurons often respond most strongly to rapid changes in stimulus properties and are relatively insensitive to steady-state levels. Steady-state responses are highly compressed functions of stimulus intensity, typically with logarithmic or weak powerlaw dependences. This compression has an interesting psychophysical correlate. Weber measured how different the intensity of two stimuli had to be for them to be reliably discriminated, the "just noticeable" difference s. He found that, for a given stimulus, s is proportional to the magnitude of the stimulus s, so that ss is constant. This relationship is called Webers law. Fechner suggested that noticeable differences set the scale Webers law for perceived stimulus intensities. Integrating Webers law, this means that the perceived intensity of a stimulus of absolute intensity s varies as log s. This is known as Fechners law.Sensory systems make numerous adaptations, using a variety of mechanisms, to adjust to the average level of stimulus intensity. When a stimulus generates such adaptation, the relationship between stimulus and response is often studied in a potentially simpler regime by describing responses to fluctuations about a mean stimulus level. In this case, s is defined so that its time average over the duration of a trial is 0, T 0 dt s T  0 T 0 dt s T  0. We frequently impose this condition. Our analysis of neural encoding involves two different types of averages: averages over repeated trials that employ the same stimulus, which we denote by angle brackets, and averages over different stimuli. We could introduce a second notation for averages over stimuli, but this can be avoided when using time-dependent stimuli. Instead of presenting a number of different stimuli and averaging over them, we can string together all of the stimuli we wish to consider into a single time-dependent stimulus sequence and average over time. Thus, stimulus averages are replaced by stimulus and time averages time averages.Although a response recorded over a trial depends only on the values taken by s during that trial, some of the mathematical analyses presented in this chapter and in chapter 2 are simplified if we define the stimulus at other times as well. It is convenient if integrals involving the stimulus are time-translationally invariant so that for any function h and time interval  To assure the last equality, we define the stimulus outside the time limits periodic stimulus of the trial by the relation s  s for any , thereby making the stimulus periodic.The spike-triggered average stimulus, C, is the average value of the stimulus a time interval  before a spike is fired. In other words, for a spike occurring at time t i , we determine s, and then we sum over all n spikes in a trial, i  1, 2, . . . , n, and divide the total by n. In addition, we average over trials. Thus, spike-triggered average CThe approximate equality of the last expression follows from the fact that if n is large, the total number of spikes on each trial is well approximated by the average number of spikes per trial, n  n . We make use of this approximation because it allows us to relate the spike-triggered average to other quantities commonly used to characterize the relationship between stimulus and response. The spike-triggered average stimulus can be expressed as an integral of the stimulus times the neural response function of equation 1.1. If we replace the sum over spikes with an integral, as in equation 1.2, and use the approximate expression for C in equation 1.19, we find The second equality is due to the equivalence of  and r within integrals. Equation 1.20 allows us to relate the spike-triggered average to the correlation function of the firing rate and the stimulus.Correlation functions are a useful way of determining how two quantities that vary over time are related to one another. The two quantities being related are evaluated at different times, one at time t and the other at time firing-rate stimulus correlation function Q rs t  . The correlation function is then obtained by averaging their product over all t values, and it is a function of . The correlation function of the firing rate and the stimulus is Q rs  1 T T 0 dt rs .By comparing equations 1.20 and 1.21, we find that C  1 r Q rs , Figure 1.9 shows the spike-triggered average stimulus for a neuron in the electrosensory lateral-line lobe of the weakly electric fish Eigenmannia. Weakly electric fish generate oscillating electric fields from an internal electric organ. Distortions in the electric field produced by nearby objects are detected by sensors spread over the skin of the fish. The lateral-line lobe acts as a relay station along the processing pathway for electrosensory signals. Fluctuating electrical potentials, such as that shown in the upper left trace of figure 1.9, elicit responses from electrosensory lateral-line lobe neurons, as seen in the lower left trace. The spike-triggered average stimulus, plotted at the right, indicates that, on average, the electric potential made a positive upswing followed by a large negative deviation prior to a spike being fired by this neuron.The results obtained by spike-triggered averaging depend on the particular set of stimuli used during an experiment. How should this set be chosen In chapter 2, we show that there are certain advantages to using a stimulus that is uncorrelated from one time to the next, a white-noise stimulus. A heuristic argument supporting the use of such stimuli is that in asking what makes a neuron fire, we may want to sample its responses to stimulus fluctuations at all frequencies with equal weight, and this is one of the properties of white-noise stimuli. In practice, white-noise stimuli can be generated with equal power only up to a finite frequency cutoff, but neurons respond to stimulus fluctuations only within a limited frequency range anyway. The defining characteristic of a white-noise stimulus is that its value at any one time is uncorrelated with its value at any other time. This condition can be expressed using the stimulus-stimulus correlation function, also called the stimulus autocorrelation, which is defined by analogy with equation Just as a correlation function provides information about the temporal relationship between two quantities, so an autocorrelation function tells us about how a quantity at one time is related to itself evaluated at another time. For white noise, the stimulus autocorrelation function is 0 in the range T2    T2 except when   0, and over this range Q ss   2 s  .The constant  s , which has the units of the stimulus times the square root of the unit of time, reflects the magnitude of the variability of the white noise. In appendix A, we show that equation 1.24 is equivalent to the statement that white noise has equal power at all frequencies.No physical system can generate noise that is white to arbitrarily high frequencies. Approximations of white noise that are missing high-frequency components can be used, provided the missing frequencies are well above the sensitivity of the neuron under investigation. To approximate white noise, we consider times that are integer multiples of a basic unit of duration t, that is, times t  m t for m  1, 2, . . . , M where M t  T. The function s is then constructed as a discrete sequence of stimulus values. This produces a steplike stimulus waveform, like the one that appears in figure 1.8, with a constant stimulus value s m presented during time bin m. The factor of 1 t on the right side of this equation reproduces the  function of equation 1.24 in the limit t  0. For approximate white noise, the autocorrelation function is 0 except for a region around   0 with width of order t. Similarly, the binning of time into discrete intervals of size t means that the noise generated has a flat power spectrum only up to frequencies of order 1.  In addition to triggering on single spikes, stimulus averages can be computed by triggering on various combinations of spikes. Spike-triggered averages of other stimulus-dependent quantities can provide additional insight into neural encoding, for example, spike-triggered average autocorrelation functions. Obviously, spike-triggered averages of higher-order stimulus combinations can be considered as well.A complete description of the stochastic relationship between a stimulus and a response would require us to know the probabilities corresponding to every sequence of spikes that can be evoked by the stimulus. Spike times are continuous variables, and, as a result, the probability for a spike to occur at any precisely specified time is actually zero. To get a nonzero value, we must ask for the probability that a spike occurs within a specified interval, for example, the interval between times t and t  t. For small t, the probability of a spike falling in this interval is proportional to the size of the interval, t. A similar relation holds for any continuous stochastic variable z. The probability that z takes a value between z and z  z, for small z, is equal to p Throughout this book, we use the notation P  to denote probabilities and p  to denote probability densities. We use the bracket notation P  generically for the probability of something occurring and also to denote a specific probability function. In the latter case, the notation P would be more appropriate, but switching between square brackets and parentheses is confusing, so the reader will have to use the context to distinguish between these cases. The probability of a spike sequence appearing is proportional to the probability density of spike times, pt 1 , t 2 , . . . , t n . In other words, the probability Pt 1 , t 2 , . . . , t n  that a sequence of n spikes occurs with spike i falling between times t i and t i  t for i  1, 2, . . . , n is given in terms of this density by the relation Pt 1 , t 2 , . . . , t n   pt 1 , t 2 , . . . , t n  n . Unfortunately, the number of possible spike sequences is typically so large that determining or even roughly estimating all of their probabilities of occurrence is impossible. Instead, we must rely on some statistical model that allows us to estimate the probability of an arbitrary spike sequence occurring, given our knowledge of the responses actually recorded. The firing rate r determines the probability of firing a single spike in a small interval around the time t, but r is not, in general, sufficient information to predict the probabilities of spike sequences. For example, the probability of two spikes occurring together in a sequence is not necessarily equal to the product of the probabilities that they occur individually, because the presence of one spike may effect the occurrence of the other. If, however, the probability of generating an action potential is independent of the presence or timing of other spikes the firing rate is all that is needed to compute the probabilities for all possible action potential sequences.A stochastic process that generates a sequence of events, such as action point process potentials, is called a point process. In general, the probability of an event occurring at any given time could depend on the entire history of preceding events. If this dependence extends only to the immediately preceding event, so that the intervals between successive events are independent, the point process is called a renewal process. If there is no dependence renewal process at all on preceding events, so that the events themselves are statistically independent, we have a Poisson process. The Poisson process provides an extremely useful approximation of stochastic neuronal firing. To makePoisson process the presentation easier to follow, we separate two cases, the homogeneous Poisson process, for which the firing rate is constant over time, and the inhomogeneous Poisson process, which involves a time-dependent firing rate.We denote the firing rate for a homogeneous Poisson process by r  r because it is independent of time. When the firing rate is constant, the Poisson process generates every sequence of n spikes over a fixed time interval with equal probability. As a result, the probability Pt 1 , t 2 , . . . , t n  can be expressed in terms of another probability function P T We can assume that t is small enough so that we never get two spikes within any one bin because, at the end of the calculation, we take the limit t  0. P T binomial coefficient Mn. Putting all these factors together, we findTo take the limit, we note that as t  0, M grows without bound because M t  T. Because n is fixed, we can write M  n  M  T t. Using this approximation and defining   r t, we find that lim t0 Mn  lim 0 1 rT  e rT  exp because lim 0 1 is, by definition, e  exp. For large M, M  M n  n , so P T We can compute the variance of spike counts produced by a Poisson process from the probabilities in equation 1.29. For spikes counted over an interval of duration T, the variance of the spike count is  2 n  n 2  n 2  rT .Thus the variance and mean of the spike count are equal. The ratio of these two quantities,  2 n  n , is called the Fano factor and takes the value 1 for a Fano factor homogeneous Poisson process, independent of the time interval T. The probability density of time intervals between adjacent spikes is called the interspike interval distribution, and it is a useful statistic for character-interspike interval distribution izing spiking patterns. Suppose that a spike occurs at a time t i for some value of i. The probability of a homogeneous Poisson process generating the next spike somewhere in the interval t i    t i1  t i    t, for small t, is the probability that no spike is fired for a time , times the probability, r t, of generating a spike within the following small interval t. From equation 1.29, with n  0, the probability of not firing a spike for period  is exp, so the probability of an interspike interval falling between  and   t is P  t i1  t i    t  r t exp .The probability density of interspike intervals is, by definition, this probability with the factor t removed. Thus, the interspike interval distribution for a homogeneous Poisson spike train is an exponential. The most likely interspike intervals are short ones, and long intervals have a probability that falls exponentially as a function of their duration.From the interspike interval distribution of a homogeneous Poisson spike train, we can compute the mean interspike interval, d  2 r exp   2  1 r 2 .The ratio of the standard deviation to the mean is called the coefficient of coefficient of variation C V variation, Recall that the Fano factor for a Poisson process is also 1. For any renewal process, the Fano factor evaluated over long time intervals approaches the value C 2 V .The spike interval distribution measures the distribution of times between successive action potentials in a train. It is useful to generalize this concept and determine the distribution of times between any two spikes in a train. This is called the spike-train autocorrelation function, and it is particularly useful for detecting patterns in spike trains, most notably oscillations. The spike-train autocorrelation function is the autocorrelation of the neural response function of equation 1.1 with its average over time and trials subtracted out. The time average of the neural response function, from equation 1.4, is the spike-count rate r, and the trial average of spike-train autocorrelation function Q  this quantity is r  n  T. Thus, the spike-train autocorrelation function isBecause the average is subtracted from the neural response function in this expression, Q  should really be called an autocovariance, not an autocorrelation, but in practice it isnt.The spike-train autocorrelation function is constructed from data in the form of a histogram by dividing time into bins. The value of the histogram for a bin labeled with a positive or negative integer m is computed by determining the number of the times that any two spikes in the train are separated by a time interval lying between t and t with t the bin size. This includes all pairings, even between a spike and itself. We call this number N m . If the intervals between the n 2 spike pairs in the train were uniformly distributed over the range from 0 to T, there would be n 2 t T intervals in each bin. This uniform term is removed from the autocorrelation histogram by subtracting n 2 t T from N m for all m. The spike-train autocorrelation histogram is then defined by dividing the resulting numbers by T, so the value of the histogram in bin m is H m  N m  T  n 2 t T 2 . For small bin sizes, the m  0 term in the histogram counts the average number of spikes, that is N 0  n and in the limit t  0, H 0  n  T is the average firing rate r . Because other bins have H m of order t, the large m  0 term is often removed from histogram plots. The spike-train autocorrelation function is defined as H m  t in the limit t  0, and it has the units of a firing rate squared. In this limit, the m  0 bin becomes a  function, H 0  t  r .As we have seen, the distribution of interspike intervals for adjacent spikes in a homogeneous Poisson spike train is exponential. By contrast, the intervals between any two spikes in such a train are uniformly distributed. As a result, the subtraction procedure outlined above gives H m  0 for all bins except for the m  0 bin that contains the contribution of the zero intervals between spikes and themselves. The autocorrelation function for a Poisson spike train generated at a constant rate r  r is thus Q   r .A cross-correlation function between spike trains from two different neurons can be defined by analogy with the autocorrelation function by decross-correlation function termining the distribution of intervals between pairs of spikes, one taken time time When the firing rate depends on time, different sequences of n spikes occur with different probabilities, and pt 1 , t 2 , . . . , t n  depends on the spike times. Because spikes are still generated independently by an inhomogeneous Poisson process, their times enter into pt 1 , t 2 , . . . , t n  only through the time-dependent firing rate r. Assuming, as before, that the spike times are ordered 0  t 1  t 2  . . .  t n  T, the probability density for n spike times isr .This result applies if the spike times have been written in temporal order. If the spike times are not ordered, so that, for example, we are interested in the probability density for any spike occurring at the time t 1 , not necessarily the first spike, this expression should be divided by a factor of n to account for the number of different possible orderings of spike times.Spike sequences can be simulated by using some estimate of the firing rate, r est, predicted from knowledge of the stimulus, to drive a Poisson process. A simple procedure for generating spikes in a computer program is based on the fact that the estimated probability of firing a spike during a short interval of duration t is r est t. The program progresses through time in small steps of size t and generates, at each time step, a random number x rand chosen uniformly in the range between 0 and 1. If r est t  x rand at that time step, a spike is fired otherwise it is not.For a constant firing rate, it is faster to compute spike times t i for i  1, 2, . . . n iteratively by generating interspike intervals from an exponential probability density. If x rand is uniformly distributed over the range between 0 and 1, the negative of its logarithm is exponentially distributed. Thus, we can generate spike times iteratively from the formula t i1  t i  lnr. Unlike the algorithm discussed in the previous paragraph, this method works only for constant firing rates. However, it can be extended to time-dependent rates by using a procedure called rejection sampling or spike thinning. The thinning technique requires a bound r max on the estimated firing rate such that r est  r max at all times. We first generate a spike sequence corresponding to the constant rate r max by iterating the rule t i1  t i  lnr max . The spikes are then thinned by generating another x rand for each i and removing the spike at time t i from the train if r estr max  x rand . If r estr max  x rand , spike i is retained. Thinning corrects for the difference between the estimated time-dependent rate and the maximum rate.  This is an extremely simplified model of response dynamics, because the firing rate at any given time depends only on the value of the stimulus at that instant of time and not on its recent history. Models that allow for a dependence of firing rate on stimulus history are discussed in chapter 2. In figure 1.13, the orientation angle increases in a sequence of steps. The firing rate follows these changes, and the Poisson process generates an irregular firing pattern that reflects the underlying rate but varies from trial to trial. Certain features of neuronal firing violate the independence assumption that forms the basis of the Poisson model, at least if a constant firing rate is used. We have already mentioned the absolute and relative refractory periods, which are periods of time following the generation of an action potential when the probability of a spike occurring is greatly or somewhat reduced. Frequently, these are most prominent features of real neuronal spike trains that are not captured by a Poisson model. Refractory effects can be incorporated into a Poisson model of spike generation by setting the firing rate to 0 immediately after a spike is fired, and then letting it return to its predicted value according to some dynamic rule such as an exponential recovery.The Poisson process is simple and useful, but does it match data on neural response variability To address this question, we examine Fano factors, interspike interval distributions, and coefficients of variation. The Fano factor describes the relationship between the mean spike count over a given interval and the spike-count variance. Mean spike counts n and variances  2 n from a wide variety of neuronal recordings have been fitted to the equation  2 n  A n B , and the multiplier A and exponent B have been determined. The values of both A and B typically lie between 1.0 and 1.5. Because the Poisson model predicts A  B  1, this indicates that the data show a higher degree of variability than the Poisson model would predict. However, many of these experiments involve anesthetized animals, and it is known that response variability is higher in anesthetized than in alert animals. The individual means and variances are scattered in figure 1.14A, but they cluster around the diagonal which is the Poisson prediction. Similarly, the results show A and B values close to 1, the Poisson values.Of course, many neural responses cannot be described by Poisson statistics, but it is reassuring to see a case where the Poisson model seems a reasonable approximation. As mentioned previously, when spike trains are not described very accurately by a Poisson model, refractory effects are often the primary reason.Interspike interval distributions are extracted from data as interspike interval histograms by counting the number of intervals falling in discrete time bins.  with k  0, than by the exponential distribution of the Poisson model, which has k  0.Figure 1.15B shows a theoretical histogram obtained by adding a refractory period of variable duration to the Poisson model. Spiking was prohibited during the refractory period, and then was described once again by a homogeneous Poisson process. The refractory period was randomly chosen from a Gaussian distribution with a mean of 5 ms and a standard deviation of 2 ms. The resulting interspike interval distribution of figure 1.15B agrees quite well with the data.C V values extracted from the spike trains of neurons recorded in monkeys from area MT and primary visual cortex are shown in figure 1.16. The data have been divided into groups based on the mean interspike interval, and the coefficient of variation is plotted as a function of this mean interval, equivalent to 1 r . Except for short mean interspike intervals, the values are near 1, although they tend to cluster slightly lower than 1, the Poisson value. The small C V values for short interspike intervals are due to the refractory period. The solid curve is the prediction of a Poisson model with refractoriness.The Poisson model with refractoriness provides a reasonably good description of a significant amount of data, especially considering its sim-  The nature of the neural code is a topic of intense debate within the neuroscience community. Much of the discussion has focused on whether neurons use rate coding or temporal coding, often without a clear definition of what these terms mean. We feel that the central issue in neural coding is whether individual action potentials and individual neurons encode inde- The center and right panels show recordings from neurons in vivo responding to either injected current or a moving visual image. The neural response, and its relation to the stimulus, are completely characterized by the probability distribution of spike times as a function of the stimulus. If spike generation can be described as an inhomogeneous Poisson process, this probability distribution can be computed from the time-dependent firing rate r, using equation 1.37. In this case, r contains all the information about the stimulus that can be extracted from the spike train, and the neural code could reasonably be called a rate code. Unfortunately, this definition does not agree with common usage. Instead, we will call a code based solely on the time-dependent firing rate independent-spike code an independent-spike code. This refers to the fact that the generation of each spike is independent of all the other spikes in the train. If individual spikes do not encode independently of each other, we call the code a correlation code, because correlations between spike times may carry adcorrelation code ditional information. In reality, information is likely to be carried both by individual spikes and through correlations, and some arbitrary dividing line must be established to characterize the code. Identifying a correlation code should require that a significant amount of information be carried by correlations, for example, as much as is carried by the individual spikes.A simple example of a correlation code would occur if significant amounts of information about a stimulus were carried by interspike intervals. In this case, if we considered spike times individually, independently of each other, we would miss the information carried by the intervals between them. This is just one example of a correlation code. Information could be carried by more complex relationships between spikes.Independent-spike codes are much simpler to analyze than correlation codes, and most work on neural coding assumes spike independence. When careful studies have been done, it has been found that some information is carried by correlations between two or more spikes, but this information is rarely larger than 10% of the information carried by spikes considered independently. Of course, it is possible that, due to our ignorance of the "real" neural code, we have not yet uncovered or examined the types of correlations that are most significant for neural coding. Although this is not impossible, we view it as unlikely and feel that the evidence for independent-spike coding, at least as a fairly accurate approximation, is quite convincing.The discussion to this point has focused on information carried by single neurons, but information is typically encoded by neuronal populations. When we study population coding, we must consider whether individual neurons act independently, or whether correlations between different neurons carry additional information. The analysis of population coding is easiest if the response of each neuron is considered statistically independent, and such independent-neuron coding is typically assumed in independentneuron code the analysis of population codes. The independent-neuron hypothesis does not mean that the spike trains of different neurons are not combined into an ensemble code. Rather, it means that they can be combined without taking correlations into account. To test the validity of this assumption, we must ask whether correlations between the spiking of different neurons provide additional information about a stimulus that cannot be obtained by considering all of their firing patterns individually.Synchronous firing of two or more neurons is one mechanism for conveysynchrony and oscillations ing information in a population correlation code. Rhythmic oscillations of population activity provide another possible mechanism, as discussed below. Both synchronous firing and oscillations are common features of the activity of neuronal populations. However, the existence of these features is not sufficient for establishing a correlation code, because it is essential to show that a significant amount of information is carried by the resulting correlations. The assumption of independent-neuron coding is a useful simplification that is not in gross contradiction with experimental data, but it is less well established and more likely to be challenged in the future than the independent-spike hypothesis.Place-cell coding of spatial location in the rat hippocampus is an example in which at least some additional information appears to be carried by corhippocampal place cells relations between the firing patterns of neurons in a population. The hippocampus is a structure located deep inside the temporal lobe that plays an important role in memory formation and is involved in a variety of spatial tasks. The firing rates of many hippocampal neurons, recorded when a rat is moving around a familiar environment, depend on the location of the animal and are restricted to spatially localized areas called the place fields of the cells. In addition, when a rat explores an environment, hippocampal neurons fire collectively in a rhythmic pattern with a frequency in the theta range, 7-12 Hz. The spiking time of an individual  The concept of temporal coding arises when we consider how precisely we must measure spike times to extract most of the information from a neuronal response. This precision determines the temporal resolution of the neural code. A number of studies have found that this temporal resolution is on a millisecond time scale, indicating that precise spike timing is a significant element in neural encoding. Similarly, we can ask whether high-frequency firing-rate fluctuations carry significant information about a stimulus. When precise spike timing or high-frequency firing-rate fluctuations are found to carry information, the neural code is often identified as a temporal code.The temporal structure of a spike train or firing rate evoked by a stimulus is determined both by the dynamics of the stimulus and by the nature of the neural encoding process. Stimuli that change rapidly tend to generate precisely timed spikes and rapidly changing firing rates no matter what neural coding strategy is being used. Temporal coding refers to temporal precision in the response that does not arise solely from the dynamics of the stimulus, but that nevertheless relates to properties of the stimulus. The interplay between stimulus and encoding dynamics makes the identification of a temporal code difficult.The issue of temporal coding is distinct and independent from the issue of independent-spike coding discussed above. If the independent-spike hypothesis is valid, the temporal character of the neural code is determined by the behavior of r. If r varies slowly with time, the code is typically called a rate code, and if it varies rapidly, the code is called temporal. One possibility is to use the spikes to distinguish slow from rapid, so that a temporal code is identified when peaks in the firing rate occur with roughly the same frequency as the spikes themselves. In this case, each peak corresponds to the firing of only one, or at most a few action potentials. While this definition makes intuitive sense, it is problematic to extend it to the case of population coding. When many neurons are involved, any single neuron may fire only a few spikes before its firing rate changes, but collectively the population may produce a large number of spikes over the same time period. Thus, by this definition, a neuron that appears to employ a temporal code may be part of a population that does not.Another proposal is to use the stimulus, rather than the response, to establish what makes a temporal code. In this case, a temporal code is defined as one in which information is carried by details of spike timing on a scale shorter than the fastest time characterizing variations of the stimulus. This requires that information about the stimulus be carried by Fourier components of r at frequencies higher than those present in the stimulus. Many of the cases where a temporal code has been reported using spikes to define the nature of the code would be called rate codes if the stimulus were used instead.The debate between rate and temporal coding dominates discussions about the nature of the neural code. Determining the temporal resolution of the neural code is clearly important, but much of this debate seems uninformative. We feel that the central challenge is to identify relationships 19 Time-dependent firing rates for different stimulus parameters. The rasters show multiple trials during which an MT neuron responded to the same moving, random-dot stimulus. Firing rates, shown above the raster plots, were constructed from the multiple trials by counting spikes within discrete time bins and averaging over trials. The three different results are from the same neuron but using different stimuli. The stimuli were always patterns of moving random dots, but the coherence of the motion was varied. With this chapter, we have begun our study of the way that neurons encode information using spikes. We used a sequence of  functions, the neural response function, to represent a spike train and defined three types of firing rates: the time-dependent firing rate r, the spike-count rate r, and the average firing rate r . In the discussion of how the firing rate r could be extracted from data, we introduced the important concepts of a linear filter and a kernel acting as a sliding window function. The average firing rate expressed as a function of a static stimulus parameter is called the response tuning curve, and we presented examples of Gaussian, cosine, and sigmoidal tuning curves. Spike-triggered averages of stimuli, or reverse correlation functions, were introduced to characterize the selectivity of neurons to dynamic stimuli. The homogeneous and inhomogeneous Poisson processes were presented as models of stochastic spike sequences. We defined correlation functions, auto-and cross-correlations, and power spectra, and used the Fano factor, interspike-interval histogram, and coefficient of variation to characterize the stochastic properties of spiking. We concluded with a discussion of independent-spike and independentneuron codes versus correlation codes, and of the temporal precision of spike timing as addressed in discussions of temporal coding.The Fourier transform of the stimulus autocorrelation function,is called the power spectrum. Because we have defined the stimulus as power spectrum periodic outside the range of the trial T, we have used a finite-time Fourier transform and  should be restricted to values that are integer multiples of 2 T. We can compute the power spectrum for a white-noise stimulus using the fact that Q ss   2 s  for white noise,This is the defining characteristic of white noise its power spectrum is independent of frequency.Using the definition of the stimulus autocorrelation function, we can also writThe first integral on the right side of the second equality is the complex conjugate of the Fourier transform of the stimulus,The second integral, because of the periodicity of the integrand is equal tos. Therefore, Although equations 1.40 and 1.44 are both sound, they do not provide a statistically efficient method of estimating the power spectrum of discrete approximations to white-noise sequences generated by the methods described in this chapter. That is, the apparently natural procedure of taking a white-noise sequence s for m  1, 2, . . . , T t, and computing the square amplitude of its Fourier transform at frequency ,is a biased and extremely noisy way of estimatingQ ss. This estimator is called the periodogram. The statistical problems with the periodogram, periodogram and some of the many suggested solutions, are discussed in almost any textbook on spectral analysis The average number of spikes generated by a Poisson process with constant rate r over a time T is To compute these quantities, we need to calculate the two sums appearing in these equations. A good way to do this is to compute the momentgenerating function moment-generating functionThe kth derivative of g with respect to , evaluated at the point   0, isso once we have computed g, we need to calculate only its first and second derivatives to determine the sums we need. Rearranging the terms a bit, and recalling that exp  z n  n, we findThe derivatives are thenEvaluating these at   0 and putting the results into equations 1.45 and 1.46 gives the results n  rT and  2 n  2  rT  2  rT.The probability density for a particular spike sequence with spike times t i for i  1, 2, . . . , n is obtained from the corresponding probability distribution by multiplying the probability that the spikes occur when they do by the probability that no other spikes occur. We begin by computing the probability that no spikes are generated during the time interval from t i to t i1 between two adjacent spikes. We determine this by dividing the interval into M bins of size t and setting M t  t i1  t i . We will ultimately take the limit t  0. The firing rate during bin m within this interval is r. Because the probability of firing a spike in this bin is r t, the probability of not firing a spike is 1  r t.To have no spikes during the entire interval, we must string together M such bins, and the probability of this occurring is the product of the individual probabilities,We evaluate this expression by taking its logarithm, The probability density pt 1 , t 2 , . . . , t n  is the product of the densities for the individual spikes and the probabilities of not generating spikes during the interspike intervals, between time 0 and the first spike, and between the time of the last spike and the end of the trial period:The exponentials in this expression all combine because the product of exponentials is the exponential of the sum, so the different integrals in this sum add up to form a single integral:Substituting this into 1.57 gives the result in equation 1.37. The spike-triggered average stimulus introduced in chapter 1 is a standard way of characterizing the selectivity of a neuron. In this chapter, we show how spike-triggered averages and reverse-correlation techniques can be used to construct estimates of firing rates evoked by arbitrary time-dependent stimuli. Firing rates calculated directly from reversecorrelation functions provide only a linear estimate of the response of a neuron, but in this chapter we also present various methods for including nonlinear effects such as firing thresholds.Spike-triggered averages and reverse-correlation techniques have been used extensively to study properties of visually responsive neurons in the retina LGN V1, area 17 retina, lateral geniculate nucleus, and primary visual cortex. At these early stages of visual processing, the responses of some neurons can be described quite accurately using this approach. Other neurons can be described by extending the formalism. Reverse-correlation techniques have also been applied to responses of neurons in visual areas V2, area 18, and MT, but they generally fail to capture the more complex and nonlinear features typical of responses at later stages of the visual system. Descriptions of visual responses based on reverse correlation are approximate, and they do not explain how visual responses arise from the synaptic, cellular, and network properties of retinal, LGN, and cortical circuits. Nevertheless, they provide an important framework for characterizing response selectivities, a reference point for identifying and characterizing novel effects, and a basis for building mechanistic models, some of which are discussed at the end of this chapter and in chapter 7.In chapter 1, we discussed a simple model in which firing rates were estimated as instantaneous functions of the stimulus, using response tuning curves. The activity of a neuron at time t typically depends on the behavior of the stimulus over a period of time starting a few hundred milliseconds prior to t and ending perhaps tens of milliseconds before t. Reversecorrelation methods can be used to construct a more accurate model that includes the effects of the stimulus over such an extended period of time. The basic problem is to construct an estimate r est of the firing rate r evoked by a stimulus s. The simplest way to construct an estimate is to assume that the firing rate at any given time can be expressed as a weighted sum of the values taken by the stimulus at earlier times. Because time is a continuous variable, this "sum" actually takes the form of firing rate estimate r estan integral, and we writeThe term r 0 accounts for any background firing that may occur when s  0. D is a weighting factor that determines how strongly, and with what sign, the value of the stimulus at time t   affects the firing rate at time t.Note that the integral in equation 2.1 is a linear filter of the same form as the expressions used to compute r approx in chapter 1.As discussed in chapter 1, sensory systems tend to adapt to the absolute intensity of a stimulus. It is easier to account for the responses to fluctuations of a stimulus around some mean background level than it is to account for adaptation processes. We therefore assume throughout this chapter that the stimulus parameter s has been defined with its mean value subtracted out. This means that the time integral of s over the duration of a trial is 0.We have provided a heuristic justification for the terms in equation 2.1 but, more formally, they correspond to the first two terms in a systematic expansion of the response in powers of the stimulus. Such an expansion, called a Volterra expansion, is the functional equivalent of the Taylor series Volterra expansion expansion used to generate power series approximations of functions. For the case we are considering, it takes the formThis series was rearranged by Wiener to make the terms easier to compute. The first two terms of the Volterra and Wiener expansions take the same Wiener expansion mathematical forms and are given by the two expressions on the right side of equation 2.1. For this reason, D is called the first Wiener kernel, the Wiener kernel linear kernel, or, when higher-order terms are not being considered, simply the kernel.To construct an estimate of the firing rate based on equation 2.1, we choose the kernel D to minimize the squared difference between the estimated response to a stimulus and the actual measured response averaged over the duration of the trial, T,This expression can be minimized by setting its derivative with respect to the function D to 0. The result is that D satisfies an equation involving two quantities introduced in chapter 1, the firing ratestimulus correlation function, Q rs  dt r s T, and the stimulus autocorrelation function,The method we are describing is called reverse correlation because the firing rate-stimulus correlation function is evaluated at  in this equation.Equation As a result, the kernel that provides the best linear estimate of the firing rate is white-noise kernelwhere C is the spike-triggered average stimulus and r is the average firing rate of the neuron. For the second equality, we have used the relation Q rs  r C from chapter 1. Based on this result, the standard method used to determine the optimal kernel is to measure the spiketriggered average stimulus in response to a white-noise stimulus.In chapter 1, we introduced the H1 neuron of the fly visual system, which responds to moving images. Neuronal selectivity is often characterized by describing stimuli that evoke maximal responses. The reverse-correlation approach provides a basis for this procedure by relating the optimal kernel for firing-rate estimation to the stimulus predicted to evoke the maximum firing rate, subject to a constraint. A constraint is essential because the linear estimate in equation 2.1 is unbounded. The constraint we use is that the time integral of the square of the stimulus over the duration of the trial is held fixed. We call this integral the stimulus energy. The stimulus for which equation 2.1 predicts the maximum response at some fixed time subject to this constraint, is computed in appendix B. The result is that the stimulus producing the maximum response is proportional to the optimal linear kernel or, equivalently, to the white-noise spike-triggered average stimulus. This is an important result because in cases where a white-noise analysis has not been done, we may still have some idea what stimulus produces the maximum response.The maximum stimulus analysis provides an intuitive interpretation of the linear estimate of equation 2.1. At fixed stimulus energy, the integral in 2.1 measures the overlap between the actual stimulus and the most effective stimulus. In other words, it indicates how well the actual stimulus matches the most effective stimulus. Mismatches between these two reduce the value of the integral and result in lower predictions for the firing rate.The optimal kernel produces an estimate of the firing rate that is a linear function of the stimulus. Neurons and nervous systems are nonlinear, so a linear estimate is only an approximation, albeit a useful one. The linear prediction has two obvious problems: there is nothing to prevent the predicted firing rate from becoming negative, and the predicted rate does not saturate, but instead increases without bound as the magnitude of the stimulus increases. One way to deal with these and some of the other deficiencies of a linear prediction is to write the firing rate as a background rate plus a nonlinear function of the linearly filtered stimulus. We use L to represent the linear term we have been discussing thus far:The modification is to replace the linear prediction r est  r 0  L with the generalization r est with static nonlinearitywhere F is an arbitrary function. F is called a static nonlinearity to stress that it is a function of the linear filter value evaluated instantaneously at the time of the rate estimation. If F is appropriately bounded from above and below, the estimated firing rate will never be negative or unrealistically large. at various times and for various stimuli, where r is the actual rate extracted from the data. There will be a certain amount of scatter in this plot due to the inaccuracy of the estimation. If the scatter is not too large, however, the points should fall along a curve, and this curve is a plot of the function F. It can be extracted by fitting a function to the points on the scatter plot. The function F typically contains constants used to set the firing rate to realistic values. These give us the freedom to normalize D in some convenient way, correcting for the arbitrary normalization by adjusting the parameters within F.Static nonlinearities are used to introduce both firing thresholds and saturation into estimates of neural responses. Thresholds can be described by  writingwhere L 0 is the threshold value that L must attain before firing begins.Above the threshold, the firing rate is a linear function of L, with G acting as the constant of proportionality. Half-wave rectification is a special case rectification of this with L 0  0. That this function does not saturate is not a problem if large stimulus values are avoided. If needed, a saturating nonlinearity can be included in F, and a sigmoidal function is often used for this purpose, sigmoidal functionHere r max is the maximum possible firing rate, L 12 is the value of L for which F achieves half of this maximal value, and g 1 determines how rapidly the firing rate increases as a function of L. Another choice that combines a hard threshold with saturation uses a rectified hyperbolic tangent function,where r max and g 2 play similar roles as in equation 2.10, and L 0 is the threshold. Although the static nonlinearity can be any function, the estimate of equation 2.8 is still restrictive because it allows for no dependence on weighted autocorrelations of the stimulus or other higher-order terms in the Volterra series. Furthermore, once the static nonlinearity is introduced, the linear kernel derived from equation 2.4 is no longer optimal because it was chosen to minimize the squared error of the linear estimate r est  r 0  L,  . A theorem due to Bussgang suggests that equation 2.6 will provide a reasonable kernel, even in the presence of a static nonlinearity, if the white noise stimulus used is Gaussian.In some cases, the linear term of the Volterra series fails to predict the response even when static nonlinearities are included. Systematic improvements can be attempted by including more terms in the Volterra or Wiener series, but in practice it is quite difficult to go beyond the first few terms. The accuracy with which the first term, or first few terms, in a Volterra series can predict the responses of a neuron can sometimes be improved by replacing the parameter s in equation 2.7 with an appropriately chosen function of s, so thatA reasonable choice for this function is the response tuning curve. With this choice, the linear prediction is equal to the response tuning curve, L  f, for static stimuli, provided that the integral of the kernel D is equal to 1. For time-dependent stimuli, we can think of equation 2.12 as a dynamic extension of the response tuning curve.A model of the spike trains evoked by a stimulus can be constructed by using the firing-rate estimate of equation 2.8 to drive a Poisson spike generator. Before discussing how reverse-correlation methods are applied to visually responsive neurons, we review the basic anatomy and physiology of the   The output neurons of the retina are the retinal ganglion cells, whose axons retinal ganglion cells form the optic nerve. As seen in figure 2.4B, the subthreshold potentials  Neurons in the retina, LGN, and primary visual cortex respond to light stimuli in restricted regions of the visual field called their receptive fields. Patterns of illumination outside the receptive field of a given neuron canreceptive fields not generate a response directly, although they can significantly affect responses to stimuli within the receptive field. We do not consider such effects, although they are of considerable experimental and theoretical interest. In the monkey, cortical receptive fields range in size from around a tenth of a degree near the fovea to several degrees in the periphery. Within the receptive fields, there are regions where illumination higher than the background light intensity enhances firing, and other regions where lower illumination enhances firing. The spatial arrangement of these regions determines the selectivity of the neuron to different inputs. The term "receptive field" is often generalized to refer not only to the overall region where light affects neuronal firing, but also to the spatial and temporal structure within this region.Visually responsive neurons in the retina, LGN, and primary visual cortex are divided into two classes, depending on whether or not the contribu- To streamline the discussion in this chapter, we consider only gray-scale images, although the methods presented can be extended to include color. We also restrict the discussion to two-dimensional visual images, ignoring how visual responses depend on viewing distance and encode depth. In discussing the response properties of retinal, LGN, and V1 neurons, we do not follow the path of the visual signal, nor the historical order of experimentation, but instead begin with primary visual cortex and then move back to the LGN and retina. In this chapter, the emphasis is on properties of individual neurons we discuss encoding by populations of visually responsive neurons in chapter 10.A striking feature of most visual areas in the brain, including primary visual cortex, is that the visual world is mapped onto the cortical surface in a topographic manner. This means that neighboring points in a visual image evoke activity in neighboring regions of visual cortex. The retinotopic map refers to the transformation from the coordinates of the visual world to the corresponding locations on the cortical surface. F igure 2.6 Two coordinate systems used to parameterize image location. Each rectangle represents a tangent screen, and the filled circle is the location of a particular image point on the screen. The upper panel shows polar coordinates. The origin of the coordinate system is the fixation point F, the eccentricity  is proportional to the radial distance from the fixation point to the image point, and a is the angle between the radial line from F to the image point and the horizontal axis. The lower panel shows Cartesian coordinates. The location of the origin for these coordinates and the orientation of the axes are arbitrary. They are usual chosen to center and align the coordinate system with respect to a particular receptive field being studied. A bulls-eye pattern of radial lines of constant azimuth, and circles of constant eccentricity. The center of this pattern at zero eccentricity is the fixation point F. Such a pattern was used to generate the image in figure 2.7A.Objects located a fixed distance from one eye lie on a sphere. Locations on this sphere can be represented using the same longitude and latitude angles used for the surface of the earth. Typically, the "north pole" for this spherical coordinate system is located at the fixation point, the image point that focuses onto the fovea or center of the retina. In this system of coordinates, the latitude coordinate is called the eccentricity, , and the longitude coordinate, measured from the horizontal meridian, is eccentricity  azimuth a called the azimuth, a. In primary visual cortex, the visual world is split in half, with the region 90   a  90  for  from 0  to about 70  represented on the left side of the brain, and the reflection of this region about the vertical meridian represented on the right side of the brain.In most experiments, images are displayed on a flat screen that does not coincide exactly with the sphere discussed in the previous paragraph. However, if the screen is not too large, the difference is negligible, and the eccentricity and azimuth angles approximately coincide with polar coordinates on the screen. Ordinary Cartesian coordinates can also be used to identify points on the screen. The eccentricity  and the x and y coordinates of the Cartesian system are based on measuring distances on the screen. However, it is customary to divide these measured distances by the distance from the eye to the screen and to multiply the result by 180   so that these coordinates are ultimately expressed in units of degrees. This makes sense because it is the angular, not the absolute, size and location of an image that is typically relevant for studies of the visual system. 7A was produced by imaging a radioactive analogue of glucose that was taken up by active neurons while a monkey viewed a visual image consisting of concentric circles and radial lines, similar to the pattern in figure 2.6B. The vertical lines correspond to the circles in the image, and the roughly horizontal lines are due to the activity evoked by the radial lines. The fovea is represented at the leftmost pole of this piece of cortex, and eccentricity increases toward the right. Azimuthal angles are positive in the lower half of the piece of cortex shown, and negative in the upper half.To describe the map in figure 2.7A mathematically, we write the horizontal and vertical coordinates, X and Y, describing points on the cortical surface as functions of the eccentricity  and azimuth a of the corresponding points in the visual field, X, For the macaque monkey, results such as figure 2.7A suggest that For purely meridional displacements, the angular distance between two points at eccentricity  with an azimuthal angle difference a is a180  . Here, the factor of  corrects for the increase of arc length as a function of eccentricity, and the factor of 180  converts  from degrees to radians. The separation on the cortex, Y, corresponding to these points has a magnitude given by the cortical amplification times this distance. Taking the limit a  0, we find thatThe minus sign in this relationship appears because the visual field is inverted on the cortex. Solving equation 2.16 givesThe map defined by equations 2.15 and 2.17 is only approximate, particularly for small eccentricities. It is also not isotropic, which means that the magnification factor M only describes displacements for which either   0 or a  0. Nevertheless, figure 2.7B shows that these coordinates agree fairly well with the map in figure 2.7A.For eccentricities appreciably greater than 1  , equations 2.15 and 2.17 reduce to X   ln and Y  a180  . These two formulas can be combined by defining the complex numbers Z  X  iY and z  exp and writing Z   ln. For this reason, the cortical map is sometimes called a complex logarithmic map. For an image scaled radially by a factor , eccentricities complex logarithmic map change according to    while a is unaffected. Scaling of the eccentricity produces a shift X  X   ln over the range of values where the simple logarithmic form of the map is valid. The logarithmic transformation thus causes images that are scaled radially outward on the retina to be represented at locations on the cortex translated in the X direction.Earlier in this chapter, we used the function s to characterize a timedependent stimulus. The description of visual stimuli is more complex. Gray-scale images appearing on a two-dimensional surface, such as a video monitor, can be described by giving the luminance, or light intensity, at each point on the screen. These pixel locations are parameterized by Cartesian coordinates x and y, as in the lower panel of figure 2.6A. However, pixel-by-pixel light intensities are not a useful way of parameterizing a visual image for the purposes of characterizing neuronal responses. This is because visually responsive neurons, like many sensory neurons, adapt to the overall level of screen illumination. To avoid dealing with adaptation effects, we describe the stimulus by a function s  determines the spatial location of the light and dark stripes of the grating. Changing by an amount shifts the grating in the direction perpendicular to its orientation by a fraction 2 of its wavelength. The contrast amplitude A controls the maximum degree of difference between light and dark areas. Because x and y are measured in degrees, K is expressed in the rather unusual units of radians per degree and K2 is typically reported in units of cycles per degree.has units of radians,  is in radianss, and 2 is in Hz.Experiments that consider reverse correlation and spike-triggered averages use various types of random and white-noise stimuli in addition to bars and gratings. A white-noise stimulus, in this case, is one that is uncorrelated in both space and time so that white-noise image Of course, in practice a discrete approximation of such a stimulus must be used by dividing the image space into pixels and time into small bins. In addition, more structured random sets of images are sometimes used to enhance the responses obtained during stimulation.Many factors limit the maximal spatial frequency that can be resolved by the visual system, but one interesting effect arises from the size and spacing of individual photoreceptors on the retina. The region of the retina with the highest resolution is the fovea at the center of the visual field. Within the macaque or human fovea, cone photoreceptors are densely packed in a regular array. Along any direction in the visual field, a regular array of tightly packed photoreceptors of size x samples points at locations m x for m  1, 2, . . .. The frequency that defines the resolution of such an array is called the Nyquist frequency and is given by Nyquist frequency  cos  cos  cos by the periodicity and evenness of the cosine function. As a result, these two gratings cannot be distinguished by examining them only at the sampled points. Any two spatial frequencies K  K nyq and 2K nyq  K .9 Aliasing and the Nyquist frequency. The two curves are the functions cos and cos plotted against x, and the dots show points sampled with a spacing of x  3. The Nyquist frequency in this case is 3, and the two cosine curves match at the sampled points because their spatial frequencies satisfy the relation 23  6  2.can be confused with one another in this way, a phenomenon known as aliasing. Conversely, if an image is constructed solely of frequencies less than K nyq , it can be reconstructed perfectly from the finite set of samples provided by the array. There are 120 cones per degree at the fovea of the macaque retina, which makes K nyq   1  60 cycles per degree. In this result, we have divided the right side of equation 2.20, which gives K nyq in units of radians per degree, by 2 to convert the answer to cycles per degree.The spike-triggered average for visual stimuli is defined, as in chapter 1, as the average over trials of stimuli evaluated at times t i  , where t i for i  1, 2, . . . , n are the spike times. Because the light intensity of a visual image depends on location as well as time, the spike-triggered average stimulus is a function of three variables,Here, as in chapter 1, the brackets denote trial averaging, and we have used the approximation 1 n  1 n . C is the average value of the visual stimulus at the point a time  before a spike was fired. Similarly, we can define the correlation function between the firing rate at time t and the stimulus at time t  , for trials of duration T, asThe spike-triggered average is related to the reverse-correlation function, as discussed in chapter 1, bywhere r is, as usual, the average firing rate over the entire trial, r  n  T.. As in equation 2.7, the linear estimate L is obtained by integrating over the past history of the stimulus with a kernel acting as the weighting function. Because visual stimuli depend on spatial location, we must decide how contributions from different image locations are to be combined to determine L. The simplest assumption is that the contributions from linear response estimate different spatial points sum linearly, so that L is obtained by integrating over all x and y values:The kernel D determines how strongly, and with what sign, the visual stimulus at the point and at time t   affects the firing rate of the neuron at time t. As in equation 2.6, the optimal kernel is given in terms of the firing rate-stimulus correlation function, or the spiketriggered average, for a white-noise stimulus with variance parameter  2The kernel D defines the space-time receptive field of a neuron. Because D is a function of three variables, it can be difficult to space-time receptive field measure and visualize. For some neurons, the kernel can be written as a product of two functions, one that describes the spatial receptive field and the other, the temporal receptive field, Such neurons are said to have separable space-time receptive fields. Separability requires that the spatial structure of the receptive field not separable receptive field change over time except by an overall multiplicative factor. When D cannot be written as the product of two terms, the neuron is said to have a nonseparable space-time receptive field. Given the freedom nonseparable receptive field in equation 2.8 to set the scale of D, we typically normalize D s so that its integral is 1, and use a similar rule for the components from which D t is constructed. We begin our analysis by studying first the spatial and then the temporal components of a separable space-time receptive field, and then proceed to the nonseparable case. For simplicity, we ignore the possibility that cells can have slightly different receptive fields for the two eyes, which underlies the disparity tuning considered in chapter 1.Figures 2.10A and C show the spatial structure of spike-triggered average stimuli for two simple cells in the primary visual cortex of a cat with approximately separable space-time receptive fields. These receptive fields are elongated in one direction. There are some regions within the receptive field where D s is positive, called ON regions, and others where it is negative, called OFF regions. The integral of the linear kernel times the stimulus can be visualized by noting how the OFF and ON regions overlap the image . The response of a neuron is enhanced if ON regions are illuminated or if OFF regions are darkened relative to the background level of illumination. Conversely, they are suppressed by darkening ON regions or illuminating OFF regions. As a result, the neurons of figures 2.10A and C respond most vigorously to light-dark edges positioned along the border between the ON and OFF regions, and oriented parallel to this border and to the elongated direction of the receptive fields. Figures 2.10 and 2.11 show receptive fields with two major subregions. Simple cells are found with from one to five subregions. Along with the ON-OFF patterns we have seen, another typical arrangement is a three-lobed receptive field with OFF-ON-OFF or ON-OFF-ON subregions.A mathematical approximation of the spatial receptive field of a simple cell is provided by a Gabor function, which is a product of a Gaussian Gabor function function and a sinusoidal function. Gabor functions are by no means the only functions used to fit spatial receptive fields of simple cells. For example, gradients of Gaussians are sometimes used. However, we will stick to Gabor functions, and to simplify the notation, we choose the coordinates x and y so that the borders between the ON and OFF regions are parallel to the y axis. We also place the origin of the coordinates at the center of the receptive field. With these choices, we can approximate the observed receptive field structures using the Gabor functionThe parameters in this function determine the properties of the spatial receptive field:  x and  y determine its extent in the x and y directions, rf size  x ,  y respectively k, the preferred spatial frequency, determines the spacing of preferred spatial frequency k light and dark bars that produce the maximum response and  is the preferred spatial phase, which preferred spatial phase  determines where the ON-OFF boundaries fall within the receptive field. For this spatial receptive field, the sinusoidal grating described by equation 2.18 that produces the maximum response for a fixed value of A has K  k,  , and  0. The grating shown is nonoptimal due to a mismatch in both the spatial phase and frequency, so that the ON and OFF regions each overlap both light and dark stripes. The grating shown is at a nonoptimal orientation because each region of the receptive field overlaps both light and dark stripes. seen in figure 2.12, Gabor functions can have various types of symmetry, and variable numbers of significant oscillations within the Gaussian envelope. The number of subregions within the receptive field is determined by the product k x and is typically expressed in terms of a quantity known as the bandwidth b. The bandwidth is defined as bandwidth b  log 2, where K   k and K   k are the spatial frequencies of gratings that produce one-half the response amplitude of a grating with K  k. High bandwidths correspond to low values of k x , meaning that the receptive field has few subregions and poor spatial frequency selectivity. Neurons with more subfields are more selective to spatial frequency, and they have smaller bandwidths and larger values of k x .. The values of K  and K  needed to compute the bandwidth are thus determined by the condition exp 12 , but this is usually the case. Bandwidths typically range from about 0.5 to 2.5, corresponding to k x between 1.7 and 6.9.The response characterized by equation 2.27 is maximal if light-dark edges are parallel to the y axis, so the preferred orientation angle is 0. An arbitrary preferred orientation, , can be created by rotating the coordinates, making the substitutions x  x cos  y sin and y  y cos  preferred orientation  x sin in equation 2.27. This produces a spatial receptive field that is maximally responsive to a grating with  . Similarly, a receptive field centered at the point rather than at the origin can be constructed by making the substitutions x  x  x 0 and y  y  y 0 .rf center x 0 , y 0Figure 2.13 reveals the temporal development of the space-time receptive field of a neuron in the cat primary visual cortex through a series of snapshots of its spatial receptive field. More than 300 ms prior to a spike, there is little correlation between the visual stimulus and the upcoming spike. Around 210 ms before the spike, a two-lobed OFF-ON receptive field, similar to the ones in figure 2.10, is evident. As  decreases, this structure first fades away and then reverses, so that the receptive field 75 ms before a spike has the opposite sign from what appeared at   210 ms. Due to latency effects, the spatial structure of the receptive field is less significant for   75 ms. The stimulus preferred by this cell is thus an appropriately aligned dark-light boundary that reverses to a light-dark boundary over time.Reversal effects like those seen in figure 2.13 are a common feature of space-time receptive fields. Although the magnitudes and signs of the different spatial regions in figure 2.13 vary over time, their locations and shapes remain fairly constant. This indicates that the neuron has, to a good approximation, a separable space-time receptive field. When a space-time receptive field is separable, the reversal can be described by a function D t that rises from 0, becomes positive, then negative, and ultimately returns to 0 as  increases. The response of a simple cell to a counterphase grating stimulus can be estimated by computing the function L. For the separable receptive field given by the product of the spatial factor in equation 2.27 and the temporal factor in 2.29, the linear estimate of the response can be written as the product of two terms, whereandThe reader is invited to compute these integrals for the case  x   y  .To show the selectivity of the resulting spatial receptive fields, we plot L s as functions of the parameters , K, and that determine the orientation, spatial frequency, and spatial phase of the stimulus. It is also instructive to write out L s for various special parameter values. First, if the spatial phase of the stimulus and the preferred spatial phase of the receptive field are 0, we find that To display the function D in a space-time plot rather than as a sequence of spatial plots, we suppress the y dependence and plot an x- projection of the space-time kernel. We can also plot the visual stimulus in a space-time diagram, suppressing the y coordinate by assuming that the image does not vary as a function of y. For example, figure 2.18A shows a grating of vertically oriented stripes moving to the left on an x-y plot. In the x-t plot of figure 2.18B, this image appears as a series of sloped dark and light bands. These represent the projection of the image in figure 2.18A onto the x axis evolving as a function of time. The leftward slope of the bands corresponds to the leftward movement of the image.Most neurons in primary visual cortex do not respond strongly to static images, but respond vigorously to flashed and moving bars and gratings. The receptive field structure of figure 2.17 reveals why this is the case, as is shown in figures 2.19 and 2.20. The image in figures 2.19A-C is a dark bar that is flashed on for a brief period of time. To describe the linear response estimate at different times, we consider a space-time receptive field similar to the one in figure 2.17A. The receptive field is positioned at three different times in figures 2.19A, B, and C. The height of the horizontal axis of the receptive field diagram indicates the time when the estimation is being made. Many neurons in primary visual cortex are selective for the direction of motion of an image. Accounting for direction selectivity requires nonseparable space-time receptive fields. An example of a nonseparable receptive field is shown in figure 2.21A. This neuron has a three-lobed OFF-ON-OFF spatial receptive field, and these subregions shift to the left as time moves forward. This means that the optimal stimulus for this neuron has light and dark areas that move toward the left. One way to describe a nonseparable receptive field structure is to use a separable function constructed from a product of a Gabor function for D s and equation 2.29 for D t , but to write these as functions of a mixture or rotation of the x and  variables. The rotation of the space-time receptive field, as seen in figure 2.21B, is achieved by mixing the space and time coordinates, using the transformation  The factor c converts between the units of time and space, and  is the space-time rotation angle. The rotation operation is not the only way to generate nonseparable space-time receptive fields. They are often constructed by adding together two or more separable space-time receptive fields with different spatial and temporal characteristics.  , where F is an appropriately chosen static nonlinearity. The simplest choice for F consistent with the positive nature of firing rates is rectification, F  GL  , with G set to fit the magnitude of the measured firing rates. However, this choice makes the firing rate a linear function of the contrast amplitude, which does not match the data on the contrast dependence of visual responses. Neural responses saturate as the contrast of the image increases, and are more accurately described contrast saturation by r  A n  where n is near 2, and A 12 is a parameter equal to the contrast amplitude that produces a half-maximal response. This led as a function of the ratio of the stimulus spatial frequency to its preferred value, K k, for a grating oriented in the preferredas a function of stimulus spatial phase for a grating with the preferred spatial frequency and orientation, K  k and  0.where B is a temporal and spatial frequency-dependent amplitude factor. We do not need the explicit form of B here, but the reader is urged to derive it. For preferred spatial phase   2,If we square and add these two terms, we obtain a result that does not depend on ,because cos 2  sin 2  1. Thus, we can describe the spatial-phase-invariant response of a complex cell by writing  The description of a complex cell response that we have presented is called an "energy" model because of its resemblance to the equation for the enenergy model ergy of a simple harmonic oscillator. The pair of linear filters used, with preferred spatial phases separated by 2, is called a quadrature pair. Because of rectification, the terms L 2 1 and L 2 2 cannot be constructed by squaring the outputs of single simple cells. However, they can each be constructed by summing the squares of rectified outputs from two simple cells with preferred spatial phases separated by . Thus, we can write the complex cell response as the sum of the squares of four rectified simple cell responses, Here the center of the receptive field has been placed at x  y  0. The first Gaussian function in equation 2.45 describes the center, and the second, the surround. The size of the central region is determined by the parameter  cen , while  sur , which is greater than  cen , determines the size of the surround. B controls the balance between center and surround contributions. The  sign allows both ON-center and OFF-center cases to be represented. The parameters  cen and  sur control the latency of the response in the center and surround regions, respectively, and  cen and  sur affect the time of the reversal. This function has characteristics similar to the function in equation 2.29, but the latency effect is less pronounced.   The models of visual receptive fields we have been discussing are purely descriptive, but they provide an important framework for studying how the circuits of the retina, LGN, and primary visual cortex generate neural responses. In an example of a more mechanistic model, rangement of LGN receptive fields that, when summed, form bands of ON and OFF regions resembling the receptive field of an oriented simple cell. This model accounts for the selectivity of a simple cell purely on the basis of feedforward input from the LGN. We leave the study of this model as an exercise for the reader. Other models, which we discuss in chapter 7, include the effects of recurrent intracortical connections as well.In a previous section, we showed how the properties of complex cell responses could be accounted for by using a squaring static nonlinearity. While this provides a good description of complex cells, there is little indication that complex cells actually square their inputs. Models of complex cells can be constructed without introducing a squaring nonlinearity. One such example is another model proposed by We continued from chapter 1 our study of the ways that neurons encode information, focusing on reverse-correlation analysis, particularly as applied to neurons in the retina, visual thalamus, and primary visual cortex. We used the tools of systems identification, especially the linear filter, Wiener kernel, and static nonlinearity, to build descriptive linear and nonlinear models of the transformation from dynamic stimuli to time-dependent firing rates. We discussed the complex logarithmic map governing the way that neighborhood relationships in the retina are transformed into cortex, Nyquist sampling in the retina, and Gabor functions as descriptive models of separable and nonseparable receptive fields. Models based on Gabor filters and static nonlinearities were shown to account for the basic response properties of simple and complex cells in primary visual cortex, including selectivity for orientation, spatial frequency and phase, velocity, and direction. Retinal ganglion cell and LGN responses were modeled using a difference-of-Gaussians kernel. We briefly described simple circuit models of simple and complex cells.Using equation 2.1 for the estimated firing rate, the expression in equation 2.3 to be minimized isThe minimum is obtained by setting the derivative of E with respect to functional derivative the function D to 0. A quantity, such as E, that depends on a function, D in this case, is called a functional, and the derivative we need is a functional derivative. Finding the extrema of functionals is the subject of a branch of mathematics called the calculus of variations. A simple way to define a functional derivative is to introduce a small time interval t and evaluate all functions at integer multiples of t. We defineIf t is small enough, the integrals in equation 2.48 can be approximated by sums, and we can writeE is minimized by setting its derivative with respect to D j for all values of j to 0,Rearranging and simplifying this expression gives the conditionIf we take the limit t  0 and make the replacements i t  t, j t  , and k t    , the sums in equation 2.51 turn back into integrals, the indexed variables become functions, and we findThe term proportional to r 0 on the right side of this equation can be dropped because the time integral of s is 0. The remaining term is the firing rate-stimulus correlation function evaluated at , Q rs. The estimate written in this acausal form satisfies a slightly modified version of equation 2.4,We define the Fourier transforms   We seek the stimulus that produces the maximum predicted responses at time t subject to the fixed energy constraintWe impose this constraint by the method of Lagrange multipliers, which means that we must find the unconstrained maximum value with respect to s ofwhere  is the Lagrange multiplier. Setting the derivative of this expression with respect to the function s to 0 givesThe value of  is determined by requiring that condition 2.60 is satisfied, but the precise value is not important for our purposes. The essential result is the proportionality between the optimal stimulus and D.C: Bussgangs Theorem , and has been applied in an approach more closely related to the representational learning models of chapter 10 by In chapters 1 and 2, we considered the problem of predicting neural responses to known stimuli. The nervous system faces the reverse problem, determining what is going on in the real world from neuronal spiking patterns. It is interesting to attempt such computations ourselves, using the responses of one or more neurons to identify a particular stimulus or to extract the value of a stimulus parameter. We will assess the accuracy with which this can be done primarily by using optimal decoding techniques, regardless of whether the computations involved seem biologically plausible. Some biophysically realistic implementations are discussed in chapter 7. Optimal decoding allows us to determine limits on the accuracy and reliability of neuronal encoding. In addition, it is useful for estimating the information content of neuronal spike trains, an issue addressed in chapter 4.As we discuss in chapter 1, neural responses, even to a single repeated stimulus, are typically described by stochastic models due to their inherent variability. In addition, stimuli themselves are often described stochastically. For example, the stimuli used in an experiment might be drawn randomly from a specified probability distribution. Natural stimuli can also be modeled stochastically as a way of capturing the statistical properties of complex environments.Given this twofold stochastic model, encoding and decoding are related through a basic identity of probability theory called Bayes theorem. Let r represent the response of a neuron or a population of neurons to a stimulus characterized by a parameter s. Throughout this chapter, r  for N neurons is a list of spike-count firing rates, although, for the present discussion, it could be any other set of parameters describing the neuronal response. Several different probabilities and conditional probabilities enter into our discussion. A conditional probability is just conditional probability an ordinary probability of an event occurring, except that its occurrence is subject to an additional condition. The conditional probability of event A occurring subject to the condition B is denoted by P , the probability of stimulus s being presented, often called the prior probability prior probability P  P  P  P 0% coherence 50% coherence 100% coherence As a final example, we return to single neurons and discuss spike-train decoding, in which an estimate of a time-varying stimulus is constructed from the spike train it evokes.To introduce the notion of discriminability and the receiver operating characteristic that lie at the heart of discrimination analysis, we will discuss a fascinating study performed by To introduce a sense of directed movement at a particular velocity, a percentage of the dots move together by a fixed amount in a fixed direction. The coherently moving dots are selected randomly at each time step, and the remaining dots move to random new locations. The percentage of dots that move together in the fixed direction is called the coherence level. At 0% coherence, the image appears chaotic with no sense of any particular direction of motion. As the coherence increases, a sense of movement in a particular direction appears in the image until, at 100% coherence, the entire array of dots moves together on the monitor. By varying the degree of coherence, the task of detecting the movement direction can be made more or less difficult.The experiments combined neural recording with behavioral measurements. In the behavioral part, the monkey had to report the direction   In the example we are considering, decoding involves using the neural response to determine in which of the two possible directions the stimulus moved. A simple decoding procedure is to determine the firing rate r during a trial and compare it to a threshold number z. If r  z, we report "plus" otherwise we report "minus". The probability that the procedure outlined in the previous paragraph will generate the correct answer when the stimulus is moving in the plus direction is the conditional probability that r  z given a plus stimulus, Pr  z. The probability that it will give the answer "plus" when the stimulus is actually moving in the minus direction is similarly Pr  z. These two probabilities completely determine the performance of the decoding procedure because the probabilities for the other two cases are 1  Pr  z and 1  Pr  z, respectively. In signal detection theory, the quantity used to perform the discrimination, r in our case, is called the test, and the two probabilities corresponding to reporting a "plus" answer have specific names: test size and power or false alarm and hit rate   Pr  z is the size or false alarm rate of the test   Pr  z is the power or hit rate of the test.The following table shows how the probabilities of the test giving correct and incorrect answers in the different cases depend on  and .2 if the plus and minus stimuli occur with equal probability. Although this is a possible approach for the experiment we are studying, the analysis we present introduces a powerful technique that makes better use of the full range of recorded data and can be generalized to tasks where the optimal strategy is unknown. This approach makes use of ROC curves, which indicate how the size and power of a test trade off as the threshold is varied.The receiver operating characteristic curve provides a way of evaluating how test performance depends on the choice of the threshold z. Each point on an ROC curve corresponds to a different value of z. The receiver operating characteristic, ROC x coordinate of the point is , the size of the test for this value of z, and the y coordinate is , its power. As the threshold is varied continuously, these points trace out the ROC plot. If z  0, the firing rate will always be greater than or equal to z, so the decoding procedure will always give the answer "plus". Thus, for z  0,     1, producing a point at the upperright corner of the ROC plot. At the other extreme, if z is very large, r will always be less than z, the test will always report "minus", and     0. This produces a point at the bottom-left corner of the plot. Between these extremes, a curve is traced out as a function of z. At high coherence levels, when the task is easy, the ROC curve rises rapidly from   0,   0 as the threshold is lowered from a high value, and the probability  of a correct "plus" answer quickly approaches 1 without a concomitant increase in . As the threshold is lowered further, the probability of giving the answer "plus" when the correct answer is "minus" also rises, and  increases. When the task is difficult, the curve rises more slowly as z is lowered and if the task is impossible, in that the test merely gives random answers, the curve will lie along the diagonal   , because the probabilities of answers being correct and incorrect are equal. This is exactly the trend of the ROC curves at different coherence levels shown in figure 3.3.Examination of figure 3.3 suggests a relationship between the area under the ROC curve and the level of performance on the task. When the ROC curve in In the two-alternative force-choice task, the value of r on one trial serves as the threshold for the other trial. For example, if the order of stimulus presentation is plus, then minus, the comparison procedure we have outlined will report the correct answer if r 1  z where z  r 2 , and this has probability Pr 1  z   with z  r 2 . To determine the probability of getting the correct answer in a two-alternative forced-choice task, we need to integrate this probability over all possible values of r 2 weighted by their probability of occurrence. For small z, the probability that r 2 takes a value in the range between z and z z when the second trial has a minus stimulus is pz z, where pz is the conditional firing-rate probability density for a firing rate r  z. Integrating over all values of z gives the probability of getting the correct answer,Because the two-alternative forced-choice test is symmetric, this is also the probability of being correct if the order of the stimuli is reversed.The probability that r  z for a minus stimulus, which is just , can be written as an integral of the conditional firing-rate probability densityTaking the derivative of this equation with respect to z, we find thatThis allows us to make the replacement dz pz  d in the integral of equation 3.6 and to change the integration variable from z to . Noting that   1 when z  0 and   0 when z  , we findThe ROC curve is just  plotted as a function of , so this integral is the area under the ROC curve. Thus, the area under the ROC curve is the probability of responding correctly in the two-alternative forced-choice test. To interpret their experiment as a two-alternative forced-choice task, Britten et al. imagined that, in addition to being given the firing rate of the recorded neuron during stimulus presentation, the observer is given the firing rate of a hypothetical "anti-neuron" having response characteristics exactly opposite from the recorded neuron. In reality, the responses of this anti-neuron to a plus stimulus were just those of the recorded neuron to a minus stimulus, and vice versa. The idea of using the responses of a single neuron to opposite stimuli as if they were the simultaneous responses of two different neurons also reappears in our discussion of spike-train decoding. An observer predicting motion directions on the basis of just these two neurons at a level equal to the area under the ROC curve is termed an ideal observer.Figure 3.2A shows a typical result for the performance of an ideal observer using one recorded neuron and its anti-neuron partner. The open circles in figure 3.2A were obtained by calculating the areas under the ROC curves for this neuron. Amazingly, the ability of the ideal observer to perform the discrimination task using a single neuronanti-neuron pair is equal to the ability of the monkey to do the task. Although the choices of the ideal observer and the monkey do not necessarily match on a trial-to-trial basis, their performances are comparable when averaged over trials. This seems remarkable because the monkey presumably has access to a large population of neurons, while the ideal observer uses only two. One speculation is that correlations in the response variability between neurons limit the performance of the monkey.The discrimination test we have considered compares the firing rate to a threshold value. Could an observer do better than this already remarkable performance by comparing some other function of the firing rate to a threshold What is the best test function to use for this purpose The Neyman-Pearson lemma shows that it is impossiNeyman-Pearson lemma ble to do better than to choose as the test function the ratio of probability densities, There is a direct relationship between the likelihood ratio and the ROC curve. As in equations 3.7 and 3.8, we can writeCombining this result with 3.8, we find that Another way of seeing that comparing the likelihood ratio to a threshold value is an optimal decoding procedure for discrimination uses a Bayesian approach based on associating a cost or penalty with getting the wrong answer. Suppose that the penalty associated with answering "minus" when the correct answer is "plus" is quantified by the loss parameter L  . Similarly, quantify the loss for answering "plus" when the correct answer is "minus" as L  . For convenience, we assume that there is neither loss nor gain for answering correctly. The probabilities that the correct answer is "plus" or "minus", given the firing rate r, are Pr and Pr respectively. These probabilities are related to the conditional firing-rate probability densities by Bayes theorem,The average loss expected for a "plus" answer when the firing rate is r is the loss associated with being wrong times the probability of being wrong,. Similarly, the expected loss when answering "minus" is Loss   L  P The use of large numbers of neurons to represent information is a basic operating principle of many nervous systems. Population coding has a number of advantages, including reduction of uncertainty due to neuronal variability and the ability to represent a number of different stimulus attributes simultaneously. Individual neurons in such a population typically have different but overlapping selectivities, so that many neurons, but not necessarily all, respond to a given stimulus. In the previous section, we discussed discrimination between stimuli on the basis of the response of a single neuron. The responses of a population of neurons can also be used for discrimination, with the only essential difference being that terms such as p The cercal system of the cricket, which senses the direction of incoming air currents as a warning of approaching predators, is an interesting example of population coding involving a relatively small number of neurons. Crickets and related insects have two appendages called cerci extending  To determine the wind direction from the firing rates of the cercal interneurons, it is useful to change the notation somewhat. In place of the angle s, we can represent wind direction by a spatial vector v pointing parallel to the wind velocity and having unit length  v  1. Similarly, we can represent the preferred wind direction for each interneuron by a vector c a of unit length pointing in the direction specified by the angle s a . In this case, we can use the vector dot product to write cos  v  c a . In terms of these vectors, the average dot product firing rate is proportional to a half-wave rectified projection of the wind direction vector onto the preferred-direction axis of the neuron,Decoding the cercal system is particularly easy because of the close relationship between the representation of wind direction it provides and a two-dimensional Cartesian coordinate system. In a Cartesian system, vectors are parameterized by their projections onto x and y axes, v x and v y . These projections can be written as dot products of the vector being represented, v, with vectors of unit length x and y lying along the x and y axes, v x  v  x and v y  v  y. Except for the half-wave rectification, these equations are identical to equation 3.21. Furthermore, the preferred directions of the four interneurons, like the x and y axes of a Cartesian coordinate system, lie along two perpendicular directions. Four neurons are required, rather than two, because firing rates cannot represent negative projections. The cricket discovered the Cartesian coordinate system long before Descartes did, but failed to invent negative numbers Perhaps credit should also be given to the leech, for A vector v can be reconstructed from its Cartesian components through the component-weighted vector sum v  v x x  v y y. Because the firing rates of the cercal interneurons we have been discussing are proportional to the Cartesian components of the wind direction vector, a similar sum should allow us to reconstruct the wind direction from a knowledge of the interneuron firing rates, except that four, not two, terms must be included. If r a is the spike-count firing rate of neuron a, an estimate of the wind direction on any given trial can be obtained from the direction of the vectorThis vector is known as the population vector, and the associated decoding population vector method is called the vector method. This decoding scheme works quite vector method well. As discussed in chapter 1, tuning curves of certain neurons in the primary motor cortex of the monkey can be described by cosine functions of arm movement direction. Thus, a vector decomposition similar to that of the cercal system appears to take place in M1. Many M1 neurons have nonzero offset rates, r 0 , so they can represent the cosine function over most or all of its range. When an arm movement is made in the direction represented by a vector of unit length, v, the average firing rates for such an M1 neuron, labeled by an index a, can be written aswhere c a is the preferred-direction vector that defines the selectivity of the neuron. Because these firing rates represent the full cosine function, it would, in principle, be possible to encode all movement directions in three dimensions using just three neurons. Instead, many thousands of M1 neurons have arm-movement-related tuning curves, resulting in a highly redundant representation. Of course, these neurons encode additional movement-related quantities for example, their firing rates depend on the initial position of the arm relative to the body as well as on movement ve-locity and acceleration. This complicates the interpretation of their activity as reporting movement direction in a particular coordinate system. We leave as an exercise the proof that v pop is approximately parallel to v if a large enough number of neurons is included in the sum, and if their preferred-direction vectors point randomly in all directions with equal probability. Later in this chapter, we discuss how corrections can be made if the distribution of preferred directions is not uniform or the number of neurons is not large. The population vectors constructed from equation 3.24 on the basis of responses of neurons in primary motor cortex, recorded while a monkey performed a reaching task, are compared with the actual directions of arm movements in figure 3.6.The vector method is a simple decoding method that can perform quite well in certain cases, but it is neither a general nor an optimal way to reconstruct a stimulus from the firing rates of a population of neurons. In this section, we discuss two methods that can, by some measure, be considered optimal. These are called Bayesian inference and maximum a posteriori inference. We also discuss a special case of MAP called maximum likelihood inference. The Bayesian approach involves finding the minimum of a loss function that expresses the cost of estimation errors. MAP inference and ML inference generally produce estimates that are as accurate, in terms of the variance of the estimate, as any that can be achieved by a wide class of estimation methods, at least when large numbers of neurons are used in the decoding. Bayesian and MAP estimates use the conditional probability that a stimulus parameter takes a value between s and s  s, given that the set of N encoding neurons fired at rates given by r. The probability density  The Bayesian result has a slightly smaller average error across all angles. The dips in the error curves in Comparing these results with Up to now, we have considered the decoding of a direction angle. We now turn to the more general case of decoding an arbitrary continuous stimulus parameter. An instructive example is provided by an array of N neurons with preferred stimulus values distributed uniformly across the full range of possible stimulus values. An example of such an array for Gaussian tuning curves,is shown in figure 3.8. In this example, each neuron has a tuning curve with a different preferred value s a and potentially a different width  a. If the tuning curves are evenly and densely distributed across the range of s values, the sum of all tuning curves f a is approximately independent of s. The roughly flat line in figure 3.8 is proportional to this sum. The constancy of the sum over tuning curves will be useful in the following analysis.Tuning curves give the mean firing rates of the neurons across multiple trials. In any single trial, measured firing rates will vary from their mean values. To implement the Bayesian, MAP, or ML approach, we need to know the conditional firing-rate probability density p If we assume that each neuron fires independently, the firing-rate proba- bility for the population is the product of the individual probabilities,The assumption of independence simplifies the calculations considerably.The filled circles in figure 3.9 show a set of randomly generated firing rates for the array of Gaussian tuning curves in where the ellipsis represents terms that are independent or approximately independent of s, including, as discussed above, f a. Because maximizing a function and maximizing its logarithm are equivalent, we can use the logarithm of the conditional probability in place of the actual probability in ML decoding.The ML estimated stimulus, s ML , is the stimulus that maximizes the right side of equation 3.31. Setting the derivative to 0, we find that s ML is deter-  If the stimulus or prior distribution is itself Gaussian with mean s prior and variance  prior , and we use the Gaussian array of tuning curves, equation 3.36 yields The accuracy with which an estimate s est describes a stimulus s can be characterized by two important quantities, its bias b est and its variance bias  2 est. The bias is the difference between the average of s est across trials that use the stimulus s and the true value of the stimulus,  In other words, the average squared estimation error is the sum of the variance and the square of the bias. For an unbiased estimate, the average squared estimation error is equal to the variance of the estimator.Decoding can be used to limit the accuracy with which a neural system encodes the value of a stimulus parameter because the encoding accuracy cannot exceed the accuracy of an optimal decoding method. Of course, we must be sure that the decoding technique used to establish such a bound is truly optimal, or else the result will reflect the limitations of the decoding procedure, not bounds on the neural system being studied. The Fisher information is a quantity that provides one such measure of encoding accuracy. Through a bound known as the Cramr-Rao bound, the Fisher information limits the accuracy with which any decoding scheme can extract an estimate of an encoded quantity.The Cramr-Rao bound limits the variance of any estimate s est according Cramr-Rao bound Provided that we restrict ourselves to unbiased decoding schemes, the Fisher information sets an absolute limit on decoding accuracy, and it thus provides a useful limit on encoding accuracy. Although imposing zero bias on the decoding estimate seems reasonable, the restriction is not trivial. In general, minimizing the decoding error in equation 3.40 involves a trade-off between minimizing the bias and minimizing the variance of the estimator. In some cases, biased schemes may produce more accurate results than unbiased ones. For a biased estimator, the average squared estimation error and the variance of the estimate are not equal, and the estimation error can be either larger or smaller than 1 I F.The limit on decoding accuracy set by the Fisher information can be attained by a decoding scheme we have studied, the maximum likelihood method. In the limit of large numbers of encoding neurons, and for most firing-rate distributions, the ML estimate is unbiased and saturates the Cramr-Rao bound. In other words, the variance of the ML estimate is given asymptotically by  2 ML  1 I F. Any unbiased estimator that saturates the Cramr-Rao lower bound is called efficient. efficiency Furthermore, I F grows linearly with N, and the ML estimate obeys a central limit theorem, so that N 12 is Gaussian distributed with a variance that is independent of N in the large N limit. Finally, in the limit N  , the ML estimate is asymptotically consistent, in the sense that asymptotic consistency Ps ML s    0 for any   0.As equation 3.42 shows, the Fisher information is a measure of the expected curvature of the log likelihood at stimulus value s. Curvature is important because the likelihood is expected to be at a maximum near the true stimulus value s that caused the responses. If the likelihood is very curved, and thus the Fisher information is large, responses typical for the stimulus s are much less likely to occur for slightly different stimuli. Therefore, the typical response provides a strong indication of the value of the stimulus. If the likelihood is fairly flat, and thus the Fisher information is small, responses common for s are likely to occur for slightly different stimuli as well. Thus, the response does not as clearly determine the stimulus value. The Fisher information is purely local in the sense that it does not reflect the existence of stimulus values completely different from s that are likely to evoke the same responses as those evoked by s itself. However, this does not happen for the sort of simple population codes we consider. Shannons mutual information measure, discussed in chapter 4, takes such possibilities into account.The Fisher information for a population of neurons with uniformly arrayed tuning curves and Poisson statistics can be computed from the conditional firing-rate probability in equation 3.30. Because the spike-count rate is described here by a probability rather than a probability density, we use the discrete analog of equation 3.42, Note that we have used the full expression, equation 3.30, in deriving this result, not the truncated form of ln Prs in equation 3.31. We next make the replacement r a  f a, producing the final resultIn this expression, each neuron contributes an amount to the Fisher information proportional to the square of its tuning curve slope and inversely proportional to the average firing rate for the particular stimulus value being estimated. Highly sloped tuning curves give firing rates that are sensitive to the precise value of the stimulus. The Fisher information can be used to derive an interesting result on the optimal widths of response tuning curves. Consider a population of neu- To determine whether narrow or broad tuning curves produce the more accurate encodings, we consider a dense distribution of Gaussian tuning curves, all with  a   r . Using such curves in equation 3.45, we findThis expression can be approximated by replacing the sum over neurons with an integral over their preferred stimulus values and multiplying by sumsintegrals a density factor  s . The factor  s is the density with which the neurons cover the range of stimulus values, and it is equal to the number of neurons with preferred stimulus values lying within a unit range of s values. Replacing the sum over a with an integral over a continuous preferred stimulus parameter , we findWe have expressed the final result in this form because the number of neurons that respond to a given stimulus value is roughly  s  r , and the Fisher information is proportional to this number divided by the square of the tuning curve width. Combining these factors, the Fisher information is inversely proportional to  r , and the encoding accuracy increases with narrower tuning curves.The advantage of using narrow tuning curves goes away if the stimulus is characterized by more than one parameter. Consider a stimulus with D parameters and suppose that the response tuning curves are products of identical Gaussians for each of these parameters. If the tuning curves cover the D-dimensional space of stimulus values with a uniform density  s , the number of responding neurons for any stimulus value is proportional to  s  D r and, using the same integral approximation as in equation 3.47, the Fisher information isThis equation, which reduces to the result given above if D  1, allows us to examine the effect of tuning curve width on encoding accuracy. The trade-off between the encoding accuracy of individual neurons and the number of responding neurons depends on the dimension of the stimulus space. Narrowing the tuning curves increases the Fisher information for D  1, decreases it for D  2, and has no impact if D  2.In the first part of this chapter, we considered discrimination between two values of a stimulus. An alternative to the procedures discussed there is simply to decode the responses and discriminate on the basis of the estimated stimulus values. Consider the case of discriminating between s and s  s for small s. For large N, the average value of the difference between the ML estimates for the two stimulus values is equal to s and the variance of each estimate is 1 I F. Thus, the discriminability, defined in equation 3.4, for theThe larger the Fisher information, the higher the discriminability. We leave as an exercise the proof that for small s, this discriminability is the same as that of the likelihood ratio test Z defined in equation 3.19.Discrimination by ML estimation requires maximizing the likelihood, and this may be computationally challenging. The likelihood ratio test described previously may be simpler, especially for Poisson variability, because, for small s, the likelihood ratio test Z defined in equation 3.19 is a linear function of the firing rates, An effective number of neurons involved in the task was estimated for the different line lengths and eccentricities, using the cortical magnification factor discussed in chapter 2. The decoding methods we have considered estimate or discriminate static stimulus values on the basis of spike-count firing rates. Spike-count firing rates do not provide sufficient information for reconstructing a stimulus that varies during the course of a trial. Instead, we can estimate such a stimulus from the sequence of firing times t i for i  1, 2, . . . , n of the spikes that it evokes. One method for doing this is similar to the Wiener kernel approach used to estimate the firing rate from the stimulus in chapter 2, and to approximate a firing rate using a sliding window function in chapter 1. For simplicity, we restrict our discussion to the decoding of a single neuron. We assume, as we did in chapter 2, that the time average of the stimulus being estimated is 0.In spike-train decoding, we attempt to construct an estimate of the stimulus at time t from the sequence of spikes evoked up to that time. There are paradoxical aspects of this procedure. The firing of an action potential at time t i is only affected by the stimulus s prior to that time, t  t i , and yet, in spike decoding, we attempt to extract information from this action potential about the value of the stimulus at a later time t  t i . That is, the evoked spikes tell us about the past behavior of the stimulus and, in spike decoding, we attempt to use this information to predict the current stimulus value. Clearly, this requires that the stimulus have some form of temporal correlation so that past behavior provides information about the current stimulus value. To make the decoding task easier, we can introduce a prediction delay,  0 , and attempt to construct, from spikes prediction delay  0 occurring prior to time t, an estimate of the stimulus at time t   0. Such a delayed estimate uses a combination of spikes that could have been fired in response to the stimulus s being estimated, and spikes that occurred too early to be affected by the value of s, but that can contribute to its estimation on the basis of stimulus correlations. The estimation task gets easier as  0 is increased, but this delays the decoding and makes the result less behaviorally relevant. We will consider decoding with an arbitrary delay and later discuss how to set a specific value for  0 .The stimulus estimate is constructed as a linear sum over all spikes. A stimulus estimate spike occurring at time t i contributes a kernel K, and the total estimate is obtained by summing over all spikes,The last term, with r  n  T the average firing rate over the trial, is included to impose the condition that the time average of s est is 0, in agreement with the time-average condition on s. The sum in equation 3.51 includes all spikes, so the constraint that only those spikes occurring prior to the time t should be included must be imposed by requiring K  0 for t  t i  0. A kernel satisfying this constraint is termed causal. We ignore the causality constraint for now and construct an acausal kernel, but we will return to issues of causality later in the discussion. Equation Using this form of the estimate, the construction of the optimal kernel K proceeds very much like the construction of the optimal kernel for predicting firing rates in chapter 2. We choose K so that the squared difference between the estimated stimulus and the actual stimulus, averaged over both time and trials,is minimized. The calculation proceeds as in appendix A of chapter 2, and the result is that K obeys the equationwhere Q  is the spike-train autocorrelation function,as defined in chapter 1. Q rs is the correlation of the firing rate and the stimulus, which is related to the spike-triggered average C, both introduced in chapter 1,At this point in the derivation of the optimal linear kernel for firing-rate prediction in chapter 2, we chose the stimulus to be uncorrelated so that an integral equation similar to 3.54 would be simplified. This could always be done because we have complete control over the stimulus in this type of experiment. However, we do not have similar control of the neuron, and must deal with whatever spike-train autocorrelation function it gives us. If the spike train is uncorrelated, which tends to happen at low rates,and we find from equation 3.54 thatThis is the average value of the stimulus at time    0 relative to the appearance of a spike. Because    0 can be either positive or negative, stimulus estimation, unlike firing-rate estimation, involves both forward and backward correlation and the average values of the stimulus both before and after a spike. Decoding in this way follows a simple rule: every time a spike appears, we replace it with the average stimulus surrounding a spike, shifted by an amount  0.The need for either stimulus correlations or a nonzero prediction delay is clear from equation 3.58. Correlations between a spike and subsequent stimuli can arise, in a causal system, only from correlations between the stimulus and itself. If these are absent, as for white noise, K will be 0 for    0 . For causal decoding, we must also have K  0 for   0.Thus, if  0  0 and the stimulus is uncorrelated, K  0 for all values of .When the spike-train autocorrelation function is not a  function, an acausal solution for K can be expressed as an inverse Fourier transform, optimal kernelwhere, as shown in appendix C,HereQ rs andQ  are the Fourier transforms of Q rs and Q  . The numerator in this expression reproduces the expression Q rs in equation 3.58. The role of the denominator is to correct for any autocorrelations in the response spike train. Such correlations introduce a bias in the decoding, and the denominator in equation 3.60 corrects for this bias.If we ignore the constraint of causality, then, because the occurrence of a spike cannot depend on the behavior of a stimulus in the very distant past, we can expect K from equation 3.58 to vanish for sufficiently negative values of    0 . For most neurons, this will occur for    0 more negative than minus a few hundred ms. The decoding kernel can therefore be made small for negative values of  by choosing  0 large enough, but this may require a fairly large prediction delay. We can force exact adherence to the causality constraint for   0 by replacing K byK, where causality constraint is defined such that  1 for   0 and  0 for   0. The causality constraint was imposed in this way in figure 3.13B. When it is multiplied by, the restricted K is no longer the optimal decoding kernel, but it may be close to optimal.Another way of imposing causality on the decoding kernel is to expand K as a weighted sum of causal basis functions. The optimal weights are then determined by minimizing the estimation error. This approach has the advantage of producing a truly optimal kernel for any desired value of  0 . A simpler but nonoptimal approach is to consider a fixed functional form for K that vanishes for   0 and is characterized by a number of free parameters that can be determined by minimizing the decoding error. Finally, the optimal causal kernel, also called the Wiener-Hopf filter, can be obtained by a technique that involves so-called spectral factorization ofQ . 14 shows an example of spike-train decoding for the H1 neuron of the fly discussed in chapter 2. The top panel gives two reconstruction kernels, one acausal and one causal, and the bottom panel compares the reconstructed stimulus velocity with the actual stimulus velocity. The middle panel in figure 3.14 points out one further wrinkle in the procedure. Flies have two H1 neurons, one on each side of the body, that respond to motion in opposite directions. As is often the case, half-wave rectification prevents a single neuron from encoding both directions of motion. In the experiment described in the figure, rather than recording from both H1 neurons, The fly has only two H1 neurons from which it must extract information about visual motion, so it seems reasonable that stimulus reconstruction using the spike-train decoding technique can produce quite accurate results. It is perhaps more surprising that accurate decoding, at least in the sense of percent correct discriminations, can be obtained from single neurons out of the large population of MT neurons responding to visual motion in the monkey. Of course, the reconstruction of a time-dependent stimulus from H1 responses is more challenging than the binary discrimination done with MT neurons. Furthermore, it is worth remembering that in all the examples we have considered, including decoding wind direction from the cercal system and arm movement direction Figure 3.14 Decoding the stimulus from an H1 neuron of the fly. The upper panel is the decoding kernel. The jagged curve is the optimal acausal filter, and the smooth curve is a kernel obtained by expanding in a causal set of basis functions. In both cases, the kernels are shifted by  0  40 ms. The middle panel shows typical responses of the H1 neuron to the stimuli s and s. The dashed line in the lower panel shows the actual stimulus, and the solid line is the estimated stimulus from the optimal linear reconstruction using the acausal filter. We have considered the decoding of stimulus characteristics from the responses they evoke, including discrimination between stimulus values, the decoding of static stimuli on the basis of population responses, and the decoding of dynamic stimulus parameters from spike trains. Discrimination was studied using the receiver operating characteristic, likelihood ra-tio tests, and the Neyman-Pearson lemma. For static parameter decoding we introduced the vector method Bayesian, maximum a posteriori, and maximum likelihood inference the Fisher information and the Cramr-Rao lower bound. We also showed how to use ideas from Wiener filtering to reconstruct an approximation of a time-varying stimulus from the spike trains it evokes.Consider the difference  in the power of two tests that have identical sizes .For the test h using the threshold z h , Similar equations hold for the  l and  l values for the test l using the threshold z l . We use the function, which is 1 for positive and 0 for negative values of its argument, to impose the condition that the test is greater than the threshold. Comparing the  values for the two tests, we findThe range of integration where l  z l and also h  z h cancels between these two integrals, so, in a more compact notation, we can write, we findBy definition, these integrals are the sizes of the two tests, which are equal by hypothesis. Thus   0, showing that no test can be better than the likelihood ratio l, at least in the sense of maximizing the power for a given size.The Cramr-Rao lower bound for an estimator s est is based on the CauchyCauchy-Schwarz inequality Schwarz inequality, which states that for any two quantities A and B,To prove this inequality, note thatbecause it is the average value of a square. Computing the square givesfrom which the inequality follows directly.Consider the inequality of equation 3.67 with A   ln ps and B  s est  s est . From equations 3.43 and 3.39, we have A 2  I F and B 2   2 est . The Cauchy-Schwarz inequality then givesTo evaluate the expression on the right side of the inequality 3.70, we differentiate the defining equation for the bias, which, when rearranged, is the Cramr-Rao bound of equation 3.41.The optimal linear kernel for spike-train decoding is determined by solving equation 3.54. This is done by taking the Fourier transform of both sides of the equation, that is, multiplying both sides by exp and integrating over , By making the replacement of integration variable      0 , we find that the right side of this equation iswhereQ rs is the Fourier transform of Q rs. The integral of the product of two functions that appears on the left side of equations 3.54 and 3.75 is a convolution. As a result of the theorem on the Fourier transforms of convolutions,whereQ  andK are the Fourier transforms of Q  and K respectively: Putting the left and right sides of equation 3.75 together as we have evaluated them, we find thatEquation Statistical analysis of discrimination, various forms of decoding, the Neyman-Pearson lemma, the Fisher information, and the Cramr-Rao lower bound can be found in Cox  Hinckley. Receiver operator characteristics and signal detection theory are described comprehensively in Green  Swets and Neural encoding and decoding focus on the question "What does the response of a neuron tell us about a stimulus" In this chapter we consider a related but different question "How much does the neural response tell us about a stimulus" The techniques of information theory allow us to answer this question in a quantitative manner. Furthermore, we can use them to ask what forms of neural response are optimal for conveying information about natural stimuli. Information theoretic principles play an important role in many of the unsupervised learning methods that are discussed in chapters 8 and 10.Shannon invented information theory as a general framework for quantifying the ability of a coding scheme or a communication channel to convey information. It is assumed that the code involves a number of symbols, and that the coding and transmission processes are stochastic and noisy. The quantities we consider in this chapter, the entropy and the mutual information, depend on the probabilities with which these symbols, or combinations of them, are used. Entropy is a measure of the theoretical capacity of a code to convey information. Mutual information measures how much of that capacity is actually used when the code is employed to describe a particular set of data. Communication channels, if they are noisy, have only limited capacities to convey information. The techniques of information theory are used to evaluate these limits and find coding schemes that saturate them.In neuroscience applications, the symbols we consider are neuronal responses, and the data sets they describe are stimulus characteristics. In the most complete analyses, which are considered at the end of the chapter, the neuronal response is characterized by a list of action potential firing times. The symbols being analyzed in this case are sequences of action potentials.Computing the entropy and mutual information for spike sequences can be difficult because the frequency of occurrence of many different spike sequences must be determined. This typically requires a large amount of data. For this reason, many information theory analyses use simplified descriptions of the response of a neuron that reduce the number of possible "symbols" that need to be considered. We discuss cases in which the symbols consist of responses described by spike-count firing rates. We also consider the extension to continuous-valued firing rates. Because a reduced description of a spike train can carry no more information than the full spike train itself, this approach provides a lower bound on the actual information carried by the spike train.Entropy is a quantity that, roughly speaking, measures how "interesting" or "surprising" a set of responses is. Suppose that we are given a set of neural responses. If each response is identical, or if only a few different responses appear, we might conclude that this data set is relatively uninteresting. A more interesting set might show a larger range of different responses, perhaps in a highly irregular and unpredictable sequence. How can we quantify this intuitive notion of an interesting set of responsesWe begin by characterizing the responses in terms of their spike-count firing rates, which can take a discrete set of different values. The methods we discuss are based on the probabilities Pr of observing a response with a spike-count rate r. The most widely used measure of entropy, due to Shannon, expresses the "surprise" associated with seeing a response rate r as a function of the probability of getting that response, h The logarithm is the only function that satisfies such an identity for all P. Thus, it only remains to decide what base to use for the logarithm. By convention, base 2 logarithms are used so that information can be compared easily with results for binary systems. To indicate that the base 2 logarithm is being used, information is reported in units of "bits", with bitsThe minus sign makes h a decreasing function of its argument, as required. Note that information is really a dimensionless number. The bit, like the radian for angles, is not a dimensional unit but a reminder that a particular system is being used.Expression quantifies the surprise or unpredictability associated with a particular response. Shannons entropy is just this measure averaged entropy over all responses,In the sum that determines the entropy, the factor h   log 2 Pr is multiplied by the probability that the response with rate r occurs. Responses with extremely low probabilities may contribute little to the total entropy, despite having large h values, because they occur so rarely. In the limit when Pr  0, h  , but an event that does not occur does not contribute to the entropy because the problematic expression 0 log 2 0 is evaluated as  log 2  in the limit   0, which is 0. Very high probability responses also contribute little because they have h  0. The responses that contribute most to the entropy have high enough probabilities so that they appear with a fair frequency, but not high enough to make h too small.Computing the entropy in some simple cases helps provide a feel for what it measures. First, imagine the least interesting situation: when a neuron responds every time by firing at the same rate. In this case, all of the probabilities Pr are 0, except for one of them, which is 1. This means that every term in the sum of equation This entropy, plotted in figure 4.1A, takes its maximum value of 1 bit whenThus, a code consisting of two equally likely responses has one bit of entropy.To convey information about a set of stimuli, neural responses must be different for different stimuli. Entropy is a measure of response variability, but it does not tell us anything about the source of that variability. A neuron can provide information about a stimulus only if its response variability is correlated with changes in that stimulus, rather than being purely random or correlated with other unrelated factors. One way to determine whether response variability is correlated with stimulus variability is to compare the responses obtained using a different stimulus on every trial with those measured in trials involving repeated presentations of the same is the probability of a response at rate r  , andis the probability of the other response, r  . The entropy is maximum whenThe mutual information for a binary encoding of a binary stimulus. P X is the probability of an incorrect response being evoked. The plot shows only P X  12 because values of P X  12 correspond to an encoding in which the relationship between the two responses and the two stimuli is reversed and the error probability is 1  P X .stimulus. Responses that are informative about the identity of the stimulus should exhibit larger variability for trials involving different stimuli than for trials that use the same stimulus repetitively. Mutual information is an entropy-based measure related to this idea.The mutual information is the difference between the total response entropy and the average response entropy on trials that involve repetitive presentation of the same stimulus. Subtracting the entropy when the stimulus does not change removes from the total entropy the contribution from response variability that is not associated with the identity of the stimulus. When the responses are characterized by a spike-count rate, the total response entropy is given by equation 4.3. The entropy of the responses evoked by repeated presentations of a given stimulus s is computed using the conditional probability P If we average this quantity over all the stimuli, we obtain a quantity called the noise entropy noise entropyThis is the entropy associated with that part of the response variability that is not due to changes in the stimulus, but arises from other sources. The mutual information is obtained by subtracting the noise entropy from the full response entropy, which from equations 4.3 and 4.6 givesThe probability of a response r is related to the conditional probability Prs and the probability Ps that stimulus s is presented by the identity,Using this, and writing the difference of the two logarithms in equation 4.7 as the logarithm of the ratio of their arguments, we can rewrite the mutual mutual information information asRecall from chapter 3 thatwhere P This equation reveals that the mutual information is symmetric with respect to interchange of s and r, which means that the mutual information that a set of responses conveys about a set of stimuli is identical to the mutual information that the set of stimuli conveys about the responses. To see this explicitly, we apply equation 4.10 again to writeThis result is the same as equation 4.7, except that the roles of the stimulus and the response have been interchanged. Equation 4.12 shows how response variability limits the ability of a spike train to carry information. The second term on the right side, which is negative, is the average uncertainty about the identity of the stimulus given the response, and reduces the total stimulus entropy represented by the first term.To provide some concrete examples, we compute the mutual information for a few simple cases. First, suppose that the responses of the neuron are completely unaffected by the identity of the stimulus. The last expression, which follows from the fact that Pr s   P Finally, imagine that there are only two possible stimulus values, which we label  and , and that the neuron responds with just two rates, r  and r  . We associate the response r  with the  stimulus, and the response r  with the  stimulus, but the encoding is not perfect. The probability of an incorrect response is P X , meaning that for the correct responses PrWe assume that the two stimuli are presented with equal probability so that Pr    Pr    12, which, from equation 4.4, makes the full response entropy 1 bit. The noise entropy is  log 2  P X log 2 P X . Thus, the mutual information isThis is plotted in It is instructive to consider this example from the perspective of decoding. We can think of the neuron as being a communication channel that reports noisily on the stimulus. From this perspective, we want to know the probability that a  was presented, given that the response r  was recorded. By Bayes theorem, this is PBefore the response is recorded, the expectation was that  and  were equally likely. If the response r  is recorded, this expectation changes to 1  P X . The mutual information measures the corresponding reduction in uncertainty or, equivalently, the tightening of the posterior distribution due to the response.The mutual information is related to a measure used in statistics called the Kullback-Leibler divergence. The KL divergence between one KL divergenceThe KL divergence has a property normally associated with a distance measure, D KL  0 with equality if and only if P  Q. However, unlike a distance, it is not symmetric with respect to interchange of P and Q. . Thus, the mutual information is the KL divergence between the actual probability distribution P Up to now we have characterized neural responses using discrete spikecount rates. As in chapter 3, it is often convenient to treat these rates instead as continuous variables. There is a complication associated with entropies that are defined in terms of continuous response variables. If we could measure the value of a continuously defined firing rate with unlimited accuracy, it would be possible to convey an infinite amount of information using the endless sequence of decimal digits of this single variable. Of course, practical considerations always limit the accuracy with which a firing rate can be measured or conveyed.To define the entropy associated with a continuous measure of a neural response, we must include some limit on the measurement accuracy. The effects of this limit typically cancel in computations of mutual information because the mutual information is the difference between two entropies. In this section, we show how entropy and mutual information are computed for responses characterized by continuous firing rates. For completeness, we also treat the stimulus parameter s as a continuous variable. This means that the probability Ps is replaced by the probability density p For a continuously defined firing rate, the probability of the firing rate lying in the range between r and r  r, for small r, is expressed in terms of a probability density as pr r. Summing over discrete bins of size r, we find, by analogy with equation,To extract the last term we have expressed the logarithm of a product as the sum of two logarithms and used the fact that the sum of the response probabilities is 1. We would now like to take the limit r  0 but we cannot, because the log 2 r term diverges in this limit. This divergence reflects the fact that a continuous variable measured with perfect accuracy has infinite entropy. However, for reasonable p r is best thought of as a limit on the resolution with which the firing continuous entropy rate can be measured. Unless this limit is known, the entropy of a probability density for a continuous variable can be determined only up to an additive constant. However, if two entropies computed with the same resolution are subtracted, the troublesome term involving r cancels, and we can proceed without knowing its precise value. Note that the factor of log 2 r cancels in the expression for the mutual information because both entropies are evaluated at the same resolution.In chapter 3, we described the Fisher information as a local measure of how tightly the responses determine the stimulus. The Fisher information is local because it depends on the expected curvature of the likelihood Prs evaluated at the true stimulus value. The mutual information is a global measure in the sense that it depends on the average overall uncertainty in the decoding distribution p Entropy and mutual information are useful quantities for characterizing the nature and efficiency of neural encoding and selectivity. Often, in addition to such characterizations, we seek to understand the computational implications of an observed response selectivity. For example, we might ask whether neural responses to natural stimuli are optimized to convey as much information as possible. This hypothesis can be tested by computing the response characteristics that maximize the mutual information conveyed about naturally occurring stimuli and comparing the results with responses observed experimentally.Because the mutual information is the full response entropy minus the noise entropy, maximizing the information involves a compromise. We must make the response entropy as large as possible without allowing the noise entropy to get too big. If the noise entropy is small, maximizing the response entropy, subject to an appropriate constraint, maximizes the mutual information to a good approximation. We therefore begin our discussion by studying how response entropy can be maximized. Later in the discussion, we will consider the effects of noise entropy.Constraints play a crucial role in this analysis. We have already seen that the theoretical information-carrying capacity associated with a continuous firing rate is limited only by the resolution with which the firing rate can be defined. Even with a finite resolution, a firing rate could convey an infinite amount of information if it could take arbitrarily high values. Thus, we must impose some constraint that limits the firing rate to a realistic range. Possible constraints include limiting the maximum allowed firing rate or holding the average firing rate or its variance fixed.To maximize the response entropy, we must find a probability density p The result, computed using Lagrange multipliers, is that the probability density that maximizes the entropy sub-ject to this constraint is a constant,independent of r. The entropy for this probability density, for finite firingrate resolution r, is H  log 2 r max  log 2 r  log 2 r max r .Equation For small s, the probability that the continuous stimulus variable falls in the range between s and s  s is given in terms of the stimulus probability density by p Consider the case of a monotonically increasing response so that f  f for positive s. Then, in the limit s  0, the equalization condition becomeswhich has the solutionwhere s min is the minimum value of s, which is assumed to generate no response. Thus, entropy maximization requires that the average firing rate of the responding neuron be proportional to the integral of the probability density of the stimulus. When a population of neurons encodes a stimulus, optimizing their individual response properties will not necessarily lead to an optimized population response. Optimizing individual responses could result in a highly redundant population representation in which different neurons encode the same information. Entropy maximization for a population requires that the neurons convey independent pieces of information. Let the vector r with components r a for a  1, 2, . . . , N denote the firing rates for a population of N neurons, measured with resolution r. If pr is the probability of evoking a population response characterized by the vector r, the entropy for the entire population response isAlong with the full population entropy of Equation 4.26, we can also con-sider the entropy associated with individual neurons within the popula-is the probability density for response r a from neuron a, its entropy isThe true population entropy can never be greater than the sum of these individual neuron entropies over the entire population,To prove this, we note that the difference between the full entropy and the sum of individual neuron entropies isThe inequality follows from the fact that the middle expression is the KL divergence between the probability distributions pr and a pr a , and a KL divergence is always nonnegative. Equality holds only ifthat is, if the responses of the neurons are statistically independent. Thus, the full response entropy is never greater than the sum of the entropies of the individual neurons in the population, and it reaches the limiting value when equation 4.30 is satisfied. A code that satisfies this condition is called factorial code a factorial code because the probability factorizes into a product of single neuron probabilities. When the population-response probability density factorizes, this implies that the individual neurons respond independently. The entropy difference in equation 4.29 has been suggested as a measure of redundancy. redundancy Combining this result with the results of the previous section, we conclude that the maximum population-response entropy can be achieved by satisfying two conditions. First, the individual neurons must respond independently, which means that pr  a pr a  must factorize. Second, factorization they must all have response probabilities that are optimal for whatever constraints are imposed. If the same constraint is imposed on every neuron, the second condition implies that every neuron must have the same response probability density. In other words, pr a  must be the same for all a values, a property called probability equalization. This does not imply that all the neurons respond identiprobability equalization cally to every stimulus. Indeed, the conditional probabilities pr a s must be different for different neurons if they are to act independently. We proceed by considering factorization and probability equalization as general principles of entropy maximization, without imposing explicit constraints.Exact factorization and probability equalization are difficult to achieve, especially if the form of the neural response is restricted. These goals are likely to be impossible to achieve, for example, if the neural responses are modeled as having a linear relation to the stimulus. A more modest goal is to require that the lowest-order moments of the population-response probability distribution match those of a fully factorized and equalized distribution. If the individual response probability distributions are equal, the average firing rates and firing rate variances will be the same for all neurons, r a  r and 2   2 r for all a. Furthermore, the covariance matrix for a factorized and probability-equalized population distribution is proportional to the identity matrix,Finding response distributions that satisfy only the decorrelation and varidecorrelation and variance equalization ance equalization condition of equation 4.31 is usually tractable. In the following examples, we restrict ourselves to this easier task. This maximizes the entropy only if the statistics of the responses are Gaussian, but it is a reasonable procedure even in a non-Gaussian case, because it typically reduces the redundancy in the population code and spreads the load of information transmission equally among the neurons.Entropy and information maximization have been used to explain properties of visual receptive fields in the retina, LGN, and primary visual cortex. The basic assumption is that these receptive fields serve to maximize the amount of information that the associated neural responses convey about natural visual scenes in the presence of noise. Information theoretical analyses are sensitive to the statistical properties of the stimuli being represented, so the statistics of natural scenes play an important role in these studies. Natural scenes exhibit substantial spatial and temporal redundancy. Maximizing the information conveyed requires removing this redundancy from the neural responses.It should be kept in mind that the information maximization approach sets limited goals and requires strong assumptions about the nature of the constraints relevant to the nervous system. In addition, the approach analyzes only the representational properties of neural responses and ignores the computational goals of the visual system, such as object recognition or target tracking. Finally, maximizing other measures of performance, different from the mutual information, may give similar results. Nevertheless, the principle of information maximization is quite successful at accounting for properties of receptive fields early in the visual pathway.In chapter 2, a visual image was defined by a contrast function s with a trial-averaged value of 0. For the calculations we present here, it is more convenient to express the x and y coordinates for locations on the viewing screen in terms of a single vector x , or sometimes y . Using this notation, the linear estimate of the response of a visual neuron discussed in chapter 2 can be written asand we can rewrite L as the product of integrals involving temporal and spatial filters. To keep the notation simple, we assume that the stimulus can also be separated, so that swhereandIn the following, we analyze the spatial and temporal components, D s and D t , separately by considering the information-carrying capacity of L s and L t . We study the spatial receptive fields of retinal ganglion cells in this section, and the temporal response properties of LGN cells in the next. Later, we discuss the application of information maximization ideas to primary visual cortex.To derive appropriately optimal spatial filters, we consider an array of retinal ganglion cells with receptive fields covering a small patch of the retina. We assume that the statistics of the input are spatially stationary or translation-invariant. This means that all locations and directions in space, at least within the patch we consider, are equivalent. This equivalence allows us to give all of the receptive fields the same spatial structure, with the receptive fields of different cells merely being shifted to different points within the visual field. As a result, we write the spatial kernel describing a retinal ganglion cell with receptive field centered at the point a as D s. The linear response of this cell is thenNote that we are labeling the neurons by the locations a of the centers of their receptive fields rather than by an integer index such as i. This is a convenient labeling scheme that allows sums over neurons to be replaced by sums over parameters describing their receptive fields. The vectors a for the different neurons take on discrete values corresponding to the different neurons in the population. If many neurons are being considered, these discrete vectors may fill the range of receptive field locations quite densely. In this case, it is reasonable to approximate the large but discrete set of a values with a vector a that is allowed to vary continuously. In other words, as an approximation, we proceed as if there were a neuron corresponding to every continuous value of a. This allows us to treat L as a function of a and to replace sums over neurons with integrals over a. In the case we are considering, the receptive fields of retinal ganglion cells cover the retina densely, with many receptive fields overlapping each point on the retina, so the replacement of discrete sums over neurons with continuous integrals over a is quite accurate.We will not attempt a complete entropy maximization for the case of retinal ganglion cells. Instead, we follow the approximate procedure of setting the correlation matrix between different neurons within the population proportional to the identity matrix. The relevant correlation is the average, over all stimuli, of the product of the linear responses of two cells, with receptive fields centered at a and b, The average here, denoted by angle brackets, is not over trials but over the set of natural scenes for which we believe the receptive field is optimized. By analogy with equation 4.31, decorrelation and variance equalization of the different retinal ganglion cells, when a and b are taken to be continuous variables, require that we set this correlation function proportional to a  function,This is the continuous variable analog of making a discrete correlation matrix proportional to the identity matrix. The  function with vector arguments is nonzero only when all of the components of a and b are identical.The quantity s ss s in equation 4.36 is the correlation function of the stimulus averaged over natural scenes. Our assumption of homogeneity implies that this quantity is only a function of the vector difference x  y, and we write it asTo determine the form of the receptive field filter that is optimal, we must solve equation 4.37 for D s . This is done by expressing D s and Q ss in terms of their Fourier transforms D s  and Q ss  ,Q ss  , which is real and nonnegative, is also called the stimulus power spectrum. In terms of these Fourier transforms, equation 4.37 becomesfrom which we findThe linear kernel described by equation 4.42 exactly compensates for whitening filter whatever dependence the Fourier transform of the stimulus correlation function has on the spatial frequency , making the product Q ss D s  2 independent of . This product is the power spectrum of L. The output of the optimal filter has a power spectrum that is independent of spatial frequency, and therefore has the same characteristics as white noise. Therefore, the kernel in equation 4.42 is called a whitening filter. Different spatial frequencies act independently in a linear system, so decorrelation and variance equalization require them to be utilized at equal signal strength.The calculation we have performed determines only the amplitude D s , and not D s  itself. Thus, decorrelation and variance equalization do not uniquely specify the form of the linear kernel. We study some consequences of the freedom to choose different linear kernels satisfying equation 4.42 later in the chapter.The spatial correlation function for natural scenes has been measured, with the result that Q ss  is proportional to 1  2 over the range it has been evaluated. The behavior near   0 is not well established, but the divergence of 1  2 near   0 can be removed by setting Q ss  proportional to 1 where  0 is a constant. The stimuli of interest in the calculation of retinal ganglion receptive fields are natural images as they appear on the retina, not in the photographs from which the natural scenes statistics are measured. An additional factor must be included in Q ss  to account for filtering introduced by the optics of the eye. A simple model of the optical modulation transfer function results in an exponential correction to the stimulus correlation optical modulation transfer function function,with  a parameter. Substituting this into equation 4.42 gives the rather peculiar result that the amplitude D s , being proportional to the inverse of the square root of Q ss  , is predicted to grow exponentially for large  . Whitening filters maximize entropy by equalizing the distribution of response power over the entire spatial frequency range. High spatial frequency components of images are relatively rare in natural scenes and, even if they occur, are greatly attenuated by the eye. The whitening filter compensates for this by boosting the responses to high spatial frequencies. Although this is the result of the entropy maximization calculation, it is not a good strategy to use in an unrestricted way for visual processing. Real inputs to retinal ganglion cells involve a mixture of true signal and noise coming from biophysical sources in the retina. At high spatial frequencies, for which the true signal is weak, inputs to retinal ganglion cells are likely to be dominated by noise, especially in low-light conditions. Boosting the amplitude of this noise-dominated input and transmitting it to the brain is not an efficient visual encoding strategy.The problem of excessive boosting of responses at high spatial frequency arises in the entropy maximization calculation because no distinction has been made between the entropy coming from true signals and that coming from noise. To correct this problem, we should maximize the information transmitted by the retinal ganglion cells about natural scenes, rather than maximize the entropy. A full information-maximization calculation of the receptive field properties of retinal ganglion cells can be performed, but this requires introducing a number of assumptions about the constraints that are relevant, and it is not entirely obvious what these constraints should be. Instead, we will follow an approximate procedure that prefilters the input to eliminate as much noise as possible, and then uses the results of this section to maximize the entropy of a linear filter acting on the prefiltered input signal.Suppose that the visual stimulus on the retina is the sum of the true stimulus s s that should be conveyed to the brain and a noise term  that reflects image distortion, photoreceptor noise, and other signals that are not worth conveying beyond the retina. To deal with such a mixed input signal, we express the Fourier transform of the linear kernel D s  as a product of two terms: a noise filter, D , that eliminates as much of the noise as possible and a whitening filter, D  w, that satisfies equation 4.42. The Fourier transform of the complete filter is thenTo determine the form of the noise filter, we demand that when it is applied to the total input s s  , the result is as close to the signal part of the input, s s, as possible. The problem of minimizing the average squared difference between the filtered noisy signal and the true signal is formally the same as the problems we solved in chapter 2 and chapter 3 to determine the optimal kernels for rate prediction and for spike decoding. The general solution is that the Fourier transform of the optimal filter is the Fourier transform of the cross-correlation between the quantity being filtered and the quantity being approximated divided by the Fourier transform of the autocorrelation of the quantity being filtered. In the present example, there is a slight complication that the integral in equation 4.35 is not in the form of a convolution, because D s is written as a function of x  a rather than a  x. However, in the case we consider, this ultimately makes no difference to the final answer. The calculation simplifies because we assume that the signal and noise terms are uncorrelated, so that s s  0. Then, the relevant crosscorrelation for this problem isand the autocorrelation iswhere Q ss and Q  are, respectively, the stimulus and noise autocorrelations functions. These results imply that the optimal noise filter is real and given, in terms of the Fourier transforms of Q ss and Q  , by noise filterBecause the noise filter is designed so that its output matches the signal as closely as possible, we make the approximation of using the same whitening filter as before When the noise level is high, the structure of the optimal receptive field is different. In spatial frequency terms, the filter is now low-pass, and the receptive field loses its surround. This structure averages over neighboring pixels to extract the true signal obscured by the uncorrelated noise. In the retina, we expect the signal-to-noise ratio to be controlled by the level of ambient light, with low levels of illumination corresponding to the high-noise case. The predicted change in the receptive fields at low illumination matches what actually happens in the retina. At low light levels, circuitry changes within the retina remove the opposing surrounds from retinal ganglion cell receptive fields.Natural images tend to change relatively slowly over time. This means that there is substantial redundancy in the succession of natural images, suggesting an opportunity for efficient temporal filtering to complement efficient spatial filtering. An analysis similar to that of the previous section can be performed to account for the temporal receptive fields of visually responsive neurons early in the visual pathway. Recall that the predicted linear temporal response is given by L t, as expressed in equation 4.34. The analog of equation 4.37 for temporal decorrelation and variance equalization isThis is mathematically identical to equation 4.37 except that the role of the spatial variables a and b has been replaced by the temporal variables t and t  . The analysis proceeds exactly as above, and the optimal filter is the product of a noise filter and a temporal whitening filter, as before. The temporal linear kernel D t is written in terms of its Fourier transform,  In this case, Q ss  and Q  are the power spectra of the signal and the noise in the temporal domain. where  0 is a constant. The resulting filter, in both the temporal frequency and the time domains, is plotted in figure 4.4. An interesting test of the notion of optimal coding was carried out by Computational concerns beyond mere linear information transfer are likely to be relevant at the level of cortical processing of visual images. For one thing, the primary visual cortex has many more neurons than theLGN, yet they can collectively convey no more information about the visual world than they receive. As we saw in chapter 2, neurons in primary visual cortex are selective for quantities, such as spatial frequency and orientation, that are of particular importance in relation to object recognition but not for information transfer. Nevertheless, the methods described in the previous section can be used to understand restricted aspects of receptive fields of neurons in primary visual cortex, namely, the way that their multiple selectivities are collectively assigned. For example, cells that respond best at high spatial frequencies tend to respond more to low temporal frequency components of images, and vice versa.The stimulus power spectrum written as a function of both spatial and temporal frequency has been estimated as Q ss .6 Dependence of temporal frequency tuning on preferred spatial frequency for space-time receptive fields derived from information maximization in the presence of noise. The curves show a transition from partial whitening in temporal frequency for low preferred spatial frequency to temporal summation for high preferred spatial frequency. If instead the cell is selective for low spatial frequencies, the signal dominates the noise up to higher temporal frequencies, and the whitening filter causes the response to increase as a function of temporal frequency up to a maximum value where the noise filter begins to suppress the response. Receptive fields with preference for high spatial frequency thus act as low-pass temporal filters, and receptive fields with selectivity for low spatial frequency act as bandpass temporal filters.Similar conclusions can be drawn concerning other joint selectivities. For example, color-selective cells tend to be selective for low temporal frequencies, because their input signal-to-noise ratio is lower than that for broadband cells. There is also an interesting predicted relationship between ocular dominance and spatial frequency tuning due to the nature of the correlations between the two eyes. Optimal receptive fields with low spatial frequency tuning have enhanced sensitivity to differences between inputs coming from the two eyes. Receptive fields tuned to intermediate and high spatial frequencies suppress ocular differences.Computing the entropy or information content of a neuronal response characterized by spike times is much more difficult than computing these quantities for responses described by firing rates. Nevertheless, these computations are important, because firing rates are incomplete descriptions that can lead to serious underestimates of the entropy and information. In this section, we discuss how the entropy and mutual information can be computed for spike trains. Extensive further discussion can be found in the book by Spike-train entropy calculations are typically based on the study of longduration recordings consisting of many action potentials. The entropy or mutual information typically grows linearly with the length of the spike train being considered. For this reason, the entropy and mutual information of spike trains are reported as entropy or information rates. These are entropy and information rates the total entropy or information divided by the duration of the spike train. We write the entropy rate as rather than H. Alternatively, entropy and mutual information can be divided by the total number of action potentials and reported as bits per spike rather than bits per second.To compute entropy and information rates for a spike train, we need to determine the probabilities that various temporal patterns of action potentials appear. These probabilities replace the factors P The probability of an interspike interval falling in the range between  and    is given in terms of the interspike interval probability density by p Note that the variable T s is used here to denote the duration of the spike sequence being considered, while T, which is much larger than T s , is the duration of the entire spike train.The time that a spike occurs is a continuous variable, so, as in the case of interspike intervals, a resolution must be specified when spike train entropies are computed. This can be done by dividing time into discrete bins of size t. We assume that the bins are small enough so that not more than one spike appears in a bin. Depending on whether or not a spike occurred within it, each bin is labeled by a 0 or a 1. A spike sequence defined over a block of duration T s is thus represented by a string of T s  t zeros and ones. We denote such a sequence by B, where B is a T s  t bit binary number, and t specifies the time of the first bin in the sequence being considered. Both T s and t are integer multiples of the bin size t.The probability of a sequence B occurring at any time during the entire response is denoted by P If the spike sequences in nonoverlapping intervals of duration T s are independent, the full spike-train entropy rate is also given by equation 4.54. For any finite data set, T s cannot be increased past a certain point, because there will not be enough spike sequences of duration T s in the data set to determine their probabilities. Thus, in practice, T s must be increased until the point where the extraction of probabilities becomes problematic, and some form of extrapolation to T s   must be made.Statistical mechanics arguments suggest that the difference between the entropy for finite T s and the true entropy for T s   should be proportional to 1 T s for large T s . Therefore, the true entropy can be estimated, as in figure 4.7, by linearly extrapolating a plot of the entropy rate versus 1 T s to the point 1 T s  0. In figure 4.7, this has been done for data from the motion-sensitive H1 neuron of the fly visual system. The plotted points show entropy rates computed for different values of 1 T s , and they vary linearly over most of the range of the plot. However, when 1 T s goes below about 20s, the dependence suddenly changes. This is the point at which the amount of data is insufficient to extract even an overestimate of the entropy. By linearly extrapolating the linear part of the series of computed points in figure 4.7, Strong et al. estimated that the H1 spike trains had an entropy rate of 157 bitss when the resolution was t  3 ms.To compute the mutual information rate for a spike train, we must subtract the noise entropy rate from the full spike-train entropy rate. The noise entropy rate is determined from the probabilities of finding various sequences B, given that they were evoked by the same stimulus. This is done by considering sequences B that start at a fixed time t. If the same stimulus is used in repeated trials, sequences that begin at time t in every trial are generated by the same stimulus. Therefore, the conditional probability of the response, given the stimulus, is in this case the distribution PB for response sequences beginning at time t. This is obtained by determining the fraction of trials on which B was evoked. Note that P The full noise entropy is computed by averaging the noise entropy at time t over all t values. The average over t plays the role of the average over The straight line is a linear extrapolation to 1 T s  0, which corresponds to T s  . The lower trace shows the spike train noise entropy rate for different values of 1 T s . The straight line is again an extrapolation to 1 T s  0. Both entropy rates increase as functions of 1 T s , and the true spike-train and noise entropy rates are overestimated at large values of 1 T s . At 1 T s  20s, there is a sudden shift in the dependence. This occurs when there is insufficient data to compute the spike sequence probabilities. The difference between the y intercepts of the two straight lines plotted is the mutual information rate. The resolution is t  3 ms. If equation 4.55 is based on finite-length spike sequences, it provides an upper bound on the noise entropy rate. The true noise entropy rate is estimated by performing a linear extrapolation in 1 T s to 1 T s  0, as was done for the spike-train entropy rate. The result, shown in figure 4.7, is a noise entropy of 79 bitss for t  3 ms. The information rate is obtained by taking the difference between the extrapolated values for the spike-train and noise entropy rates. The result for the fly H1 neuron used in figure 4.7 is an information rate of 157 -79  78 bitss or 1.8 bitsspike. Values in the range 1 to 3 bitsspike are typical results of such calculations for a variety of preparations.Both the spike-train and noise entropy rates depend on t. The leading dependence, coming from the log 2 t term discussed previously, cancels in the computation of the information rate, but the information can still depend on t through nondivergent terms. This reflects the fact that more information can be extracted from accurately measured spike times than from poorly measured spike times. Thus, we expect the information rate to increase with decreasing t, at least over some range of t values. At some critical value of t that matches the natural degree of noise jitter in the spike timings, we expect the information rate to stop increasing. This value of t is interesting because it tells us about the degree of spike timing accuracy in neural encoding.The information conveyed by spike trains can be used to compare responses to different stimuli and thereby reveal stimulus-specific aspects of neural encoding. For example, Shannons information theory can be used to determine how much a neural response tells both us and, presumably, the animal in which the neuron lives, about a stimulus. Entropy is a measure of the uncertainty or surprise associated with a stochastic variable, such as a stimulus. Mutual information quantifies the reduction in uncertainty associated with the observation of another variable, such as a response. The mutual information is related to the Kullback-Leibler divergence between two probability distributions. We defined the response and noise entropies for probability distributions of discrete and continuous firing rates, and considered how the information transmitted about a set of stimuli might be optimized. The principles of entropy and information maximization were used to account for features of the receptive fields of cells in the retina, LGN, and primary visual cortex. This analysis introduced probability factorization and equalization, and whitening and noise filters. Finally, we discussed how the information conveyed about dynamic stimuli by spike sequences can be estimated.The logarithm is a concave function, which means that log 2 z  log 2 z , where the angle brackets denote averaging with respect to some probabilJensens inequality ity distribution and z is any positive quantity. gives a treatment specialized to neural coding. Information theory and theories inspired by it, such as histogram equalization, were adopted in neuroscience and psychology as a way of understanding sensory transduction and coding, as discussed by Barlow and Uttley. We followed a more recent set of studies, inspired by The statistics of natural sensory inputs is reviewed by Field. We followed the technique of A great deal is known about the biophysical mechanisms responsible for generating neuronal activity, and this knowledge provides a basis for constructing neuron models. Such models range from highly detailed descriptions involving thousands of coupled differential equations to greatly simplified caricatures useful for studying large interconnected networks.In this chapter, we discuss the basic electrical properties of neurons and the mathematical models by which they are described. We present a simple but nevertheless useful model neuron, the integrate-and-fire model, in a basic version and with added membrane and synaptic conductances. We also discuss the Hodgkin-Huxley model, which describes the conductances responsible for generating action potentials. In chapter 6, we continue by presenting more complex models, in terms of their conductances and morphology. Circuits and networks of model neurons are discussed in chapter 7. This chapter makes use of basic concepts of electrical circuit theory, which are reviewed in the Mathematical Appendix.Like other cells, neurons are packed with a huge number and variety of ions and molecules. A cubic micron of cytoplasm might contain, for example, 10 10 water molecules, 10 8 ions, 10 7 small molecules such as amino acids and nucleotides, and 10 5 proteins. Many of these molecules carry charges, either positive or negative. Most of the time, there is an excess concentration of negative charge inside a neuron. Excess charges that are mobile, like ions, repel each other and build up on the inside surface of the cell membrane. Electrostatic forces attract an equal density of positive ions from the extracellular medium to the outside surface of the membrane.The cell membrane is a lipid bilayer 3 to 4 nm thick that is essentially cell membrane impermeable to most charged molecules. This insulating feature causes the cell membrane to act as a capacitor by separating the charges lying channel pore lipid bilayer By convention, the potential of the extracellular fluid outside a neuron is defined to be 0. When a neuron is inactive, the excess internal negative charge causes the potential inside the cell membrane to be negative. This membrane potential potential is an equilibrium point at which the flow of ions into the cell matches that out of the cell. The potential can change if the balance of ion flow is modified by the opening or closing of ion channels. Under normal conditions, neuronal membrane potentials vary over a range from about -90 to 50 mV. The order of magnitude of these potentials can be estimated from basic physical principles.Membrane potentials are small enough to allow neurons to take advantage of thermal energy to help transport ions across the membrane, but are large enough so that thermal fluctuations do not swamp the signaling capabilities of the neuron. These conditions imply that potential differences across the cell membrane must lie in a range such that the energy gained or lost by an ion traversing the membrane is the same order of magnitude as its thermal energy. The thermal energy of an ion is about k B T where k B is the Boltzmann constant and T is the temperature on an absolute Kelvin scale. For chemists and biologists, it is more customary to discuss moles of ions rather than single ions. A mole of ions has Avogadros number times as much thermal energy as a single ion, or RT, where R is the universal gas constant, equal to 8.31 joulesmol K   1.99 calmol K  . RT is about 2500 joulesmol or 0.6 kCalmol at normal temperatures.To estimate the size of typical membrane potentials, we equate the thermal energy of a mole of ions to the energy gained or lost when a mole of ions crosses a membrane with a potential difference V T across it. This energy is FV T , where F is the Faraday constant, F  96,480 coulombsmol, equal to Avogadros number times the charge of a single proton, q.This is an important parameter that enters into a number of calculations. V T is between 24 and 27 mV for the typical temperatures of cold-and warm-blooded animals. This sets the overall scale for membrane potentials across neuronal membranes, which range from about -3 to 2 times V T .Membrane potentials measured at different places within a neuron can take different values. For example, the potentials in the soma, dendrite, and axon can all be different. Potential differences between different parts of a neuron cause ions to flow within the cell, which tends to equalize these differences. The intracellular medium provides a resistance to such flow. This resistance is highest for long, narrow stretches of dendritic or axonal cable, such as the segment shown in figure 5.2. The longitudinal current I L flowing along such a cable segment can be computed from Ohms law. longitudinal current I L For the cylindrical segment of dendrite shown in figure 5.2, the longitudinal current flowing from right to left satisfiesHere, R L is the longitudinal resistance, which grows in proportion to the length of the longitudinal resistance R L segment and is inversely proportional to the cross-sectional area of the segment. The constant of proportionality, called the intracellular resistivity, r L , typically falls in a range from 1 intracellular resistivity r L to 3 k mm. The longitudinal resistance of the segment in figure 5.2 is r L times the length L divided by the cross-sectional area a 2 , R L  r L La 2 . A segment 100 m long with a radius of 2 m has a longitudinal resistance of about 8 M . A voltage difference of 8 mV would be required to force 1 nA of current down such a segment.We can also use the intracellular resistivity to estimate crudely the conductance of a single channel. The conductance, being the inverse of a resistance, is equal to the cross-sectional area of the channel pore divided by The intracellular resistance to current flow can cause substantial differences in the membrane potential measured in different parts of a neuron, especially during rapid transient excursions of the membrane potential from its resting value, such as action potentials. Neurons that have few of the long, narrow cable segments that produce high longitudinal resistances may have relatively uniform membrane potentials across their surfaces. Such neurons are termed electrotonically compact. For electrotonelectrotonic compactness ically compact neurons, or for less compact neurons in situations where spatial variations in the membrane potential are not thought to play an important functional role, the entire neuron may be adequately described by a single membrane potential. Here, we discuss the membrane capacitance and resistance using such a description. An analysis for the case of spatially varying membrane potentials is presented in chapter 6.We have mentioned that there is typically an excess negative charge on the inside surface of the cell membrane of a neuron, and a balancing positive charge on its outside surface. In this arrangement, the membrane capacitance C m cell membrane creates a capacitance C m , and the voltage across the membrane V and the amount of this excess charge Q are related by the standard equation for a capacitor, Q  C m V. The membrane capacitance is proportional to the total amount of membrane or, equivalently, to the surface area of the cell. The constant of proportionality, called the specific membrane capacitance, is the capacitance per unit area of membrane, and specific membrane capacitance c m it is approximately the same for all neurons, c m  10 nFmm 2 . The total capacitance C m is the membrane surface area A times the specific capacitance, C m  c m A. Neuronal surface areas tend to be in the range 0.01 to 0.1 mm 2 , so the membrane capacitance for a whole neuron is typically 0.1 to 1 nF. For a neuron with a total membrane capacitance of 1 nF, 7  10 11 coulomb or about 10 9 singly charged ions are required to produce a resting potential of -70 mV. This is about 1100,000 of the total number of ions in a neuron and is the amount of charge delivered by a 0.7 nA current in 100 ms.We can use the membrane capacitance to determine how much current is required to change the membrane potential at a given rate. The time derivative of the basic equation relating the membrane potential and charge,plays an important role in the mathematical modeling of neurons. The time derivative of the charge dQdt is equal to the current passing into the cell, so the amount of current needed to change the membrane potential of a neuron with a total capacitance C m at a rate dVdt is C m dVdt. For example, 1 nA will change the membrane potential of a neuron with a capacitance of 1 nF at a rate of 1 mVms.The capacitance of a neuron determines how much current is required to make the membrane potential change at a given rate. Holding the membrane potential steady at a level different from its resting value also requires current, but this current is determined by the membrane resistance rather than by the capacitance of the cell. For example, if a small constant current I e is injected into a neuron through an electrode, as in The membrane resistance is the inverse of the membrane conductance, and, like the capacitance, the conductance of a piece of cell membrane membrane conductance is proportional to its surface area. The constant of proportionality is the membrane conductance per unit area, but we write it as 1r m , where r m is called the specific membrane resistance. Conversely, the membrane resisspecific membrane resistance r m tance R m is equal to r m divided by the surface area. When a neuron is in a resting state, the specific membrane resistance is around 1 M mm 2 . This number is much more variable than the specific membrane capacitance. Membrane resistances vary considerably among cells, and under different conditions and at different times for a given neuron, depending on the number, type, and state of its ion channels. For total surface areas between 0.01 and 0.1 mm 2 , the membrane resistance is typically in the range 10 to 100 M . With a 100 M membrane resistance, a constant current of 0.1 nA is required to hold the membrane potential 10 mV away from its resting value.The product of the membrane capacitance and the membrane resistance is a quantity with the units of time called the membrane time constant,  m  membrane time constant  m R m C m . Because C m and R m have inverse dependences on the membrane surface area, the membrane time constant is independent of area and equal to the product of the specific membrane capacitance and resistance,  m  r m c m . The membrane time constant sets the basic time scale for changes in the membrane potential and typically falls in the range between 10 and 100 ms.Electric forces and diffusion are responsible for driving ions through channel pores. Voltage differences between the exterior and interior of the cell produce forces on ions. Negative membrane potentials attract positive ions into the neuron and repel negative ions. In addition, ions diffuse through channels because the ion concentrations differ inside and outside the neuron. These differences are maintained by the ion pumps within the cell membrane. The concentrations of Na  and Ca 2 are higher outside the cell than inside, so these ions are driven into the neuron by diffusion. K  is more concentrated inside the neuron than outside, so it tends to diffuse out of the cell.It is convenient to characterize the current flow due to diffusion in terms of an equilibrium potential. This is defined as the membrane potential at equilibrium potential which current flow due to electric forces cancels the diffusive flow. For channels that conduct a single type of ion, the equilibrium potential can be computed easily. The potential difference across the cell membrane biases the flow of ions into or out of a neuron. Consider, for example, a positively charged ion and a negative membrane potential. In this case, the membrane potential opposes the flow of ions out of the cell. Ions can cross the membrane and leave the interior of the cell only if their thermal energy suffices to overcome the energy barrier produced by the membrane poten-tial. If the ion has an electric charge zq, where q is the charge of one proton, it must have a thermal energy of at least zqV to cross the membrane. The probability that an ion has a thermal energy greater than or equal to zqV, when the temperature is T, is exp. This is determined by integrating the Boltzmann distribution for energies greater than or equal to zqV.In molar units, this result can be written as exp, which is equal to exp by equation 5.1.The biasing effect of the electrical potential can be overcome by an opposing concentration gradient. A concentration of ions inside the cell, Solving this equation for E, we find Nernst equation.4 is the Nernst equation. The reader can check that if the result is derived for either sign of ionic charge or membrane potential, the result is identical to 5.4, which thus applies in all cases. The equilibrium potential for a K  conducting channel, labeled E K , typically falls in the range between -70 and -90 mV. The Na  equilibrium potential, E Na , is 50 mV or higher, and E Ca , for Ca 2 channels, is higher still, around 150 mV. Finally, Cl  equilibrium potentials are typically around -60 to -65 mV, near the resting potential of many neurons.The Nernst equation applies when the channels that generate a particular conductance allow only one type of ion to pass through them. Some channels are not so selective, and in this case the potential E is not determined by equation 5.4. Instead, it takes a value intermediate between the equilibrium potentials of the individual ion types that it conducts. An approximate formula, known as the Goldman equation A conductance with an equilibrium or reversal potential E tends to move the membrane potential of the neuron toward the value E. When V  E, this means that positive current will flow outward, and when V  E, positive current will flow inward. Because Na  and Ca 2 conductances have positive reversal potentials, they tend to depolarize a neuron. K  conductances, with their negative E values, normally hyperpolarize a neuron. Cl  conductances, with reversal potentials near the resting potential, may pass little net current. Instead, their primary impact is to change the membrane resistance of the cell. Such conductances are sometimes called shunting, although all conductances "shunt", shunting conductances that is, increase the total conductance of a neuron. Synaptic conductances are also characterized by reversal potentials and are termed excitatory or inhibitory on this basis. Synapses with reversal potentials less than the threshold for action potential generation are typically called inhibitory, inhibitory and excitatory synapses and those with reversal potentials above the action potential threshold are called excitatory.The total current flowing across the membrane through all of its ion channels is called the membrane current of the neuron. By convention, the membrane current is defined as positive when positive ions leave the neuron and negative when positive ions enter the neuron. The total membrane current is determined by summing currents due to all of the different types of channels within the cell membrane, including voltage-dependent and synaptic channels. To facilitate comparisons between neurons of different sizes, it is convenient to use the membrane current per unit area of cell membrane, which we call i m . The total membrane current is obtained from membrane current per unit area i m i m by multiplying it by A, the total surface area of the cell.We label the different types of channels in a cell membrane with an index i. As discussed in the last section, the current carried by a set of channels of type i with reversal potential E i , vanishes when the membrane potential satisfies V  E i . For many types of channels, the current increases or decreases approximately linearly when the membrane potential deviates from this value. The difference V  E i is called the driving force, and driving force the membrane current per unit area due to the type i channels is written specific conductance g i as g i. The factor g i is the conductance per unit area, or specific conductance, due to these channels. Summing over the different types of channels, we obtain the total membrane current, membrane currentSometimes a more complicated expression called the Goldman-HodgkinKatz formula is used to relate the membrane current to g i and membrane potential Much of the complexity and richness of neuronal dynamics arises because membrane conductances change over time. However, some of the factors that contribute to the total membrane current can be treated as relatively constant, and these are typically grouped together into a single term called the leakage current. The currents carried by ion pumps that mainleakage current tain the concentration gradients that make equilibrium potentials nonzero typically fall into this category. For example, one type of pump uses the energy of ATP hydrolysis to move three Na  ions out of the cell for every two K  ions it moves in.It is normally assumed that ion pumps work at relatively steady rates so that the currents they generate can be included in a time-independent leakage conductance. Sometimes, this assumption is dropped and explicit pump currents are modeled. In either case, all of the time-independent contributions to the membrane current can be lumped together into a single leakage term g L. Because this term hides many sins, its reversal potential E L is not usually equal to the equilibrium potential of any specific ion. Instead, it is often kept as a free parameter and adjusted to make the resting potential of the model neuron match that of the cell being resting potential modeled. Similarly, g L is adjusted to match the membrane conductance at rest. The line over the parameter g L is used to indicate that it has constant value. A similar notation is used later in this chapter to distinguish variable conductances from the fixed parameters that describe them. The leakage conductance is called a passive conductance to distinguish it from variable conductances that are termed active.Models that describe the membrane potential of a neuron by a single variable V are called single-compartment models. This chapter deals exclusively with such models. Multi-compartment models, which can describe spatial variations in the membrane potential, are considered in chapter 6. The equations for single-compartment models, like those of all neuron models, describe how charges flow into and out of a neuron and affect its membrane potential. Equation 5.2 provides the basic relationship that determines the membrane potential for a single-compartment model. This equation states that the rate of change of the membrane potential is proportional to the rate at which charge builds up inside the cell. The rate of charge buildup is, in turn, equal to the total amount of current entering the neuron. The relevant currents are those arising from all the membrane and synaptic conductances plus, in an experimental setting, any current injected into the cell through an electrode. From equation 5.2, the sum of these currents is equal to C m dVdt, the total capacitance of the neuron times the rate of change of the membrane potential. Because the membrane current is usually characterized as a current per unit area, i m , it is more convenient to divide this relationship by the surface area of the neuron. Then, the total current per unit area is equal to c m dVdt, where c m  C m A is the specific membrane capacitance. One complication in this procedure is that the electrode current, I e , is not typically expressed as a current per unit area, so we must divide it by the total surface area of the neuron, A. Putting all this together, the basic equation for all single-compartment models isBy convention, current that enters the neuron through an electrode is defined as positive-inward, whereas membrane current is defined as positive-outward. This explains the different signs for the currents in equation 5.6. The membrane current in equation 5.6 is determined by equation 5.5 and additional equations that specify the conductance variables g i . The structure of such a model is the same as that of an electrical circuit, called the equivalent circuit, consisting of a capacitor and a set equivalent circuit of variable and nonvariable resistors corresponding to the different membrane conductances. A neuron will typically fire an action potential when its membrane potential reaches a threshold value of about -55 to -50 mV. During the action potential, the membrane potential follows a rapid, stereotyped trajectory and then returns to a value that is hyperpolarized relative to the threshold potential. As we will see, the mechanisms by which voltage-dependent K  and Na  conductances produce action potentials are well understood and can be modeled quite accurately. On the other hand, neuron models can be simplified and simulations can be accelerated dramatically if the biophysical mechanisms responsible for action potentials are not explicitly included in the model. Integrate-and-fire models do this by stipulating that an action potential occurs whenever the membrane potential of the model neuron reaches a threshold value V th . After the action potential, the potential is reset to a value V reset below the threshold potential, V reset  V th .The basic integrate-and-fire model was proposed by Lapicque in 1907, long before the mechanisms that generate action potentials were understood. Despite its age and simplicity, the integrate-and-fire model is still an extremely useful description of neuronal activity. By avoiding a biophysical description of the action potential, integrate-and-fire models are left with the simpler task of modeling only subthreshold membrane potential dynamics. This can be done with various levels of rigor. In the simplest version of these models, all active membrane conductances are ignored, including, for the moment, synaptic inputs, and the entire membrane conductance is modeled as a single passive leakage It is convenient to multiply equation 5.7 by the specific membrane resistance r m , which in this case is given by r m  1g L . This cancels the factor of g L on the right side of the equation and leaves a factor c m r m   m on the left side, where  m is the membrane time constant of the neuron. The electrode current ends up being multiplied by r m A, which is the total membrane resistance R m . Thus, the basic equation of the passive integrate-andfire models isTo generate action potentials in the model, equation 5.8 is augmented by the rule that whenever V reaches the threshold value V th , an action potential is fired and the potential is reset to V reset . Equation 5.8 indicates that when I e  0, the membrane potential relaxes exponentially with time constant  m to V  E L . Thus, E L is the resting potential of the model cell.The membrane potential for the passive integrate-and-fire model is determined by integrating equation 5.8 and applying the threshold and reset rule for action potential generation. The response of a passive integrate-and-fire model neuron to a time-varying electrode current is shown in The firing rate of an integrate-and-fire model in response to a constant injected current can be computed analytically. When I e is independent of time, the subthreshold potential V can easily be computed by solving equation 5.8, and iswhere V is the value of V at time t  0. This solution can be checked by substituting it into equation 5.8. It is valid for the integrate-and-fire model only as long as V stays below the threshold. Suppose that at t  0, the neuron has just fired an action potential and is thus at the reset potential, so that V  V reset . The next action potential will occur when the membrane potential reaches the threshold, that is, at a time t  t isi whenBy solving this for t isi , the time of the next action potential, we can determine the interspike interval for constant I e , or equivalently its inverse, which we call the interspike-interval firing rate of the neuron,This expression is valid if R m I e  V th  E L  otherwise r isi  0. For sufficiently large values of I e , we can use the linear approximation of the loga- The passive integrate-and-fire model that we have described thus far is based on two separate approximations, a highly simplified description of the action potential and a linear approximation for the total membrane current. If details of the action-potential generation process are not important for a particular modeling goal, the first approximation can be retained while the membrane current is modeled in as much detail as is necessary. We will illustrate this process by developing a heuristic description of spike-rate adaptation using a model conductance that has characteristics similar to measured neuronal conductances known to play important roles in producing this effect.We model spike-rate adaptation by including an additional current in the model,The spike-rate adaptation conductance g sra has been modeled as a K  conductance so, when activated, it will hyperpolarize the neuron, slowing any spiking that may be occurring. We assume that this conductance relaxes to 0 exponentially with time constant  sra through the equationWhenever the neuron fires a spike, g sra is increased by an amount g sra , that is, g sra  g sra  g sra . During repetitive firing, the current builds up in a sequence of steps causing the firing rate to adapt. Figures 5.6B and 5.6C compare the adapting firing pattern of a cortical neuron with the output of the model.As discussed in chapter 1, the probability that a neuron fires is significantly reduced for a short period of time after the appearance of an action potential. Such a refractory effect is not included in the basic integrate-and-fire model. The simplest way of including an absolute refractory period in the model is to add a condition to the basic threshold crossing rule that forbids firing for a period of time immediately after a spike. Refractoriness can be incorporated in a more realistic way by adding a conductance similar to the spike-rate adaptation conductance discussed above, but with a faster recovery time and a larger conductance increment following an action potential. With a large increment, the current can essentially clamp the neuron to E K following a spike, temporarily preventing further firing and producing an absolute refractory period. As this conductance relaxes back to 0, firing will be possible but initially less likely, producing a relative refractory period. When recovery is completed, normal firing can resume.Another scheme that is sometimes used to model refractory effects is to raise the threshold for action-potential generation following a spike and then allow it to relax back to its normal value. Spike-rate adaptation can also be described by using an integrated version of the integrate-and-fire model known as the spike-response model, in which membrane potential waveforms are determined by summing precomputed postsynaptic potentials and after-spike hyperpolarizations. Finally, spike-rate adaptation and other effects can be incorporated into the integrate-and-fire framework by allowing the parameters g L and E L in equation 5.7 to vary with time.Most of the interesting electrical properties of neurons, including their ability to fire and propagate action potentials, arise from nonlinearities .7 Recording of the current passing through a single ion channel. This is a synaptic receptor channel sensitive to the neurotransmitter acetylcholine. A small amount of acetylcholine was applied to the preparation to produce occasional channel openings. In the open state, the channel passes 6.6 pA at a holding potential of -140 mV. This is equivalent to more than 10 7 charges per second passing through the channel, and corresponds to an open channel conductance of 47 pS. In a later section of this chapter, we discuss stochastic models of individual channels based on state diagrams and transition rates. However, most neuron models use deterministic descriptions of the conductances arising from many channels of a given type. This is justified because of the large number of channels of each type in the cell membrane of a typical neuron. If large numbers of channels are present, and if they fluctuate independently of each other, then, from the law of large numbers, the fraction of channels open at any given time is approximately equal to the probability that any one channel is in an open state. This allows us to move between single-channel probabilistic formulations and macroscopic deterministic descriptions of membrane conductances.We have denoted the conductance per unit area of membrane due to a set of ion channels of type i by g i . The open probability of a voltage-dependent conductance depends, as its name suggests, on the membrane potential of the neuron. In this chapter, we discuss models of two such conductances, the so-called delayedrectifier K  and fast Na  conductances. The formalism we present, which is almost universally used to describe voltage-dependent conductances, was developed by The opening of the gate that describes a persistent conductance may involve a number of conformational changes. For example, the delayedrectifier K  conductance is constructed from four identical subunits, and it appears that all four must undergo a structural change for the channel to open. In general, if k independent, identical events are required for a channel to open, P K can be written aswhere n is the probability that any one of the k independent gating events has occurred. Here, n, which varies between 0 and 1, is called a gating or an activation variable, and a description of its voltage and time depenactivation variable n dence amounts to a description of the conductance. We can think of n as the probability of an individual subunit gate being open, and 1  n as the probability that it is closed.  We describe the transition of each subunit gate by a simple kinetic scheme in which the gating transition closed  open occurs at a voltagechannel kinetics dependent rate  n, and the reverse transition, open  closed, occurs at a voltage-dependent rate  n. The probability that a subunit gate opens over a short interval of time is proportional to the probability of finding the gate closed, 1  n, multiplied by the opening rate  n. Likewise, the probability that a subunit gate closes during a short time opening rate  n interval is proportional to the probability of finding the gate open, n, multiplied by the closing rate  n. The rate at which the open probability closing rate  n for a subunit gate changes is given by the difference of these two terms,The first term in equation 5.16 describes the opening process, and the second term the closing process that lowers the probability of being in the configuration with an open subunit gate. Equation 5.16 can be written in another useful form by dividing through byEquation The key elements in the equation that determines n are the opening and closing rate functions  n and  n. These are obtained by fitting experimental data. It is useful to discuss the form that we expect these rate functions to take on the basis of thermodynamic arguments. The state transitions described by  n , for example, are likely to be rate-limited by barriers requiring thermal energy. These transitions involve the movement of charged components of the gate across part of the membrane, so the height of these energy barriers should be affected by the membrane potential. The transition requires the movement of an effective charge, which we denote by qB  , through the potential V. This requires an energy qB  V. The constant B  reflects both the amount of charge being moved and the distance over which it travels. The probability that thermal fluctuations will provide enough energy to surmount this energy barrier is proportional to the Boltzmann factor, exp. Based on this argument, we expect  n to be of the formfor some constant A  . The closing rate  n should be expressed similarly, except with different constants A  and B  . From equation 5.19, we then find that n  is expected to be a sigmoidal function , where V is expressed in mV, and  n and  n are both expressed in units of 1ms. The fit for  n is exactly the exponential form we have discussed, with A   0.125 exp ms 1 and B   V T  0.0125 mV 1 , but the fit for  n uses a different functional form. The dashed curves in figure 5.9 plot the formulas of equation 5.22.Some channels only open transiently when the membrane potential is depolarized because they are gated by two processes with opposite voltage dependences. For the channel in figure 5.8B to conduct, both gates must be open, and assuming the two gates act independently, this has probabilityThis is the general form used to describe the open probability for a transient conductance. We could raise the h factor in this expression to an arbitrary power, as we did for m, but we omit this complication to streamline the discussion. The activation m and inactivation h, like all gating variables, vary between 0 and 1. They are described by equations identical to 5.16, except that the rate functions  n and  n are replaced by either  m and  m , or  h and  h . These rate functions were fitted by Hodgkin and Huxley using the equationsand h  describing the steady-state activation and inactivation levels, and voltage-dependent time constants for m and h can be defined as in equations 5.19 and 5.18. These are plotted in figure 5.10. For comparison, n  and  n for the K  conductance are also plotted. Note that h , because it corresponds to an inactivation variable, is flipped relative to m  and n , so that it approaches 1 at hyperpolarized voltages and 0 at depolarized voltages.The presence of two factors in equation gives a transient conductance some interesting properties. To turn on a transient conductance maximally, it may first be necessary to hyperpolarize the neuron below its resting potential and then to depolarize it. Hyperpolarization raises the value of the inactivation h, a process called deinactivation. The second deinactivation step, depolarization, increases the value of m, which is activation. Only when m and h are both nonzero is the conductance turned on. Note that the conductance can be reduced in magnitude by either decreasing m or h. Decreasing h is called inactivation to distinguish it from decreasing m, inactivation which is deactivation.Persistent currents act as if they are controlled by an activation gate, while transient currents act as if they have both an activation and an inactivation , and n , the steady-state levels of activation and inactivation of the Na  conductance, and activation of the K  conductance. The right panel shows the voltage-dependent time constants that control the rates at which these steady-state levels are approached for the three gating variables.gate. Another class of conductances, the hyperpolarization-activated conductances, behave as if they are controlled solely by an inactivation gate. They are thus persistent conductances, but they open when the neuron is hyperpolarized rather than depolarized. The opening probability for such channels is written solely in terms of an inactivation variable similar to h. Strictly speaking, these conductances deinactivate when they turn on and inactivate when they turn off. However, most people cannot bring themselves to say "deinactivate" all the time, so they say instead that these conductances are activated by hyperpolarization.The Hodgkin-Huxley model for the generation of the action potential, in its single-compartment form, is constructed by writing the membrane current in equation 5.6 as the sum of a leakage current, a delayed-rectified K  current, and a transient Na  current,The maximal conductances and reversal potentials used in the model are g L  0.003 mSmm 2 , g K  0.36 mSmm 2 , g Na  1.2 mSmm 2 , E L  -54.387 mV, E K  -77 mV and E Na  50 mV. The full model consists of equation 5.6 with equation 5.25 for the membrane current, and equations of the form 5.17 for the gating variables n, m, and h. These equations can be integrated numerically, using the methods described in appendices A and B.The temporal evolution of the dynamic variables of the Hodgkin-Huxley model during a single action potential is shown in membrane potential up to about -50 mV, the m variable that describes activation of the Na  conductance suddenly jumps from nearly 0 to a value near 1. Initially, the h variable, expressing the degree of inactivation of the Na  conductance, is around 0.6. Thus, for a brief period both m and h are significantly different from 0. This causes a large influx of Na  ions, producing the sharp downward spike of inward current shown in the second trace from the top. The inward current pulse causes the membrane potential to rise rapidly to around 50 mV. The rapid increase in both V and m is due to a positive feedback effect. Depolarization of the membrane potential causes m to increase, and the resulting activation of the Na  conductance makes V increase. The rise in the membrane potential causes the Na  conductance to inactivate by driving h toward 0. This shuts off the Na  current. In addition, the rise in V activates the K  conductance by driving n toward 1. This increases the K  current, which drives the membrane potential back down to negative values. The final recovery involves the readjustment of m, h, and n to their initial values.The Hodgkin-Huxley model can also be used to study propagation of an action potential down an axon, but for this purpose a multi-compartment model must be constructed. Methods for building such a model, and results from it, are described in chapter 6.In previous sections, we described the Hodgkin-Huxley formalism for describing voltage-dependent conductances arising from a large number of channels. With the advent of single-channel studies, microscopic descriptions of the transitions between the conformational states of channel molecules have been developed. Because these models describe complex molecules, they typically involve many states and transitions. Here, we discuss simple versions of these models that capture the spirit of singlechannel modeling without getting mired in the details. The lower panels in figure 5.12 show simulations of this model involving 1, 10, and 100 channels. The sum of currents from all of these channels is compared with the current predicted by the Hodgkin-Huxley model 3 n 2 n  n 4 n 3 n 2 n  n . For each channel, the pattern of opening and closing is random, but when enough channels are summed, the total current matches that of the Hodgkin-Huxley model quite well.To see how the channel model in A solution for these equations can be constructed if we recall that, in the 5ms were used in the simulations shown in the lower panels. For these simulations, the membrane potential was initially held at -100 mV, then held at 10 mV for 20 ms, and finally returned to a holding potential of -100 mV. The smooth curves in these panels show the current predicted by the Hodgkin-Huxley model in this situation. The left panel shows a simulation of a single channel that opened once during the depolarization. The middle panel shows the total current from 10 simulated channels, and the right panel corresponds to 100 channels. As the number of channels increases, the Hodgkin-Huxley model provides a fairly accurate description of the current, but it is not identical to the channel model in this case.Hodgkin-Huxley model, n is the probability of a subunit gate being in the open state and 1  n is the probability of it being closed. If we use that same notation here, state 1 has four closed subunit gates, and thus In the Hodgkin-Huxley model of the Na  conductance, the activation and inactivation processes are assumed to act independently. The schematic in figure 5.8B, which cartoons the mechanism believed to be responsible for inactivation, suggests that this assumption is incorrect. The ball that inactivates the channel is located inside the cell membrane, where it cannot be affected directly by the potential across the membrane. Furthermore, in this scheme the ball cannot occupy the channel pore until the activation gate has opened, making the two processes interdependent.The state diagram in figure 5.13 reflects this by having a state-dependent, voltage-independent inactivation mechanism. This diagram is a simplistate-dependent inactivation fied version of an Na  channel model due to Synaptic transmission at a spike-mediated chemical synapse begins when an action potential invades the presynaptic terminal and activates voltagedependent Ca 2 channels, leading to a rise in the concentration of Ca 2 within the terminal. This causes vesicles containing transmitter molecules to fuse with the cell membrane and release their contents into the synaptic cleft between the pre-and postsynaptic sides of the synapse. The transmitter molecules then diffuse across the cleft and bind to receptors on the postsynaptic neuron. Binding of transmitter molecules leads to the opening of ion channels that modify the conductance of the postsynaptic neuron, completing the transmission of the signal from one neuron to the other. Postsynaptic ion channels can be activated directly by binding to the transmitter, or indirectly when the transmitter binds to a distinct receptor that affects ion channels through an intracellular second-messenger signaling pathway.As with a voltage-dependent conductance, a synaptic conductance can be written as the product of a maximal conductance and an open channel probability, g s  g s P. The open probability for a synaptic conductance can be expressed as a product of two terms that reflect processes occurring on the pre-and postsynaptic sides of the synapse, P  P s P rel . The factor P s is synaptic open probability P s the probability that a postsynaptic channel opens, given that the transmitter was released by the presynaptic terminal. Because there are typically many postsynaptic channels, this can also be taken as the fraction of channels opened by the transmitter.P rel is related to the probability that transmitter is released by the presynaptic terminal following the arrival of an action potential. This reflects transmitter release probability P rel the fact that transmitter release is a stochastic process. Release of transmitter at a presynaptic terminal does not necessarily occur every time an action potential arrives and, conversely, spontaneous release can occur even in the absence of the depolarization due to an action potential. The interpretation of P rel is a bit subtle because a synaptic connection between neurons may involve multiple anatomical synapses, and each of these may have multiple independent transmitter release sites. The factor P rel , in our discussion, is the average of the release probabilities at each release site. If there are many release sites, the total amount of transmitter released by all the sites is proportional to P rel . If there is a single release site, P rel is the probability that it releases transmitter. We will restrict our discussion to these two interpretations of P rel . For a modest number of release sites with widely varying release probabilities, the current we discuss describes only an average over multiple trials.Synapses can exert their effects on the soma, dendrites, axon spikeinitiation zone, or presynaptic terminals of their postsynaptic targets. There are two broad classes of synaptic conductances that are distinguished by whether the transmitter binds to the synaptic channel and activates it directly, or the transmitter binds to a distinct receptor that acti-ionotropic receptor vates the conductance indirectly through an intracellular signaling pathway. The first class is called ionotropic and the second, metabotropic. metabotropic receptor Ionotropic conductances activate and deactivate more rapidly than metabotropic conductances. Metabotropic receptors can, in addition to opening channels, cause long-lasting changes inside a neuron. They typically operate through pathways that involve G-protein-mediated receptors and various intracellular signaling molecules known as second messengers. Many neuromodulators, including serotonin, dopamine, norepinephrine, and acetylcholine, act through metabotropic receptors. These have a wide variety of important effects on the functioning of the nervous system.Glutamate and GABA are the major excitatory and glutamate, GABA inhibitory transmitters in the brain. Both act ionotropically and metabotropically. The principal ionotropic receptor types for glutamate are called AMPA and NMDA. Both AMPA and NMDA receptors produce mixed-AMPA, NMDA cation conductances with reversal potentials around 0 mV. The AMPA current activates and deactivates rapidly. The NMDA receptor is somewhat slower to activate and deactivates considerably more slowly. In addition, NMDA receptors have an unusual voltage dependence that we discuss in a later section, and are more permeable to Ca 2 than AMPA receptors.GABA activates two important inhibitory synaptic conductances in the GABA A , GABA B brain. GABA A receptors produce a relatively fast ionotropic Cl  conductance. GABA B receptors are metabotropic, and act to produce a slower and longer-lasting K  conductance.In addition to chemical synapses, neurons can be coupled through electrical synapses that produce a synaptic current proportional gap junctions to the difference between the pre-and postsynaptic membrane potentials. Some gap junctions rectify so that positive and negative current flows are not equal for potential differences of the same magnitude.In a simple model of a directly activated receptor channel, the transmitter interacts with the channel through a binding reaction in which k transmitter molecules bind to a closed receptor and open it. In the reverse reaction, the transmitter molecules unbind from the receptor and it closes. These processes are analogous to the opening and closing involved in the gating of a voltage-dependent channel, and the same type of equation is used to describe how the open probability P s changes with time,Here,  s determines the closing rate of the channel and is usually assumed to be a constant. The opening rate,  s , on the other hand, depends on the concentration of transmitter available for binding to the receptor. If the concentration of transmitter at the site of the synaptic channel is transmitter, the probability of finding k transmitter molecules within binding range of the channel is proportional to transmitter k , and  s is some constant of proportionality times this factor.When an action potential invades the presynaptic terminal, the transmitter concentration rises and  s grows rapidly, causing P s to increase. Following the release of transmitter, diffusion out of the cleft, enzyme-mediated degradation, and presynaptic uptake mechanisms can all contribute to a rapid reduction of the transmitter concentration. This sets  s to 0, and P s follows suit by decaying exponentially with a time constant 1 s . Typically, the time constant for channel closing is considerably larger than the opening time.As a simple model of transmitter release, we assume that the transmitter concentration in the synaptic cleft rises extremely rapidly after vesicle release, remains at a high value for a period of duration T, and then falls rapidly to 0. Thus, the transmitter concentration is modeled as a square pulse. While the transmitter concentration is nonzero,  s takes a constant value much greater than  s , otherwise  s  0. Suppose vesicle release occurs at time t  0 and that the synaptic channel open probability takes the value P s at this time. While the transmitter concentration in the cleft 10 ms 60 pA The open probability takes its maximum value at time t  T and then, for t  T, decays exponentially at a rate determined by the constant  s ,If P s  0, as it will if there is no synaptic release immediately before the release at t  0, equation 5.28 simplifies to P s  1  exp for 0  t  T, and this reaches a maximum value P max  P s  1  exp.In terms of this parameter, a simple manipulation of equation 5.28 shows that we can write, in the general case,Figure 5.14 shows a fit to a recorded postsynaptic current using this formalism. In this case,  s was set to 0.19 ms 1 . The transmitter concentration was modeled as a square pulse of duration T  1 ms during which  s  0.93 ms 1 . Inverting these values, we find that the time constant determining the rapid rise seen in For a fast synapse like the one shown in immediately after each presynaptic action potential.Equations 5.28 and 5.29 can also be used to model synapses with slower rise times, but other functional forms are often used. One way of describing both the rise and the fall of a synaptic conductance is to express P s as the difference of two exponentials. For an isolated presynaptic action potential occurring at t  0, the synaptic conductance is written as The rise time of the synapse is determined by  rise   1  2 , while the fall time is set by  1 . This conductance reaches its peak value  rise ln after the presynaptic action potential.Another way of describing a synaptic conductance is to use the expression We mentioned earlier in this chapter that NMDA receptor conductance has an additional dependence on the postsynaptic potential not normally NMDA receptor seen in other conductances. To incorporate this dependence, the current due to the NMDA receptor can be described using an additional factor that depends on the postsynaptic potential, V. The NMDA current is written as g NMDA G NMDAP. P is the usual open probability factor. The factor G NMDA describes an extra voltage dependence due to the fact that when the postsynaptic neuron is near its resting potential, NMDA receptors are blocked by Mg 2 ions. To activate the conductance, the postsynaptic neuron must be depolarized to knock out the blocking ions. The probability of transmitter release and the magnitude of the resulting conductance change in the postsynaptic neuron can depend on the history of activity at a synapse. The effects of activity on synaptic conductances are termed short-and long-term. Short-term plasticity refers to a numshort-term plasticity ber of phenomena that affect the probability that a presynaptic action potential opens postsynaptic channels and that last anywhere from milliseconds to tens of seconds. The effects of long-term plasticity are extremely long-term plasticity persistent, lasting, for example, as long as the preparation being studied can be kept alive. The modeling and implications of long-term plasticity are considered in chapter 8. Here we present a simple way of describing short-term synaptic plasticity as a modification in the release probability for synaptic transmission. Short-term modifications of synaptic transmission can involve other mechanisms than merely changes in the probability of transmission, but for simplicity we absorb all these effects into a modification of the factor P rel introduced previously. Thus, P rel can be interpreted more generally as a presynaptic factor affecting synaptic transmission. Facilitation and depression can both be modeled as presynaptic processes that modify the probability of transmitter release. We describe them using a simple nonmechanistic model that has similarities to the model of P s presented in the previous subsection. For both facilitation and depression, the release probability after a long period of presynaptic silence is P rel  P 0 . Activity at the synapse causes P rel to increase in the case of facilitation and to decrease for depression. Between presynaptic action potentials, the release probability decays exponentially back to its "resting" value, P 0 ,The parameter  P controls the rate at which the release probability decays to P 0 .The models of facilitation and depression differ in how the release probability is changed by presynaptic activity. In the case of facilitation, P rel is augmented by making the replacement P rel  P rel  f F immediately after a presynaptic action potential  exp, which is obtained by integrating equation 5.37. The average value of the exponential decay factor in this expression is the integral over all positive  values of exp times the probability density for a Poisson spike train with a firing rate r to produce an interspike interval of duration , which is r exp. Thus, the average exponential decrement isIn order for the release probability to return, on average, to its steady-state value between presynaptic spikes, we must therefore require that Solving for P rel givesThis equals P 0 at low rates and rises toward the value 1 at high rates. As a result, isolated spikes in low-frequency trains are transmitted with lower probability than spikes occurring within highfrequency bursts. The synaptic transmission rate when the presynaptic neuron is firing at rate r is the firing rate times the release probability. This is approximately P 0 r for small rates and approaches r at high rates.The value of P rel for a Poisson presynaptic spike train can also be computed in the case of depression. The only difference from the above derivation is that following a presynaptic spike, P rel is decreased to f D P rel . Thus, the consistency condition 5.39 is replaced bygivingThis equals P 0 at low rates and decreases as 1r at high rates, which has some interesting consequences. As noted above, the average rate of successful synaptic transmissions is equal to P rel times the presynaptic rate r. Because P rel is proportional to 1r at high rates, the average transmission rate is independent of r in this range. This can be seen by the flattening of the solid curve in figure 5.18B. As a result, synapses that depress do not convey information about the values of constant, high presynaptic firing rates to their postsynaptic targets. The presynaptic firing rate at which transmission starts to become independent of r is aroundFigure 5.19 shows the average transmission rate, P rel r, in response to a series of steps in the presynaptic firing rate. Note first that the steadystate transmission rates during the 25, 100, 10, and 40 Hz periods are quite similar. This is a consequence of the 1r dependence of the average release probability, as discussed above. The largest transmission rates in the figure occur during the sharp upward transitions between different presynaptic rates. This illustrates the important point that depressing synapses amplify transient signals relative to steady-state inputs. The transients corresponding the 25 to 100 Hz transition and the 10 to 40 Hz transition are of roughly equal amplitudes, but the transient for the 10 to 40 Hz transition is broader than that for the 25 to 100 Hz transition.The equality of amplitudes of the two upward transients in . Synaptic inputs can be incorporated into an integrate-and-fire model by including synaptic conductances in the membrane current appearing in equation 5.8,For simplicity, we assume that P rel  1 in this example. The synaptic current is multiplied by r m in equation 5.43 because equation 5.8 was multiplied by this factor. To model synaptic transmission, P s changes whenever the presynaptic neuron fires an action potential using one of the schemes described previously.Figures 5.20A and 5.20B show examples of two integrate-and-fire neurons driven by electrode currents and connected by identical excitatory or inhibitory synapses. The synaptic conductances in this example are described by the  function model. This means that the synaptic conductance a time t after the occurrence of a presynaptic action potential is given by equation 5.35. The figure shows a nonintuitive effect. When the synaptic time constant is sufficiently long, excitatory connections produce a state in which the two neurons fire synchronous and asynchronous firing alternately, out of phase with one another, while inhibitory synapses produce synchronous firing. It is normally assumed that excitation produces synchrony. Actually, in some cases inhibitory connections can be more effective than excitatory connections at synchronizing neuronal firing.Synapses have multiple effects on their postsynaptic targets. In equation 5.43, the term r m g s P s E s acts as a source of current to the neuron, while the term r m g s P s V changes the membrane conductance. The effects of the latter term are referred to as shunting, and they can be identified most easily if we divide equation 5.43 by 1  r m g s P s to obtainThe shunting effects of the synapse are seen in this equation as a decrease in the effective membrane time constant, and a divisive reduction in the impact of the leakage and synaptic reversal potentials and of the electrode current.The shunting effects seen in equation 5.44 have been proposed as a possible basis for neural computations involving division. However, shunting has a divisive effect only on the membrane potential of an integrate-andfire neuron its effect on the firing rate is subtractive. To see this, assume that synaptic input is arriving at a sufficient rate to maintain a relatively constant value of P s . In this case, shunting amounts to changing the value of the membrane resistance from R m to R m . Recalling equation 5.12 for the firing rate of the integrate-and-fire model, and the fact that  m  C m R m , we can write the firing rate in a form that reveals its dependence on R m ,Changing R m modifies only the constant term in this equation it has no effect on the dependence of the firing rate on I e .Integrate-and-fire models are useful for studying how neurons sum large numbers of synaptic inputs and how networks of neurons interact. One issue that has received considerable attention is the degree of variability in the firing output of integrate-and-fire neurons receiving synaptic input. This work has led to the realization that neurons can respond to multiple synaptic inputs in two different modes of operation depending on the balance that exists between excitatory and inhibitory contributions.The two modes of operation are illustrated in The irregularity of a spike train can be quantified using the coefficient of variation, the ratio of the standard deviation to the mean of the interspike intervals. For the Poisson inputs being used in this example, C V  1, while for the spike train in the lower panel of figure 5.21A, C V  0.3. Thus, the output spike train is much more regular than the input trains. This is not surprising, because the model neuron effectively averages its many synaptic inputs. In the regular firing mode, the total synaptic input attempts to charge the neuron above the threshold, but every time the potential reaches the threshold, it gets reset and starts charging again. In this mode of operation, the timing of the action potentials is determined primarily by the charging rate of the cell, which is controlled by its membrane time constant. The high degree of variability seen in the spiking patterns of in vivo recordings of cortical neurons suggests that they are better approximated by an integrate-and-fire model operating in an irregularfiring mode. There are advantages to operating in the irregular-firing mode that may compensate for its increased variability. One is that neurons firing in the irregular mode reflect in their outputs the temporal properties of fluctuations in their total synaptic input. In the regular firing mode, the timing of output spikes is only weakly related to the temporal character of the input spike trains. In addition, neurons operating in the irregular firing mode can respond more quickly to changes in presynaptic spiking patterns and firing rates than those operating in the regular firing mode.In this chapter, we considered the basic electrical properties of neurons, including their intracellular and membrane resistances, capacitances, and active voltage-dependent and synaptic conductances. We introduced the Nernst equation for equilibrium potentials and the formalism of Hodgkin and Huxley for describing persistent, transient, and hyperpolarizationactivated conductances. Methods were introduced for modeling stochastic channel opening and stochastic synaptic transmission, including the effects of synaptic facilitation and depression. We discussed a number of ways of describing synaptic conductances following the release of a neurotransmitter. Two models of action potential generation were discussed, the simple integrate-and-fire scheme and the more realistic Hodgkin-Huxley model.We begin by considering the numerical integration of equation 5.8. It is convenient to rewrite this equation in the form If we take the time interval t to be small enough so that the gating variables can be approximated as constant during this period, the membrane potential can again be integrated over one time step, using equation 5.48. Of course, the gating variables are not fixed, so once V has been updated by this rule, the gating variables must be updated as well.All the gating variables in a conductance-based model satisfy equations of the same form, An efficient integration scheme for conductance-based models is to alternate using rule to update the membrane potential and rule to update all the gating variables. It is important to alternate the updating of V with that of the gating variables, rather than doing them all simultaneously, as this keeps the method accurate to second order in t.If Ca 2 -dependent conductances are included, the intracellular Ca 2 concentration should be computed simultaneously with the membrane potential. By alternating the updating, we mean that the membrane potential is computed at times 0, t, 2 t, . . ., while the gating variables are computed at times t2, 3 t2, 5 t2, . . .. A discussion of the second-order accuracy of this scheme is given in The dynamic aspects of synaptic transmission are reviewed in Magleby and Zucker. Our presentation followed 6 Model Neurons II: Conductances andIn modeling neurons, we must deal with two types of complexity: the intricate interplay of active conductances that makes neuronal dynamics so rich and interesting, and the elaborate morphology that allows neurons to receive and integrate inputs from so many other neurons. The first part of this chapter extends the material presented in chapter 5 by examining single-compartment models with a wider variety of voltagedependent conductances, and hence a wider range of dynamic behaviors, than the Hodgkin-Huxley model. In the second part of the chapter, we introduce methods used to study the effects of morphology on the electrical characteristics of neurons. An analytic approach known as cable theory is presented first, followed by a discussion of multi-compartment models that permit numerical simulation of complex neuronal structures.Model neurons range from greatly simplified caricatures to highly detailed descriptions involving thousands of differential equations. Choosing the most appropriate level of modeling for a given research problem requires a careful assessment of the experimental information available and a clear understanding of the research goals. Oversimplified models can, of course, give misleading results, but excessively detailed models can obscure interesting results beneath inessential and unconstrained complexity.The electrical properties of neurons arise from membrane conductances with a wide variety of properties. The basic formalism developed by Hodgkin and Huxley to describe the Na  and K  conductances responsible for generating action potentials is also used to represent most of the additional conductances encountered in neuron modeling. Models that treat these aspects of ionic conductances, known as conductance-based models, can reproduce the rich and complex dynamics of real neurons quite accurately. In this chapter, we discuss both singleand multi-compartment conductance-based models, beginning with the single-compartment case.To review from chapter 5, the membrane potential of a single-compartment neuron model, V, is determined by integrating the equationwith I e the electrode current, A the membrane surface area of the cell, and i m the membrane current. In the following subsections, we present expressions for the membrane current in terms of the reversal potentials, maximal conductance parameters, and gating variables of the different conductances of the models being considered. The gating variables and V comprise the dynamic variables of the model. All the gating variables are determined by equations of the form gating equationswhere z denotes a generic gating variable. The functions  z and z  are determined from experimental data. For some conductances, these are written in terms of the opening and closing rates  z and  z, asWe have written  z and z  as functions of the membrane potential, but for Ca 2 -dependent currents they also depend on the internal Ca 2 concentration. We call  z,  z,  z, and z  gating functions. A method for numerically integrating equations 6.1 and 6.2 is described in the appendices of chapter 5.In the following subsections, some basic features of conductance-based models are presented in a sequence of examples of increasing complexity. We do this to illustrate the effects of various conductances and combinations of conductances on neuronal activity. Different cells can have quite different response properties due to their particular combinations of conductances. Research on conductance-based models focuses on understanding how neuronal response dynamics arises from the properties of membrane and synaptic conductances, and how the characteristics of different neurons interact when they are coupled in networks.The Hodgkin-Huxley model of action-potential generation, discussed in chapter 5, was developed on the basis of data from the giant axon of the squid, and we present a multi-compartment simulation of action-potential propagation using this model in a later section. The Connor-Stevens model where g L  0.003 mSmm 2 and E L  -17 mV are the maximal conductance and reversal potential for the leak conductance and g Na  1.2 mSmm 2 , g K  0.2 mSmm 2 , g A  0.477 mSmm 2 , E Na  55 mV, E K  -72 mV, and E A  -75 mV. The gating variables, m, h, n, a, and b, are determined by equations of the form 6.2 with the gating functions given in appendix A.The fast Na  and delayed-rectifier K  conductances generate action potentials in the Connor-Stevens model just as they do in the HodgkinHuxley model. What is the role of the additional A-current The range of responses exhibited by the Connor-Stevens model neuron can be extended by including a transient Ca 2 conductance. The conductance transient Ca 2 conductance we use was modeled by with, for the example given here, g CaT  0.013 mSmm 2 and E Ca  120 mV. The gating variables for the transient Ca 2 conductance are determined from the gating functions in appendix A.Several different Ca 2 conductances are commonly expressed in neuronal membranes. These are categorized as L, T, N, and P types. L-type Ca 2 L, T, N and P type Ca 2 channels currents are persistent as far as their voltage dependence is concerned, and they activate at a relatively high threshold. They inactivate due to a Ca 2 -dependent rather than voltage-dependent process. T-type Ca 2 currents have lower activation thresholds and are transient. N-and Ptype Ca 2 conductances have intermediate thresholds and are transient and persistent, respectively. They may be responsible for the Ca 2 entry that causes the release of transmitter at presynaptic terminals. Entry of Ca 2 into a neuron has many secondary consequences ranging from gating Ca 2 -dependent channels to inducing long-term modifications of synaptic conductances.A transient Ca 2 conductance acts, in many ways, like a slower version of the transient Na  conductance that generates action potentials. Instead of producing an action potential, a transient Ca 2 conductance generates a slower transient depolarization sometimes called a Ca 2 spike. This tranCa 2 spike sient depolarization causes the neuron to fire a burst of action potentials, which are Na  spikes riding on the slower Ca 2 spike. The transient Ca 2 current is an important component of models of thalamic relay neurons. These neurons exhibit different firing patterns in thalamic relay neuron sleep and wakeful states. Action potentials tend to appear in bursts during sleep. Neurons can fire action potentials either at a steady rate or in bursts even in the absence of current injection or synaptic input. Periodic bursting is a common feature of neurons in central pattern generators, which are neural circuits that produce periodic patterns of activity to drive rhythmic motor behaviors such as walking, running, or chewing. To illustrate periodic bursting, we consider a model constructed to match the activity of neurons in the crustacean stomatogastric ganglion, a neuronal cirstomatogastric ganglion cuit that controls chewing and digestive rhythms in the foregut of lobsters and crabs. The STG is a model system for investigating the effects of neuromodulators, such as amines and neuropeptides, on the activity patterns neuromodulator of a neural network. Neuromodulators modify neuronal and network behavior by activating, deactivating, or otherwise altering the properties of membrane and synaptic channels. Neuromodulation has a major impact on virtually all neural networks, ranging from peripheral motor pattern generators like the STG to the sensory, motor, and cognitive circuits of the brain.The model STG neuron contains fast Na  , delayed-rectifier K  , A-type K  , and transient Ca 2 conductances similar to those discussed above, although the formulas and parameters used are somewhat different. In addition, the model has a Ca 2 -dependent K  conductance. Due to the complexity of the model, we do not provide complete descriptions of its conductances except for the Ca 2 -dependent K  conductance which plays a particularly significant role in the model.The repolarization of the membrane potential after an action potential is often carried out both by the delayed-rectifier K  conductance and by a fast Ca 2 -dependent K  conductance. Ca 2 -dependent K  conductances Ca 2 -dependent K  conductance may be voltage dependent, but they are activated primarily by a rise in the level of intracellular Ca 2 . A slow Ca 2 -dependent K  conductance called the after-hyperpolarization conductance builds up during afterhyperpolarization conductance sequences of action potentials and typically contributes to the spike-rate adaptation discussed and modeled in chapter 5.The Ca 2 -dependent K  current in the model STG neuron is given by Here i Ca is the total Ca 2 current per unit area of membrane,  Ca is the time constant determining the rate at which intracellular Ca 2 is removed, and  is a factor that converts from the electric current due to Ca 2 ion flow  As in the models of figures 6.2 and 6.3, the bursts are transient Ca 2 spikes with action potentials riding on top of them. The Ca 2 current during these bursts causes a dramatic increase in the intracellular Ca 2 concentration. This activates the Ca 2 -dependent K  current, which, along with the inactivation of the Ca 2 current, terminates the burst. The interburst interval is determined primarily by the time it takes for the intracellular Ca 2 concentration to return to a low value, which deactivates the Ca 2 -dependent K  current, allowing another burst to be generated. Although Single-compartment models describe the membrane potential over an entire neuron with a single variable. Membrane potentials can vary considerably over the surface of the cell membrane, especially for neurons with long and narrow processes, or if we consider rapidly changing membrane potentials. The attenuation and delay within a neuron are most severe when electrical signals travel down the long, narrow, cablelike structures of dendritic or axonal branches. For this reason, the mathematical analysis of signal propagation within neurons is called cable theory. Dendritic and axonal cables cable theory are typically narrow enough that variations of the potential in the radial or axial directions are negligible compared to longitudinal variations. Therefore, the membrane potential along a neuronal cable is expressed as a function of a single longitudinal spatial coordinate x and time, V, and the basic problem is to solve for this potential.Current flows within a neuron due to voltage gradients. In chapter 5, we discussed how the potential difference across a segment of neuronal cable is related to the longitudinal current flowing down the cable. The longitudinal resistance of a cable segment of length x and radius a is given by multiplying the intracellular resistivity r L by x and dividing by the cross-sectional area, a 2 , so that R L  r L x.  , is then related to the amount of longitudinal current flow by Ohms law. In chapter 5, we discussed the magnitude of this current flow, but for the present purposes, we also need to define a sign convention for its direction. We define currents flowing in the direction of increasing x as positive. By this convention, the relationship between V and I L given by Ohm. Solving this for the longitudinal current, we findIt is useful to take the limit of this expression for infinitesimally short cable segments, that is, as x  0. In this limit, the ratio of V to x becomes the derivative Vx. We use a partial derivative here because V can also depend on time. Thus, at any point along a cable of radius a and intracellular resistivity r L , the longitudinal current flowing in the direction of increasing x isThe membrane potential V is determined by solving a partial differential equation, the cable equation, that describes how the currents entering, leaving, and flowing within a neuron affect the rate of change of the membrane potential. To derive the cable equation, we consider the currents within the small segment shown in figure 6.6. This segment has a radius a and a short length x. The rate of change of the membrane potential due to currents flowing into and out of this region is determined by its capacitance. Recall from chapter 5 that the capacitance of a membrane is determined by multiplying the specific membrane capacitance c m by the area of the membrane. The cylinder of membrane shown in figure 6.6 has a surface area of 2a x, and hence a capacitance of 2a xc m . The amount of current needed to change the membrane potential at a rate Vt is thus 2a xc m Vt. The cable equation is derived by setting the sum of all the currents shown in  Dividing both sides of this equation by 2a x, we note that the right side involves the termThe arrow refers to the limit x  0, which we now take. We can move r L outside the derivative in this equation under the assumption that it is not a function of position. However, the factor of a 2 must remain inside the derivative unless it is independent of x. Substituting the result 6.10 into 6.9, we obtain the cable equation, cable equationTo determine the membrane potential, equation must be augmented by appropriate boundary conditions. The boundary conditions specboundary conditions for the cable equation ify what happens to the membrane potential when the neuronal cable branches or terminates. The point at which a cable branches, or equivalently where multiple cable segments join, is called a node. At such a branching node, the potential must be continuous, that is, the functions V defined along each of the segments must yield the same result when evaluated at the x value corresponding to the node. In addition, charge must be conserved, which means that the sum of the longitudinal currents entering a node along all of its branches must be 0. According to equation 6.8, the longitudinal current entering a node is proportional to the square of the cable radius times the derivative of the potential evaluated at that point, a 2 Vx. The sum of the longitudinal currents entering the node, computed by evaluating these derivatives along each cable segment at the point where they meet at the node, must be 0.Several different boundary conditions can be imposed at the end of a terminating cable segment. One simple condition is that no current flows out of the end of the cable. By equation 6.8, this means that the spatial derivative of the potential must vanish at a termination point.Due to the complexities of neuronal membrane currents and morphologies, the cable equation is most often solved numerically, using multicompartmental techniques described later in this chapter. However, it is useful to study analytic solutions of the cable equation in simple cases to get a feel for how different morphological features, such as long dendritic cables, branching nodes, changes in cable radii, and cable ends, affect the membrane potential.Before we can solve the cable equation by any method, the membrane current i m must be specified. We discussed models of various ion channel contributions to the membrane current in chapter 5 and earlier in this chapter. These models typically produce nonlinear expressions that are too complex to allow analytic solution of the cable equation. The analytic solutions we discuss use two rather drastic approximations: synaptic currents are ignored, and the membrane current is written as a linear function of the membrane potential. Eliminating synaptic currents requires us to examine how a neuron responds to the electrode current i e . In some cases, electrode current can mimic the effects of a synaptic conductance, although the two are not equivalent. In any case, studying responses to electrode current allows us to investigate the effects of different morphologies on membrane potentials.Typically, a linear approximation for the membrane current is valid only if the membrane potential stays within a limited range, for example, close to the resting potential of the cell. The resting potential is defined as the potential where no net current flows across the membrane. Near this potential, we approximate the membrane current per unit area as If the radii of the cable segments used to model a neuron are constant except at branches and abrupt junctions, the factor a 2 in equation 6.11 can be taken out of the derivative and combined with the prefactor 12ar L to produce a factor a2r L that multiplies the spatial second derivative. With this modification and use of the linear expression for the membrane current, the cable equation for v isIt is convenient to multiply this equation by r m , turning the factor that multiplies the time derivative on the left side into the membrane time constant  m  r m c m . This also changes the expression multiplying the spatial second derivative on the right side of equation 6.13 to ar m 2r L . This factor has the dimensions of length squared, and it defines a fundamental length constant for a segment of cable of radius a, the electrotonic length, electrotonic length    ar m 2r L .Using the values r m  1 M mm 2 and r L  1 k mm, a cable of radius a  2 m has an electrotonic length of 1 mm. A segment of cable with radius a and length  has a membrane resistance that is equal to its longitudinal resistance, as can be seen from equation 6.14, The membrane potential is affected both by the form of the cable equation and by the boundary conditions imposed at branching nodes and terminations. To isolate these two effects, we consider two idealized cases: an infinite cable that does not branch or terminate, and a single branching node that joins three semi-infinite cables. Of course, real neuronal cables are not infinitely long, but the solutions we find are applicable for long cables far from their ends. We determine the potential for both of these morphologies when current is injected at a single point. Because the equation we are studying is linear, the membrane potential for any other spatial distribution of electrode current can be determined by summing solutions corresponding to current injection at different points. The use of point injection to build more general solutions is a standard method of linear analysis. In this context, the solution for a point source of current injection is called a Greens function.In general, solutions to the linear cable equation are functions of both position and time. However, if the current being injected is held constant, the membrane potential settles to a steady-state solution that is independent of time. Solving for this time-independent solution is easier than solving the full time-dependent equation, because the cable equation reduces to an ordinary differential equation in the static case, In the small region of size x around x  0 where the current is injected, the full equation  2 d 2 vdx 2  v  r m i e must be solved. If the total amount of current injected by the electrode is I e , the current per unit area injected into this region is I e . This grows without bound as x  0. The first derivative of the membrane potential v  B exp is discontinuous at the point x  0. For small x, the derivative at one side of the region we are discussing is approximately B, while at the other side it is B. In these expressions, we have used the fact that x is small to set exp  1. For small x, the second derivative is approximately the difference between these two first derivatives divided by x, which is 2B. We can ignore the term v in the cable equation within this small region, because it is not proportional to 1 x. Substituting the expressions we have derived for the remaining terms in the equation, we find that 2 2 B  r m I e , which means that B  I e R  2, using R  from equation 6.15. Thus, the membrane potential for static current injection at the point x  0 along an infinite cable is We now consider the membrane potential produced by an instantaneous pulse of current injected at the point x  0 at the time t  0. Specifically, we consider i e  I e  m 2a, which means that the current pulse delivers a total charge of I e  m . We do not derive the solution for this case For large x, t max  x m 2, corresponding to a velocity of 2 m . For smaller x values, the location of the maximum moves faster than this "velocity" would imply To illustrate the effects of branching on the membrane potential in response to a point source of current injection, we consider a single isolated junction of three semi-infinite cables, as shown in the bottom panels of figure 6.9. For simplicity, we discuss the solution for static current injection at a point, but the results generalize directly to the case of time-dependent currents. We label the potentials along the three segments v 1 , v 2 , and v 3 , and label the distance outward from the junction point along any given segment by the coordinate x where, for i  1, 2, and 3,Note that the distances x and y appearing in the exponential functions are divided by the electrotonic length of the segment along which the potential is measured or the current is injected. This solution satisfies the cable equation, because it is constructed by combining solutions of the form 6.18. The only term that has a discontinuous first derivative within the range being considered is the first term in the expression for v 2 , and this solves the cable equation at the current injection site because it is identical to 6.18. We leave it to the reader to verify that this solution satisfies the boundary conditions v 1  v 2  v 3 and a 2 i v i x  0. The infinite and semi-infinite cables we have considered are clearly mathematical idealizations. We now turn to a model neuron introduced by effect of these inputs is usually measured from the soma, and the spikeinitiation region of the axon that determines whether the neuron fires an action potential is typically located near the soma. In Ralls model, a compact soma region is connected to a single equivalent cylindrical cable that replaces the entire dendritic region of the neuron. The critical feature of the model is the choice of the radius and length for the equivalent cable to best match the properties of the dendritic structure being approximated.The radius a and length L of the equivalent cable are determined by matching two important elements of the full dendritic tree. These are its average length in electrotonic units, which determines the amount of attenuation, and the total surface area, which determines the total membrane resistance and capacitance. The average electrotonic length of a dendrite is determined by considering direct paths from the soma to the terminals of the dendrite. The electrotonic lengths for these paths are constructed by measuring the distance traveled along each of the cable segments traversed in units of the electrotonic length constant for that segment. In general, the total electrotonic length measured by summing these electrotonic segment lengths depends on which terminal of the tree is used as the end point. However, an average value can be used to define an electrotonic length for the full dendritic structure. The length L of the equivalent cable is then chosen so that L is equal to this average electrotonic length, where  is the length constant for the equivalent cable. The radius of the equivalent cable, which is needed to compute , is determined by setting the surface area of the equivalent cable, 2aL, equal to the surface area of the full dendritic tree.Under some restrictive circumstances the equivalent cable reproduces the effects of a full tree exactly. Among these conditions is the requirement a 32 1  a 32 2  a 32 3 on the radii of any three segments being joined at a node within the tree. Note from equation 6.22 that this condition makes p 1  p 2  p 3  12. However, even when the so-called 32 law is not exact, the equivalent cable is an extremely useful and often reasonably accurate simplification.  Figures 6.10 and 6.12 depict static solutions of the Rall model for two different recording configurations, expressed in the form of equivalent circuits. The equivalent circuits are an intuitive way of describing the solution of the cable equation. In Expressions for v soma and v, arising directly from the equivalent circuit using standard rules of circuit analysis, are given at the right side of figure 6.10.The input resistance of the Rall model neuron, as measured from the soma, is determined by the somatic resistance R soma acting in parallel with the effective resistance of the cable, and isR soma . The effective resistance of the cable,The effect of lengthening a cable saturates when it gets much longer than its electrotonic length. The voltage attenuation caused by the cable is defined as the ratio of the dendritic potential to the somatic potential, and in this case it is given by This result is plotted in andThe input resistance for this configuration, as measured from the dendrite, is determined by R 3 and R 4 acting in parallel, and is. When L and x are both much larger than , this approaches the limiting value R  . The current attenuation is defined as the ratio of the somatic current to the electrode current, and is given byThe inward current attenuation The membrane potential for a neuron of complex morphology is obviously much more difficult to compute than the simple cases we have v v anatomy attenuation delay 100 m 10 ms  . An additive quantity can be obtained by taking the logarithm of the attenuation, due to the identity ln The morphoelectrotonic transform is a diagram of a neuron in which the dismorphoelectrotonic transform tance between any two points is determined by the logarithm of the ratio of the membrane potentials at these two locations, not by the actual size of the neuron.Another morphoelectrotonic transform can be used to indicate the amount of delay in the voltage waveform produced by a transient input current.The morphoelectrotonic transform uses a definition of delay different from that used in Morphoelectrotonic transforms of a pyramidal cell from layer 5 of cat visual cortex are shown in figures 6.13 and 6.14. The left panel of figure 6.13 is a normal drawing of the neuron being studied, the middle panel shows the steady-state attenuation, and the right panel shows the delay. The transformed diagrams correspond to current being injected peripherally, with somatic potentials being compared to dendritic potentials. These The small neuron diagram at the upper left of figure 6.14 shows attenuation for the reverse situation from figure 6.13, when constant current is injected into the soma and dendritic potentials are compared with the somatic potential. Note how much smaller this diagram is than the one in the central panel of The capacitance of neuronal cables causes the voltage attenuation for timedependent current injection to increase as a function of frequency. The cable equation can be solved analytically only in relatively simple cases. When the complexities of real membrane conductances are included, the membrane potential must be computed numerically. This is done by splitting the modeled neuron into separate regions or compartments, and approximating the continuous membrane potential V by a discrete set of values representing the potentials within the different compartments. This assumes that each compartment is small enough so that there is negligible variation of the membrane potential across it.The precision of such a multi-compartmental description depends on the 100 Hz 500 Hz 0 Hz  In a multi-compartment model, each compartment has its own membrane potential V , and its own gating variables that determine the membrane current for compartment , i  m . Each membrane potential V  satisfies an equation similar to 6.1 except that the compartments couple to their neighbors in the multi-compartment structure. For a nonbranching cable, each compartment is coupled to two neighbors and the equations for the membrane potentials of the compartments areHere I  e is the total electrode current flowing into compartment , and A  is its surface area. Compartments at the ends of a cable have only one neighbor, and thus only a single term replacing the last two terms in equation 6.29. For a compartment where a cable branches in two, there are The constant g ,  that determines the resistive coupling from neighboring compartment   to compartment  is determined by computing the current that flows from one compartment to its neighbor due to Ohms law. For simplicity, we begin by computing the coupling between two compartments that have the same length L and radius a. Using the results of chapter 5, the resistance between two such compartments, measured from their centers, is the intracellular resistivity, r L times the distance between the compartment centers divided by the cross-sectional area, r L L. The total current flowing from compartment   1 to compartment  is then a 2r L L. Equation 6.29 for the potential within a compartment  refers to currents per unit area of membrane. Thus, we must divide the total current from compartment   by the surface area of compartment , 2aL, and we find thatThe value of g ,  is given by a more complex expression if the two neighboring compartments have different lengths or radii. This can occur when a tapering cable is approximated by a sequence of cylindrical compartments, or at a branch point where a single compartment connects with two other compartments, as in figure 6.16. In either case, suppose that compartment  has length L  and radius a  , and compartment   has length L   and radius a   . The resistance between these two compartments is the sum of the two resistances from the middle of each compartment to the junction between them,To compute g ,  we invert this expression and divide the result by the total surface area ofg 2,11 2g 3,5Figure 6.16 A multi-compartment model of a neuron. The expanded region shows three compartments at a branch point where a single cable splits into two. Each compartment has membrane and synaptic conductances, as indicated by the equivalent electrical circuit, and the compartments are coupled together by resistors. Although a single resistor symbol is drawn, note that g ,  is not necessarily equal to g   , . compartment , 2a  L  , which gives Equations 6.29 for all of the compartments of a model determine the membrane potential throughout the neuron with a spatial resolution given by the compartment size. An efficient method for integrating the coupled multi-compartment equations is discussed in appendix B. Using this scheme, models can be integrated numerically with excellent efficiency, even those involving large numbers of compartments. Such integration schemes are built into neuron simulation software packages such as Neuron and Genesis.As an example of multi-compartment modeling, we simulate the propagation of an action potential along an unmyelinated axon. In this model, each compartment has the same membrane conductances as the singlecompartment Hodgkin-Huxley model discussed in chapter 5. The different compartments are joined together in a single nonbranching cable representing a length of axon. Although action potentials typically move along axons in a direction outward from the soma, the basic process of action-potential propagation does not favor one direction over the other. Propagation in the reverse direction, called antidromic propagation, is orthodromic and antidromic propagation possible under certain stimulation conditions. For example, if an axon is stimulated in the middle of its length, action potentials will propagate in both directions away from the point of stimulation. Once an action potential starts moving along an axon, it does not generate a second action potential moving in the opposite direction because of refractory effects. The region in front of a moving action potential is ready to generate a spike as soon as enough current moves longitudinally down the axon from the region currently spiking to charge the next region up to spiking threshold. However, Na  conductances in the region just behind the moving action potential are still partially inactivated, so this region cannot generate another spike until after a recovery period. By the time the trailing region has recovered, the action potential has moved too far away to generate a second spike.Refractoriness following spiking has a number of other consequences for action-potential propagation. Two action potentials moving in opposite directions that collide annihilate one another because they cannot pass through each others trailing refractory regions. Refractoriness also keeps action potentials from reflecting off the ends of axon cables, which avoids the impedance matching needed to prevent reflection from the ends of ordinary electrical cables. 12 . This is proportional to the square root of the axon radius. The square-root dependence of the propagation speed on the axon radius means that thick axons are required to achieve high action-potential propagation speeds, and the squid giant axon is an extreme example. Action-potential propagation can also be sped up by covering the axon with an insulating myelin wrapping, as we discuss next.Many axons in vertebrates are covered with an insulating sheath of myelin except at gaps, called the nodes of Ranvier, where there is a high density of fast voltage-dependent Na  channels. The myelin sheath consists of many layers of glial cell membrane wrapped around the axon. This gives the myelinated region of the axon a very high membrane resistance and a small membrane capacitance. This results in what is called saltatory propagation, in which membrane potential depolarizasaltatory propagation tion is transferred passively down the myelin-covered sections of the axon, and action potentials are actively regenerated at the nodes of Ranvier. We can compute the capacitance of a myelin-covered axon by treating the myelin sheath as an extremely thick cell membrane. Consider the geometry shown in the cross-sectional diagram of figure 6.18B. The myelin sheath extends from the radius a 1 of the axon core to the outer radius a 2 . For calculational purposes, we can think of the myelin sheath as being made of a series of thin, concentric cylindrical shells. The capacitances of these shells combine in series to make up the full capacitance of the myelinated axon. If a single layer of cell membrane has thickness d m and capacitance per unit area c m , the capacitance of a cylinder of membrane of radius a, thickness a, and length L is c m 2d m La a. According to the rule for capacitors in series, the inverse of the total capacitance is obtained by adding the inverses of the individual capacitances. The capacitance of a myelinated cylinder of length L and the dimensions in This is equivalent to the diffusion equation, vt  D 2 vx 2 , with dif-It is interesting to compute the inner core radius, a 1 , that maximizes this diffusion constant for a fixed outer radius a 2 . Setting the derivative of D with respect to a 1 to 0 gives the optimal inner radius a 1  a 2 exp or a 1  0.6a 2 . An inner core fraction of 0.6 is typical for myelinated axons. This indicates that for a given outer radius, the thickness of myelin maximizes the diffusion constant along the myelinated axon segment.At the optimal ratio of radii, D  a 2 2 , which is proportional to the square of the axon radius. Because of the form of the diffusion equation it obeys with this value of D, v can be written as a function of xa 2 and t. This scaling implies that the propagation velocity for a myelinated cable is proportional to a 2 , that is, to the axon radius, not its square root. Increasing the axon radius by a factor of 4, for example, increases the propagation speed of an unmyelinated cable only by a factor of 2, while it increases the speed for a myelinated cable fourfold.We continued the discussion of neuron modeling that began in chapter 5 by considering models with more complete sets of conductances and techniques for incorporating neuronal morphology. We introduced A-type K  , transient Ca 2 , and Ca 2 -dependent K  conductances, and noted their effect on neuronal activity. The cable equation and its linearized version were introduced to examine the effects of morphology on membrane potentials. Finally, multi-compartment models were presented and used to discuss propagation of action potentials along unmyelinated and myelinated axons.The rate functions used for the gating variables n, m, and h of the Connor-The A-current is described directly in terms of the asymptotic values and  functions for its gating variables, Multi-compartment models are defined by a coupled set of differential equations, one for each compartment. There are also gating variables for each compartment, but these involve only the membrane potential within that compartment, and integrating their equations can be handled as in the single-compartment case using the approach discussed in appendix B of chapter 5. Integrating the membrane potentials for the different compartments is more complex because they are coupled to each other.Equation whereNote that the gating variables and other parameters have been absorbed into the values of the coefficients B  , C  , D  , and F  in this equation. Equation 6.44, with  running over all of the compartments of the model, generates a set of coupled differential equations. Because of the coupling between compartments, we cannot use the method discussed in appendix A of chapter 5 to integrate these equations. Instead, we present another method that shares some of the positive features of that approach. The Runge-Kutta method, which is a standard numerical integrator, is poorly suited for this application and is likely to run orders of magnitude slower than the method described below.Two of the most important features of an integration method are accuracy and stability. Accuracy refers to how closely numerical finite-difference methods reproduce the exact solution of a differential equation as a function of the integration step size t. Stability refers to what happens when t is chosen to be excessively large and the method starts to become inaccurate. A stable integration method will degrade smoothly as t is increased, producing results of steadily decreasing accuracy. An unstable method, on the other hand, will at some point display a sudden transition and generate wildly inaccurate results. Given the tendency of impatient modelers to push the limits on t, it is highly desirable to have a method that is stable.  where.49 for all  values provides a set of coupled linear equations for the quantities V  . An efficient method exists for solving these equations Substituting this into the equation 6.49 for   2 gives. We now repeat the procedure going down the cable. At each stage, we solve for V 1 in terms of V  , findingwhereFinally, when we get to the end of the cable, we can solve forThe procedure for computing all the V  is the following. Define c  1  c 1 and f  1  f 1 and iterate equations 6.54 and 6.55 down the length of the cable to define all the c  and f  parameters. Then solve for V N from equation 6.56 and iterate back up the cable, solving for the Vs using 6.53. This process takes only 2N steps.We leave the extension of this method to the case of a branched cable as an exercise for the reader. The general procedure is similar to the one we presented for a nonbranching cable. The equations are solved by starting at the ends of the branches and moving in toward their branching node, then continuing on as for a nonbranching cable, and finally reversing direction and completing the solution moving in the opposite direction along the cable and its branches.Many of the references for chapter 5 apply to this chapter as well, includ- Two freely available software packages for detailed neuronal modeling are in wide use, Neuron Extensive synaptic connectivity is a hallmark of neural circuitry. For example, a typical neuron in the mammalian neocortex receives thousands of synaptic inputs. Network models allow us to explore the computational potential of such connectivity, using both analysis and simulations. As illustrations, we study in this chapter how networks can perform the following tasks: coordinate transformations needed in visually guided reaching, selective amplification leading to models of simple and complex cells in primary visual cortex, integration as a model of short-term memory, noise reduction, input selection, gain modulation, and associative memory. Networks that undergo oscillations are also analyzed, with application to the olfactory bulb. Finally, we discuss network models based on stochastic rather than deterministic dynamics, using the Boltzmann machine as an example.Neocortical circuits are a major focus of our discussion. In the neocortex, which forms the convoluted outer surface of the human brain, neurons lie in six vertical layers highly coupled within cylindrical columns. Such columns have been suggested as basic functional units, cortical columns and stereotypical patterns of connections both within a column and between columns are repeated across cortex. There are three main classes of interconnections within cortex, and in other areas of the brain as well. Feedforward connections bring input to a given region from another refeedforward, recurrent, and top-down connections gion located at an earlier stage along a particular processing pathway. Recurrent synapses interconnect neurons within a particular region that are considered to be at the same stage along the processing pathway. These may include connections within a cortical column as well as connections between both nearby and distant cortical columns within a region. Topdown connections carry signals back from areas located at later stages. These definitions depend on how the region being studied is specified and on the hierarchical assignment of regions along a pathway. In general, neurons within a given region send top-down projections back to the areas from which they receive feedforward input, and receive top-down input from the areas to which they project feedforward output. The numbers, though not necessarily the strengths, of feedforward and top-down fibers between connected regions are typically comparable, and recurrent synapses typically outnumber feedforward or top-down inputs. We begin this chapter by studying networks with purely feedforward input and then study the effects of recurrent connections. The analysis of top-down connections, for which it is more difficult to establish clear computational roles, is left until chapter 10.The most direct way to simulate neural networks is to use the methods discussed in chapters 5 and 6 to synaptically connect model spiking neurons. This is a worthwhile and instructive enterprise, but it presents significant computational, calculational, and interpretational challenges. In this chapter, we follow a simpler approach and construct networks of neuron-like units with outputs consisting of firing rates rather than action potentials. Spiking models involve dynamics over time scales ranging from channel openings that can take less than a millisecond, to collective network processes that may be several orders of magnitude slower. Firing-rate models avoid the short time scale dynamics required to simulate action potentials and thus are much easier to simulate on computers. Firing-rate models also allow us to present analytic calculations of some aspects of network dynamics that could not be treated in the case of spiking neurons. Finally, spiking models tend to have more free parameters than firing-rate models, and setting these appropriately can be difficult.There are two additional arguments in favor of firing-rate models. The first concerns the apparent stochasticity of spiking. The models discussed in chapters 5 and 6 produce spike sequences deterministically in response to injected current or synaptic input. Deterministic models can predict spike sequences accurately only if all their inputs are known. This is unlikely to be the case for the neurons in a complex network, and network models typically include only a subset of the many different inputs to individual neurons. Therefore, the greater apparent precision of spiking models may not actually be realized in practice. If necessary, firing-rate models can be used to generate stochastic spike sequences from a deterministically computed rate, using the methods discussed in chapters 1 and 2.The second argument involves a complication with spiking models that arises when they are used to construct simplified networks. Although cortical neurons receive many inputs, the probability of finding a synaptic connection between a randomly chosen pair of neurons is actually quite low. Capturing this feature, while retaining a high degree of connectivity through polysynaptic pathways, requires including a large number of neurons in a network model. A standard way of dealing with this problem is to use a single model unit to represent the average response of several neurons that have similar selectivities. These "averaging" units can then be interconnected more densely than the individual neurons of the actual network, so fewer of them are needed to build the model. If neural responses are characterized by firing rates, the output of the model unit is simply the average of the firing rates of the neurons it represents collectively. However, if the response is a spike, it is not clear how the spikes of the represented neurons can be averaged. The way spiking models are typically constructed, an action potential fired by the model unit duplicates the effect of all the neurons it represents firing synchronously. Not surprisingly, such models tend to exhibit large-scale synchronization unlike anything seen in a healthy brain.Firing-rate models also have their limitations. They cannot account for aspects of spike timing and spike correlations that may be important for understanding nervous system function. Firing-rate models are restricted to cases where the firing of neurons in a network is uncorrelated, with little synchronous firing, and where precise patterns of spike timing are unimportant. In such cases, comparisons of spiking network models with models that use firing-rate descriptions have shown that they produce similar results. Nevertheless, the exploration of neural networks undoubtedly requires the use of both firing-rate and spiking models.As discussed in chapter 1, the sequence of spikes generated by a neuron is completely characterized by the neural response function , which consists of  function spikes located at times when the neuron fired action potentials. In firing-rate models, the exact description of a spike sequence provided by the neural response function  is replaced by the approximate description provided by the firing rate r. Recall from chapter 1 that r is defined as the probability density of firing and is obtained from  by averaging over trials. The validity of a firing-rate model depends on how well the trial-averaged firing rate of network units approximates the effect of actual spike sequences on the networks dynamic behavior.The replacement of the neural response function by the corresponding firing rate is typically justified by the fact that each network neuron has a large number of inputs. Replacing , which describes an actual spike train, with the trial-averaged firing rate r is justified if the quantities of relevance for network dynamics are relatively insensitive to the trial-totrial fluctuations in the spike sequences represented by . In a network model, the relevant quantities that must be modeled accurately are the total inputs for the neurons within the network. For any single synaptic input, the trial-to-trial variability is likely to be large. However, if we sum the input over many synapses activated by uncorrelated presynaptic spike trains, the mean of the total input typically grows linearly with the number of synapses, while its standard deviation grows only as the square root of the number of synapses. Thus, for uncorrelated presynaptic spike trains, using presynaptic firing rates in place of the actual presynaptic spike trains may not significantly modify the dynamics of the network. Conversely, a firing-rate model will fail to describe a network adequately if the presynaptic inputs to a substantial fraction of its neurons are correlated. This can occur, for example, if the presynaptic neurons fire synchronously.The synaptic input arising from a presynaptic spike train is effectively fil-tered by the dynamics of the conductance changes that each presynaptic action potential evokes in the postsynaptic neuron and the dynamics of propagation of the current from the synapse to the soma. The temporal averaging provided by slow synaptic or membrane dynamics can reduce the effects of spike-train variability and help justify the approximation of using firing rates instead of presynaptic spike trains. Firing-rate models are more accurate if the network being modeled has a significant amount of synaptic transmission that is slow relative to typical presynaptic interspike intervals.The construction of a firing-rate model proceeds in two steps. First, we determine how the total synaptic input to a neuron depends on the firing rates of its presynaptic afferents. This is where we use firing rates to approximate neural response functions. Second, we model how the firing rate of the postsynaptic neuron depends on its total synaptic input. Firingrate response curves are typically measured by injecting current into the soma of a neuron. We therefore find it most convenient to define the total synaptic input as the total current delivered to the soma as a result of all the synaptic conductance changes resulting from presynaptic action potentials. We denote this total synaptic current by I s . We then determine synaptic current I s the postsynaptic firing rate from I s . In general, I s depends on the spatially inhomogeneous membrane potential of the neuron, but we assume that, other than during action potentials or transient hyperpolarizations, the membrane potential remains close to, but slightly below, the threshold for action potential generation. An example of this type of behavior is seen in the upper panels of figure 7.2. I s is then approximately equal to the synaptic current that would be measured from the soma in a voltageclamp experiment, except for a reversal of sign. In the next section, we model how I s depends on presynaptic firing rates.In the network models we consider, both the output from, and the input to, a neuron are characterized by firing rates. To avoid a proliferation of sub-and superscripts on the quantity r, we use the letter u to denote a presynaptic firing rate, and v to denote a postsynaptic rate. Note that v is input rate u output rate v used here to denote a firing rate, not a membrane potential. In addition, we use these two letters to distinguish input and output firing rates in network models, a convention we retain through the remaining chapters. When we consider multiple input or output neurons, we use vectors u and v to represent their firing rates collectively, with the components of these input rate vector u output rate vector v vectors representing the firing rates of the individual input and output units.Consider a neuron receiving N u synaptic inputs labeled by b  1, 2, . . . , N u. The firing rate of input b is denoted by u b , and the input rates are represented collectively by the N u -component vector u. We model how the synaptic current I s depends on presynaptic firing rates by first consid-output v input u weights w ering how it depends on presynaptic spikes. If an action potential arrives at input b at time 0, we write the synaptic current generated in the soma of the postsynaptic neuron at time t as w b K s, where w b is the synaptic weight and K s is called the synaptic kernel. Collectively, the synaptic weights are represented by a synaptic weight vector w, which has N u com-synaptic weights w ponents w b . The amplitude and sign of the synaptic current generated by input b are determined by w b . For excitatory synapses, w b  0, and for inhibitory synapses, w b  0. In this formulation of the effect of presynaptic spikes, the probability of transmitter release from a presynaptic terminal is absorbed into the synaptic weight factor w b , and we do not include shortterm plasticity in the model.The synaptic kernel, K s  0, describes the time course of the synaptic synaptic kernel K s current in response to a presynaptic spike arriving at time t  0. This time course depends on the dynamics of the synaptic conductance activated by the presynaptic spike, and also on both the passive and the active properties of the dendritic cables that carry the synaptic current to the soma. For example, long passive cables broaden the synaptic kernel and slow its rise from 0. Cable calculations or multi-compartment simulations, such as those discussed in chapter 6, can be used to compute K s for a specific dendritic structure. To avoid ambiguity, we normalize K s by requiring its integral over all positive times to be 1. At this point, for simplicity, we use the same function K s to describe all synapses.Assuming that the effects of the spikes at a single synapse sum linearly, the total synaptic current at time t arising from a sequence of presynaptic spikes occurring at input b at times t i is given byIn the second expression, we have used the neural response function,  b  i , to describe the sequence of spikes fired by presynaptic neuron b. The equality follows from integrating over the sum of  functions in the definition of  b. If there is no nonlinear interaction between different synaptic currents, the total synaptic current coming from all presynaptic inputs is obtained simply by summing,As discussed previously, the critical step in the construction of a firingrate model is the replacement of the neural response function  b in equation 7.2 with the firing rate of neuron b, u b, so that we writeThe synaptic kernel most frequently used in firing-rate models is an exponential, K s  exp s . With this kernel, we can describe I s by a differential equation if we take the derivative of equation 7.3 with respect to t,In the second equality, we have expressed the sum w b u b as the dot product of the weight and input vectors, w  u. In this and the following chapdot product ters, we primarily use the vector versions of equations such as 7.4, but when we first introduce an important new equation, we often write it in its subscripted form as well.Recall that K describes the temporal evolution of the synaptic current due to both synaptic conductance and dendritic cable effects. For an electrotonically compact dendritic structure,  s will be close to the time constant that describes the decay of the synaptic conductance. For fast synaptic conductances such as those due to AMPA glutamate receptors, this may be as short as a few milliseconds. For a long, passive dendritic cable,  s may be larger than this, but its measured value is typically quite small.Equation  even when the total synaptic current varies with time. This leads to a firing-rate model in which the dynamics arises exclusively from equation 7.4, firing-rate model with current dynamicsAn alternative formulation of a firing-rate model can be constructed by assuming that the firing rate does not follow changes in the total synaptic current instantaneously, as was assumed for the model of equation 7.6. Action potentials are generated by the synaptic current through its effect on the membrane potential of the neuron. Due to the membrane capacitance and resistance, the membrane potential is, roughly speaking, a low-pass filtered version of I s. For this reason, the time-dependent firing rate is often modeled as a low-pass filtered version of the steady-state firing rate,. The low-pass filtering effect of equation 7.7 is described in the Mathematical Appendix in the context of electrical circuit theory. The argument we have used to motivate equation 7.7 would suggest that  r should be approximately equal to the membrane time constant of the neuron. However, this argument really applies to the membrane potential, not the firing rate, and the dynamics of the two are not the same. Some network models use a value of  r that is considerably less than the membrane time constant. We re-examine this issue in the following section.The second model that we have described involves the pair of equations 7.4 and 7.7. If one of these equations relaxes to its equilibrium point much more rapidly than the other, the pair can be reduced to a single equation. We discuss cases in which this occurs in the following section. The firing-rate models described by equations 7.6 and 7.8 differ in their assumptions about how firing rates respond to and track changes in the input current to a neuron. In one case, it is assumed that firing rates follow time-varying input currents instantaneously, without attenuation or delay. In the other case, the firing rate is a low-pass filtered version of the input current. To study the relationship between input current and firing rate, it is useful to examine the firing rate of a spiking model neuron in response to a time-varying injected current, I. The model used for this purpose in The right panels in figure 7.2 show that the situation is different if the input current is below the threshold for firing through a significant part  The essential message from figure 7.2 is that neither equation 7.6 nor equation 7.8 provides a completely accurate prediction of the dynamics of the firing rate at all frequencies and for all levels of injected current. A more complex model can be constructed that accurately describes the firing rate over the entire range of input current amplitudes and frequencies. The solid curves in figure 7.2 were generated by a model that expresses the firing rate as a function of both I from equation 7.6 and v from equation 7.8. In other words, it is a combination of the two models discussed in the pre- vious section with the firing rate given neither by F nor by v but by another function, G. This compound model provides quite an accurate description of the firing rate of the integrate-and-fire model, but it is more complex than the models used in this chapter. We use the notation W  u to denote the vector with components b W ab u b . The use of the dot to represent a sum of a product of two quantities over a shared index is borrowed from the notation for the dot product of two vectors. The expression F represents the vector with componentsThe recurrent network of figure 7.3B also has two layers of neurons with rates u and v, but in this case the neurons of the output layer are interconnected with synaptic weights described by a matrix M. Matrix element M aa  describes the strength of the synapse from output unit a  to output unit a. The output rates in this case are determined by recurrent modelIt is often convenient to define the total feedforward input to each neuron in the network of figure 7.3B as h  W  u. Then, the output rates are determined by the equationNeurons are typically classified as either excitatory or inhibitory, meaning that they have either excitatory or inhibitory effects on all of their postsynaptic targets. This property is formalized in Dales law, which states that Dales law a neuron cannot excite some of its postsynaptic targets and inhibit others. In terms of the elements of M, this means that for each presynaptic neuron a  , M aa  must have the same sign for all postsynaptic neurons a. To impose this restriction, it is convenient to describe excitatory and inhibitory neurons separately. The firing-rate vectors v E and v I for the excitatory and inhibitory neurons are then described by a coupled set of equations identical in form to equation 7.11, excitatoryinhibitoryThere are now four synaptic weight matrices describing the four possible types of neuronal interactions. The elements of M EE and M IE are greater than or equal to 0, and those of M EI and M II are less than or equal to 0. These equations allow the excitatory and inhibitory neurons to have different time constants, activation functions, and feedforward inputs.In this chapter, we consider several recurrent network models described by equation 7.11 with a symmetric weight matrix, M aa   M a  a for all a and a  . Requiring M to be symmetric simplifies the mathematical analysis, symmetric coupling but it violates Dales law. Suppose, for example, that neuron a, which is excitatory, and neuron a  , which is inhibitory, are mutually connected. Then, M aa  should be negative and M a  a positive, so they cannot be equal. Equation 7.11 with symmetric M can be interpreted as a special case of equations 7.12 and 7.13 in which the inhibitory dynamics are instantaneous and the inhibitory rates are given by v I  M IE v E . This produces an effective recurrent weight matrix M  M EE  M EI  M IE , which can be made symmetric by the appropriate choice of the dimension and form of the matrices M EI and M IE . The dynamic behavior of equation 7.11 is restricted by requiring the matrix M to be symmetric. For example symmetric coupling typically does not allow for network oscillations. In the latter part of this chapter, we consider the richer dynamics of models described by equations 7.12 and 7.13.It is often convenient to identify each neuron in a network by using a parameter that describes some aspect of its selectivity rather than the integer label a or b. For example, neurons in primary visual cortex can be characterized by their preferred orientation angles, preferred spatial phases and frequencies, or other stimulus-related parameters. In many of the examples in this chapter, we consider stimuli characterized by a single angle , which represents, for example, the orientation of a visual stimulus. Individual neurons are identified by their preferred stimulus angles, which are typically the values of for which they fire at maximum rates. Thus, neuron a is identified by an angle  a . The weight of the synapse from neuron b or neuron a  to neuron a is then expressed as a function of the preferred stimulus angles  b ,  a  and  a of the pre-and postsynaptic neurons,. We often consider cases in which these synaptic weight functions depend only on the difference between the pre-and postsynaptic angles, so thatIn large networks, the preferred stimulus parameters for different neurons will typically take a wide range of values. In the models we consider, the number of neurons is large and the angles  a , for different values of a, cover the range from 0 to 2 densely. For simplicity, we assume that this coverage is uniform, so that the density of coverage, the number of neurons with preferred angles falling within a unit range, which we denote by   , is constant. For mathematical convenience in these cases, we aldensity of coverage   low the preferred angles to take continuous values rather than restricting them to the actual discrete values  a for a  1, 2, . . . , N. Thus, we label the neurons by a continuous angle  and express the firing rate as a function of , so that u and v describe the firing rates of neurons with preferred angles . Similarly, the synaptic weight matrices W and M are replaced by functions W and M that characterizes the strength of synapses from a presynaptic neuron with preferred angle   to a postsynaptic neuron with preferred angle  in the feedforward and recurrent cases, respectively.If the number of neurons in a network is large and the density of coverage of preferred stimulus values is high, we can approximate the sums in equation 7.10 by integrals over   . The number of postsynaptic neurons with preferred angles within a range   is     , so, when we take the limit    0, the integral over   is multiplied by the density factor   . Thus, in the case of continuous labeling of neurons, equation 7.10 becomes continuous model As we did in equation 7.11, we can write the first term inside the integral of this expression as an input function h. We make frequent use of continuous labeling for network models, and we often approximate sums over neurons by integrals over their preferred stimulus parameters.  Substantial computations can be performed by feedforward networks in the absence of recurrent connections. Much of the work done on feedforward networks centers on plasticity and learning, as discussed in the following chapters. Here, we present an example of the computational power of feedforward circuits, the calculation of the coordinate transformations needed in visually guided reaching tasks.Reaching for a viewed object requires a number of coordinate transformations that turn information about where the image of the object falls on the retina into movement commands in shoulder-, arm-, or hand-based coordinates. To perform a transformation from retinal to body-based coordinates, information about the retinal location of an image and about the direction of gaze relative to the body must be combined. Visual neurons have receptive fields fixed to specific locations on the retina. Neurons in motor areas can display visually evoked responses that  To account for these data, we need to construct a model neuron that is driven by visual input, but that nonetheless has a tuning curve for image location that is not a function of s, the retinal location of the image, but of s  g, the location of the object in body-based coordinates. A possible basis for this construction is provided by a combined representation of s and g by neurons in area 7a in the posterior parietal cortex of the monkey. Recordings made in area 7a reveal neurons that fire at rates that depend on both the location of the stimulating image on the retina and the direction of gaze. The response tuning curves, expressed as functions of the retinal location of the stimulus, do not shift when the direction of gaze is varied. Instead, shifts of gaze direction affect the magnitude of the visual response. Thus, responses in area 7a exhibit gaze-dependent gain modulation of a retinotopic visual receptive field.  To model a neuron with a body-centered response tuning curve, we construct a feedforward network with a single output unit representing, for example, the premotor cortex neuron shown in figure 7.5. The input layer of the network consists of a population of area 7a neurons with gainmodulated responses similar to those shown in figure 7.6B. Neurons with gains that both increase and decrease as a function of g are included in the model. The average firing rates of the input layer neurons are described by tuning curves u  f u, with the different neurons taking different  and  values.We use continuous labeling of neurons, and replace the sum over presynaptic neurons by an integral over their  and  values, inserting the appropriate density factors   and   , which we assume are constant. The steady-state response of the single output neuron is determined by the continuous analog of equation 7.5. The synaptic weight from a presynaptic neuron with preferred stimulus location  and preferred gaze direction  is denoted by w, so the steady-state response of the output neuron is given byFor the output neuron to respond to the stimulus location in body-based coordinates, its firing rate must be a function of s  g. To see if this is possible, we shift the integration variables in 7.15 by     g and     g. Ignoring effects from the end points of the integration, we findThis is a function of s g provided that w  w, which holds if w is a function of the sum   . Thus, the coordinate transformation can be accomplished if the synaptic weight from a given neuron depends only on the sum of its preferred retinal and gaze angles. It has been suggested that weights of this form can arise naturally from random hand and gaze movements through correlation-based synaptic modification of the type discussed in chapter 8. Gain-modulated neurons provide a general basis for combining two different input signals in a nonlinear way. In the network we studied, it is possible to find appropriate synaptic weights w to generate output neuron responses with a wide range of different dependencies on s and g. The mechanism by which sensory and modulatory inputs combine in a multiplicative way in gain-modulated neurons is not known. Later in this chapter, we discuss a recurrent network model for generating gainmodulated responses.Recurrent networks have richer dynamics than feedforward networks, but they are more difficult to analyze. To get a feel for recurrent circuitry, we begin by analyzing a linear model, that is, a model for which the relationship between firing rate and synaptic current is linear,The linear approximation is a drastic one that allows, among other things, the components of v to become negative, which is impossible for real firing rates. Furthermore, some of the features we discuss in connection with linear, as opposed to nonlinear, recurrent networks can also be achieved by a feedforward architecture. Nevertheless, the linear model is extremely useful for exploring properties of recurrent circuits, and this approach will be used both here and in the following chapters. In addition, the analysis of linear networks forms the basis for studying the stability properties of nonlinear networks. We augment the discussion of linear networks with results from simulations of nonlinear networks.Under the linear approximation, the recurrent model of equation c e  , It is easier to solve equation 7.17 for the coefficients c  than for v directly. Substituting the expansion 7.19 into equation 7.17 and using property 7.18, we find thatThe sum over  can be eliminated by taking the dot product of each side of this equation with one of the eigenvectors, e  , and using the orthogonality property e   e     to obtainThe critical feature of this equation is that it involves only one of the coefficients, c  . For time-independent inputs h, the solution of equation 7.21 is where c  is the value of c  at time 0, which is given in terms of the initial firing-rate vector v by c   e   v..22 has several important characteristics. If    1, the exponential functions grow without bound as time increases, reflecting a fundamental instability of the network. If    1, c  approaches the steadystate value e   h exponentially with time constant  r . This steady-state value is proportional to e   h, which is the projection of the input vector onto the relevant eigenvector. For 0     1, the steadystate value is amplified relative to this projection by the factor 1, which is greater than 1. The approach to equilibrium is slowed relative to the basic time constant  r by an identical factor. The steady-state value of v, which we call v  , can be derived from equation 7.19 asThis steady-state response can also arise from a purely feedforward scheme if the feedforward weight matrix is chosen appropriately, as we invite the reader to verify as an exercise.Suppose that one of the eigenvalues of a recurrent weight matrix, denoted by  1 , is very close to 1, and all the others are significantly smaller than 1. In this case, the denominator of the   1 term on the right side of equation 7.23 is near 0, and, unless e 1  h is extremely small, this single term will dominate the sum. As a result, we can writeSuch a network performs selective amplification. The response is dominated by the projection of the input vector along the axis defined by e 1 , and the amplitude of the response is amplified by the factor 1, which may be quite large if  1 is near 1. The steady-state response of such a network, which is proportional to e 1 , therefore encodes an amplified projection of the input vector onto e 1 .Further information can be encoded if more eigenvalues are close to 1. Suppose, for example, that two eigenvectors, e 1 and e 2 , have the same eigenvalue,  1   2 , close to but less than 1. Then, equation 7.24 is replaced by v  e 1 e 2 1   1 , If the recurrent weight matrix has an eigenvalue exactly equal to 1,  1  1, and all the other eigenvalues satisfy    1, a linear recurrent network can act as an integrator of its input. In this case, c 1 satisfies the equation If h is constant, c 1 grows linearly with t. This explains why equation 7.24 diverges as  1  1. Suppose, instead, that h is nonzero for a while, and then is set to 0 for an extended period of time. When h  0, equation 7.22 shows that c   0 for all   1, because for these eigenvectors    1. Assuming that c 1  0, this means that after such a period, the firing-rate vector is given, from equations 7.27 and 7.19, by network integrationThis shows that the network activity provides a measure of the running integral of the projection of the input vector onto e 1 . One consequence of this is that the activity of the network does not cease if h  0, provided that the integral up to that point in time is nonzero. The network thus exhibits sustained activity in the absence of input, which provides a memory of the integral of prior input.Networks in the brain stem of vertebrates responsible for maintaining eye position appear to act as integrators, and networks similar to the one we have been discussing have been suggested as models of this system. As outlined in The ability of a linear recurrent network to integrate and display persistent activity relies on one of the eigenvalues of the recurrent weight matrix being exactly 1. Any deviation from this value will cause the persistent activity to change over time. Eye position does indeed drift, but matching the performance of the ocular positioning system requires fine-tuning of the eigenvalue to a value extremely close to 1. Including nonlinear interactions does not alleviate the need for a precisely tuned weight matrix. Synaptic modification rules can be used to establish the necessary synaptic weights, but it is not clear how such precise tuning is accomplished in the biological system.For a linear recurrent network with continuous labeling, the equation for the firing rate v of a neuron with preferred stimulus angle  is a linear version of equation 7.14,where h is the feedforward input to a neuron with preferred stimulus angle , and we have assumed a constant density   . Because  is an angle, h, M, and v must all be periodic functions with period 2. By making M a function of     , we are imposing a symmetry with respect to translations or shifts of the angle variables. In addition, we assume that M is an even function,. This is the analog, in a continuously labeled model, of a symmetric synaptic weight matrix.Equation We leave it as an exercise to show that the eigenfunctions are 1 12 , corresponding to   0, and cos 12 and sin 12 for   1, 2, . . .. The eigenvalues are identical for the sine and cosine eigenfunctions and are given byThe steady-state firing rates for a constant input are given by the continuous analog of equation 7.23,The integrals in this expression are the coefficients in a Fourier series for Fourier series the function h and are known as cosine and sine Fourier integrals.  which has all eigenvalues except  1 equal to 0. The network model shown in figure 7.8 has  1  0.9, so that 1  10. Input amplification can  A linear model does not provide an adequate description of the firing rates of a biological neural network. The most significant problem is that the firing rates in a linear network can take negative values. This problem can be fixed by introducing rectification into equation 7.11 by choosing rectification where    is a vector of threshold values that we often take to be 0 0 0.In this section, we show some examples illustrating the effect of including vector of zeros 0 0 0 such a rectifying nonlinearity. Some of the features of linear recurrent networks remain when rectification is included, but several new features also appear.In the examples given below, we consider a continuous model, similar to that of equation 7.29, with recurrent couplings given by equation 7.33 but now including a rectification nonlinearity, so thatIf  1 is not too large, this network converges to a steady state for any constant input, and therefore we often limit the discussion to the steady-state activity of the network.Figure 7.9 shows the nonlinear analog of the selective amplification shown for a linear network in figure 7.8. Once again, a noisy input generates a much smoother output response profile. The output response of the rectified network corresponds roughly to the positive part of the sinusoidal response profile of the linear network. The negative output has been eliminated by the rectification. Because fewer neurons in the network have nonzero responses than in the linear case, the value of the parameter  1 in equation 7.33 has been increased to 1.9. This value, being larger than 1, would lead to an unstable network in the linear case. While nonlinear networks can also be unstable, the restriction to eigenvalues less than 1 is no longer the relevant condition.In a nonlinear network, the Fourier analysis of the input and output responses is no longer as informative as it is for a linear network. Due to the rectification, the   0, 1, and 2 Fourier components are all amplified compared to their input values. Nevertheless, except for rectification, the nonlinear recurrent network amplifies the input signal selectively in a manner similar to the linear network.In chapter 2, we discussed a feedforward model in which the elongated receptive fields of simple cells in primary visual cortex were formed by summing the inputs from neurons of the lateral geniculate nucleus with their receptive fields arranged in alternating rows of ON and OFF cells. While this model quite successfully accounts for a number of features of simple cells, such as orientation tuning, it is difficult to reconcile with the anatomy and circuitry of the cerebral cortex. By far the majority of the synapses onto any cortical neuron arise from other cortical neurons, not from thalamic afferents. Therefore, feedforward models account for the response properties of cortical neurons while ignoring the inputs that are numerically most prominent. The large number of intracortical connections suggests, instead, that recurrent circuitry might play an important role in shaping the responses of neurons in primary visual cortex.Ben-Yishai, where v is the firing rate of a neuron with preferred orientation . , To study orientation selectivity, we want to examine the tuning curves of individual neurons in response to stimuli with different orientation angles . The plots of network responses that we have been using show the firing rates v of all the neurons in the network as a function of their preferred stimulus angles  when the input stimulus has a fixed value, typically  0. As a consequence of the translation invariance of the network model, the response for other values of can be obtained simply by shifting this curve so that it plots v. Furthermore, except for the asymmetric effects of noise on the input, v is a symmetric function. These features follow from the fact that the network we are studying is invariant with respect to translations and sign changes of the angle variables that characterize the stimulus and response selectivities. An important consequence of this result is that the curve v, showing the response of the entire population, can also be interpreted as the tuning curve of a single neuron. If the response of the population to a stimulus angle is v, the response of a single neuron with preferred angle   0 is v  v from the symmetry of v. Because v is the tuning curve of a single neuron with   0 to a stimulus angle , the plots we show of v can be interpreted as both population responses and individual neuronal tuning curves. In the model of orientation tuning discussed in the previous section, recurrent amplification enhances selectivity. If the pattern of network connectivity amplifies nonselective rather than selective responses, recurrent interactions can also decrease selectivity. Recall from chapter 2 that neurons in the primary visual cortex are classified as simple or complex, depending on their sensitivity to the spatial phase of a grating stimulus. Simple cells respond maximally when the spatial positioning of the light and dark regions of a grating matches the locations of the ON and OFF regions of their receptive fields. Complex cells do not have distinct ON and OFF regions in their receptive fields, and respond to gratings of the appropriate orientation and spatial frequency relatively independently of where their light and dark stripes fall. In other words, complex cells are insensitive to spatial phase. In the absence of recurrent connections, the response of a neuron labeled by  is v  h, which is equal to the response of a simple cell with preferred spatial phase . However, for  1 sufficiently close to 1, the recurrent model produces responses that resemble those of complex cells. For a linear network, the response to two superimposed inputs is simply the sum of the responses to each input separately. A nonlinear recurrent network can generate an output that resembles the gain-modulated responses of posterior parietal neurons shown in figure 7.6, as noted by The effects illustrated in figures 7.12 and 7.13 arise because the nonlinear recurrent network has a stereotyped pattern of activity that is largely determined by interactions with other neurons in the network rather than by the feedforward input. If the recurrent connections are strong enough, the pattern of population activity, once established, can become independent of the structure of the input. For example, the recurrent network we have been studying can support a pattern of activity localized around a given preferred stimulus value, even when the input is uniform. This is seen in This memory mechanism is related to the integration seen in the linear model of eye position maintenance discussed previously. The linear network has an eigenvector e 1 with eigenvalue  1  1. This allows v  c 1 e 1 to be a static solution of the equations of the network in the absence of input for any value of c 1 . As a result, the network can preserve any initial value of c 1 as a memory. In the case of figure 7.14, the steady-state activity in the absence of tuned input is a function of   , for any value of the angle . As a result, the network can preserve any initial value of as a memory Recurrent networks can generate characteristic patterns of activity even when they receive complex inputs, and can maintain these patterns while receiving constant input. When a network responds to a constant input by relaxing to a steady state with dvdt 0 0 0, it is said to exhibit fixed-point behavior. Almost all the netfixed-point work activity we have discussed thus far involves such fixed points. This is by no means the only type of long-term activity that a network model can display. In a later section of this chapter, we discuss networks that oscillate, and chaotic behavior is also possible. But if certain conditions are met, a network will inevitably reach a fixed point in response to constant input. The theory of Lyapunov functions, to which we give an informal introduction, can be used to prove when this occurs.It is easier to discuss the Lyapunov function for a network if we use the firing-rate dynamics of equation has dLdt  0 whenever dIdt  0 0 0. To see this, take the time derivative of equation 7.40 and use 7.39 to obtainIf L is bounded from below, it cannot decrease indefinitely, so I  h  M  v must converge to a fixed point. This implies that v must converge to a fixed point as well.We have required that F   0 for all values of its argument I. However, with some technical complications, it can be shown that the Lyapunov function we have presented also applies to the case of the rectifying activation function F   I  , even though it is not differentiable at I  0 and F   0 for I  0. Convergence to a fixed point, or one of a set of fixed points, requires the Lyapunov function to be bounded from below. One way to ensure this is to use a saturating activation function, so that F is bounded as I  . Another way is to keep the eigenvalues of M sufficiently small.The models of memory discussed previously in this chapter store information by means of persistent activity. This is called working or short-term memory. In biological systems, persistent activity appears to play a role in retaining information over periods of seconds to minutes. Retention of long-term memories, over periods of hours to years, is thought to involve storage by means of synaptic strengths rather than persistent activity. One general idea is that synaptic weights in a recurrently connected network are set when a memory is stored so that the network can, at a later time, internally recreate the pattern of activity that represents the stored memory.In such networks, persistent activity is used to signal memory recall and to register the identity of the retrieved item, but the synaptic weights provide the long-term storage of the possible memory patterns. The pattern of activity of the units in the network at the start of memory retrieval determines which memory is recalled through its relationship to, or association with, the pattern of activity representing that memory. Such associative networks have been used to model regions of the mammalian brain implicated in various forms of memory, including area CA3 of the hippocampus and parts of the prefrontal cortex.In an associative memory, a partial or approximate representation of a stored item is used to recall the full item. Unlike a standard computer memory, recall in an associative memory is based on content rather than on an address. An example would be recalling every digit of a familiar phone number, given a few of its digits as an initial clue. In a network associative memory, recurrent weights are adjusted so that the network has a set of discrete fixed points identical to the patterns of activity that represent the stored memories. In many cases, the dynamics of the network are governed by a Lyapunov function, ensuring the existence of fixed points. Provided that not too many memories are stored, these fixed points can perfectly, or at least closely, match the memory patterns. During recall, an associative memory network performs the computational operation of pattern matching by finding the fixed-point that most closely matches the initial state of the network. Each memory pattern has a basin of attraction, defined as the set of initial activities from which the network evolves to that particular fixed point. These basins of attraction define the pattern-matching properties of the network.Associative memory networks can be constructed from units with either continuous-valued or binary activities. We consider a network of continuous-valued units described by equation  Therefore, we examine conditions under which such solutions exist for all the memory patterns. The capacity of a network is determined in part memory capacity by the number of different pre-specified vectors that can simultaneously satisfy equation 7.42 for an appropriate choice of M. In the limit of large N v , the capacity is typically proportional to N v . Capacity is not the only relevant measure of the performance of an associative memory. Memory function can be degraded if there are spurious fixed points of the network dynamics in addition to the fixed points that represent the memory patterns. Finally, useful pattern matching requires each fixed point to have a sufficiently large basin of attraction. Analyzing spurious fixed points and the sizes of basins of attraction is beyond the scope of this text.Although the units in the network have continuous-valued activities, we consider the simple case in which the units are either inactive or active in the memory patterns themselves. Inactive units correspond to components of v m that are equal to 0, and active units, to components that are equal to some constant value c. To simplify the discussion, we assume that each of the memory patterns has exactly N v active andN v inactive units. The choice of which units are active in each pattern is random, and independent of the other patterns. The parameter  is known as the sparseness of the memory patterns. As  decreases, making the sparseness  patterns more sparse, more of them can be stored but each contains less information.To build an associative memory network, we need to construct a matrix that allows all the memory patterns to satisfy equation 7.42. To begin, suppose that we knew of a matrix K for which all the memory patterns were degenerate eigenvectors with eigenvalue ,for all m. Then, consider the matrixHere n is a vector that has each of its N v components equal to 1. The term vector of ones n nn in the matrix represents uniform inhibition between network units. The existence of spurious fixed points decreases the usefulness of a network associative memory. This might seem to be a problem in the example we are discussing because the degeneracy of the eigenvalues means that any linear combination of memory patterns also satisfies equation 7.43. However, the nonlinearity in the network can prevent linear combinations of memory patterns from satisfying equation 7.42, even if they satisfy equation 7.43, thereby eliminating at least some of the spurious fixed points.The problem of constructing an associative memory network thus reduces to finding the matrix K of equation 7.43, or at least constructing a matrix with similar properties. Because the choice of active units in each memory pattern is independent, the probability that a given unit is active in two different memory patterns is  2 . Thus,Consider the dot product of one of the memory patterns, v m , with the vector v n  cn, for some value of n.It follows from these results that the matrix Recall that the Lyapunov function in equation 7.40 guarantees that the network has fixed points only if it is bounded from below and the matrix M is symmetric. Bounding of the Lyapunov function can be achieved if the activation function saturates. However, the recurrent weight matrix obtained by substituting expression 7.47 into equation 7.44 is not likely to be symmetric. A symmetric form of the recurrent weight matrix can be constructed by writingThe reader is urged to verify that, due to the additional terms in the sum over memory patterns, the conditions that must be satisfied when using 7.48 are slightly modified from 7.46 toOne way of looking at the recurrent weights in equation 7.48 is in terms of a learning rule used to construct the matrix. In this learning rule, an excitatory contribution to the coupling between two units is added whenever both of them are either active or inactive for a particular memory pattern. An inhibitory term is added whenever one unit is active and the other is not. The learning rule associated with equation 7.48 is called a covariance rule because of its relationship to the covariance matrix of the memory patterns. Learning rules for constructing networks that perform associative memory and other tasks are discussed in chapter 8. The matrix 7.48 that we use as a basis for constructing an associative memory network satisfies the conditions required for exact storage and recall of the memory patterns only approximately. This introduces some errors in recall. As the number of memory patterns increases, the approximation becomes worse and the performance of the associative memory deteriorates, which limits the number of memories that can be stored. The simple covariance prescription for the weights in equation 7.48 is far from optimal. Other prescriptions for constructing M can achieve significantly higher storage capacities.. The amount of information that can be stored is proportional to N 2 v , which is roughly the number of synapses in the network, but the information stored per synapse is typically quite small. In this section, we discuss models in which excitatory and inhibitory neurons are described separately by equations 7.12 and 7.13. These models exhibit richer dynamics than the single population models with symmetric coupling matrices we have analyzed up to this point. In models with excitatory and inhibitory subpopulations, the full synaptic weight matrix is not symmetric, and network oscillations can arise. We begin by analyzing a model of homogeneous coupled excitatory and inhibitory populations. We introduce methods for determining whether this model exhibits constant or oscillatory activity. We then present two network models in which oscillations appear. The first is a model of the olfactory bulb, and the second displays selective amplification in an oscillatory mode.As an illustration of the dynamics of excitatory-inhibitory network models, we analyze a simple model in which all of the excitatory neurons are described by a single firing rate, v E , and all of the inhibitory neurons are described by a second rate, v I . Although we think of this example as a model of interacting neuronal populations, it is constructed as if it consists of just two neurons. Equations 7.12 and 7.13, with threshold linear response functions, are used to describe the two firing rates, so thatThe synaptic weights M EE , M IE , M EI , and M II are numbers rather than matrices in this model. In the example we consider, we set M EE  1.25,Hz,  I  10 Hz,  E  10 ms, and we vary the value of  I . The negative value of  E means that this parameter serves as a source of constant background activity rather than as a threshold.The model of interacting excitatory and inhibitory populations given by equations 7.50 and 7.51 provides an opportunity for us to illustrate some of the techniques used to study the dynamics of nonlinear systems. This model exhibits both fixed-point and oscillatory activity, depending on the values of its parameters. Stability analysis can be used to determine the parameter values where transitions between these two types of activity take place.  dv I dt are positive on one side of their nullclines and negative on the other, as the reader can verify from equations 7.50 and 7.51 Above the nullcline along which dv E dt  0, dv E dt  0, and below it dv E dt  0. Similarly, dv I dt  0 to the right of the nullcline where dv I dt  0, and dv I dt  0 to the left of it. This determines the direction of flow in the phase plane, as denoted by the horizontal and vertical arrows in figure 7.17A. Furthermore, the rate of flow typically slows if the phase-plane trajectory approaches a nullcline.At a fixed point of a dynamic system, the dynamic variables remain at constant values. In the model being considered, a fixed point occurs when the firing rates v E and v I take values that make dv E dt  dv I dt  0. Because a fixed point requires both derivatives to vanish, it can occur only at an intersection of nullclines. The model we are considering has a single fixed point denoted by the filled circle in figure 7.17A. A fixed point provides a potential static configuration for the system, but it is critically important whether the fixed point is stable or unstable. If a fixed point is stable, initial values of v E and v I near the fixed point will be drawn toward it over time. If the fixed point is unstable, nearby configurations are pushed away from the fixed point, and the system will remain at the fixed point indefinitely only if the rates are set initially to the fixed-point values with infinite precision.Linear stability analysis can be used to determine whether a fixed point is stable or unstable. To do this we take derivatives of the expressions for dv E dt and dv I dt obtained by dividing the right sides of equations 7.50 and 7.51 by  E and  I respectively. We then evaluate these derivatives at the values of v E and v I that correspond to the fixed point. The four combinations of derivatives computed in this way can be arranged into a matrix stability matrixAs discussed in the Mathematical Appendix, the stability of the fixed point is determined by the real parts of the eigenvalues of this matrix. The eigenvalues are given byIf the real parts of both eigenvalues are less than 0, the fixed point is stable, whereas if either is greater than 0, the fixed point is unstable. If the factor under the radical sign in equation 7.53 is positive, both eigenvalues are real, and the behavior near the fixed point is exponential. This means that there is exponential movement toward the fixed point if both eigenvalues are negative, or away from the fixed point if either eigenvalue is positive. We focus on the case when the factor under the radical sign is negative, so that the square root is imaginary and the eigenvalues form a complex conjugate pair. In this case, the behavior near the fixed point is oscillatory and the trajectory either spirals into the fixed point, if the real part of the eigenvalues is negative, or out from the fixed point if the real part of the eigenvalues is positive. The imaginary part of the eigenvalue determines the frequency of oscillations near the fixed point. The real and imaginary parts of one of these eigenvalues are plotted as a function of  I Figures 7.18 and 7.19 show examples in which the fixed point is stable and unstable, respectively. In There are a number of ways that a nonlinear system can make a transition from a stable fixed point to a limit cycle. Such transitions are called bifurcations. The transition seen between figures 7.18 and 7.19 is a Hopf bifurcation. In this case, a fixed point becomes unstable as a parameterHopf bifurcation is changed when the real part of a complex eigenvalue changes sign. In a Hopf bifurcation, the limit cycle emerges at a finite frequency, which is similar to the behavior of a type II neuron when it starts firing action potentials, as discussed in chapter 6. Other types of bifurcations produce type I behavior with oscillations emerging at 0 frequency. One example of this is a saddle-node bifurcation, which ocsaddle-node bifurcation curs when parameters are changed such that two fixed points, one stable and one unstable, meet at the same point in the phase plane. The olfactory bulb, and analogous olfactory areas in insects, provide examples of sensory processing involving oscillatory activity. The olfactory bulb represents the first stage of processing beyond the olfactory receptors in the vertebrate olfactory system. Olfactory receptor neurons respond to odor molecules and send their axons to the olfactory bulb. These axons terminate in glomeruli where they synapse onto mitral and tufted cells, as mitral cells tufted cells well as local interneurons. The mitral and tufted cells provide the output of the olfactory bulb by sending projections to the primary olfactory cortex. They also synapse onto the larger population of inhibitory granule cells. The granule cells in turn inhibit the mitral and tufted cells.The activity in the olfactory bulb of many vertebrates is strongly influenced by a sniff cycle in which a few quick sniffs bring odors past the olfactory receptors. The field potential in figure 7.20A shows oscillations during each sniff, but not between sniffs. For the model to match this pattern of activity, the input from the olfactory receptors, h E , must induce a transition between fixed-point and oscillatory activity. Before a sniff, the network must have a stable fixed point with low activities. As h E increases during a sniff, this steady-state configuration must become unstable, leading to oscillatory activity. The analysis of the stability of the fixed point and the onset of oscillations is closely related to our previous stability analysis of the model of homogeneous populations of coupled excitatory and inhibitory neurons. It is based on properties of the eigenvalues of the linear stability matrix. In this case, the stability matrix includes contributions from the derivatives of the activation functions evaluated at the fixed point. For the fixed point to become unstable, the real part of at least one of the eigenvalues that arise in this analysis must become larger than 0. To ensure oscillations, at least one of these destabilizing eigenvalues should have a nonzero imaginary part. These requirements impose constraints on the connections between the mitral and granule cells and on the inputs. As a final example of network oscillations, we return to amplification of input signals by a recurrently connected network. Two factors control the amount of selective amplification that is viable in networks such as that shown in figure 7.9. The most important constraint on the recurrent weights is that the network must be stable, so the activity does not increase without bound. Another possible constraint is suggested by Up to this point, we have considered models in which the output of a cell is a deterministic function of its input. In this section, we introduce a network model called the Boltzmann machine, for which the input-output Boltzmann machine relationship is stochastic. Boltzmann machines are interesting from the perspective of learning, and also because they offer an alternative interpretation of the dynamics of network models.In the simplest form of Boltzmann machine, the neurons are treated as binary, so v a  1 if unit a is active at time t, and v a  0 if it is inactive.The state of unit a is determined by its total input current,M aa  v a  , . F is a sigmoidal function, which has the property that the larger the value of I a , the more likely unit a is to take the value 1.Under equation 7.55, the state of activity of the network evolves as a Markov chain. This means that the components of v at different times areAn advantage of using Glauber dynamics to define the evolution of a network model is that general results from statistical mechanics can be used to determine the equilibrium distribution of activities. Under Glauber dynamics, v does not converge to a fixed point, but can be described by a probability distribution associated with an energy function energy functionThe probability distribution characterizing v, once the network has converged to an equilibrium state, isThe notion of convergence as t   can be made precise but informally it means that after repeated updating according to equation 7.55, the states of the network are described statistically by equation 7.57. Z is called the partition function partition function and P The Boltzmann machine is inherently stochastic. However, an approximamean-field approximation tion to the Boltzmann machine, known as the mean-field approximation, can be constructed on the basis of the deterministic synaptic current dynamics of a firing-rate model. In this case, I is determined by the dynamic equation 7.39 rather than by equation 7.54, with the function F in equation 7.39 set to the same sigmoidal function as in equation 7.55. The output v a is determined from I a at discrete times. The rule used for this is not the deterministic relationship v a  F used in the firing-rate version of the model. Instead, v a is determined from I a stochastically, being set to either 1 or 0 with probability F or 1  F respectively. Thus, although the mean-field formulation for I is deterministic, I is used to generate a probability distribution over a binary output vector v. Because v a  1 has probability F and v a  0 has probability 1  F, and the units are independent, the probability distribution for the entire vector v isThis is called the mean-field distribution for the Boltzmann machine. Note mean-field distribution that this distribution plays no role in the dynamics of the mean-field formulation of the Boltzmann machine. It is, rather, a way of interpreting the outputs.We have presented two formulations of the Boltzmann machine, Gibbs sampling and the mean-field approach, that lead to the two distributions P  the distribution generated by Gibbs sampling. In this way, the mean-field procedure can be viewed as an approximation of Gibbs sampling.The power of the Boltzmann machine lies in the relationship between the distribution of output values, equation 7.57, and the quadratic energy function of equation 7.56. This makes it possible to determine how changing the weights M affects the distribution of output states. In chapter 8, we present a learning rule for the weights of the Boltzmann machine that allows P The models in this chapter mark the start of our discussion of computation, as opposed to coding. Using a description of the firing rates of network neurons, we showed how to construct linear and nonlinear feedforward and recurrent networks that transform information from one coordinate system to another, selectively amplify input signals, integrate inputs over extended periods of time, select between competing inputs, sustain activity in the absence of input, exhibit gain modulation, allow simple decoding with performance near the Cramr-Rao bound, and act as contentaddressable memories. We used network responses to a continuous stimulus variable as an extended example. This led to models of simple and complex cells in primary visual cortex. We described a model of the olfactory bulb as an example of a system for which computation involves oscillations arising from asymmetric couplings between excitatory and inhibitory neurons. Linear stability analysis was applied to a simplified version of this model. We also considered a stochastic network model called the Boltzmann machine.Here, we show that the Lyapunov function of equation 7.40 can be reduced to equation 7.59 when applied to the mean-field version of the Boltzmann machine. Recall, from equation 7.40, thatWhen F is given by the sigmoidal function of equation 7.55,where k is a constant, as can be verified by differentiating the right side. The nonconstant part of the right side of this equation is just the entropy associated with the binary variable v a . In fact, Similarly, from equation 7.57, we can show thatCombining the results of equations 7.62, 7.63, and 7.64, we obtainwhich gives equation 7.59 withNote that in this chapters, we define the KullbackLeibler divergence using a natural logarithm, rather than the base 2 logarithm used in chapter 4. The two definitions differ only by an overall multiplicative constant. Our discussion of the feedforward coordinate transformation model is based on Activity-dependent synaptic plasticity is widely believed to be the basic phenomenon underlying learning and memory, and it is also thought to play a crucial role in the development of neural circuits. To understand the functional and behavioral significance of synaptic plasticity, we must study how experience and training modify synapses, and how these modifications change patterns of neuronal firing to affect behavior. Experimental work has revealed ways in which neuronal activity can affect synaptic strength, and experimentally inspired synaptic plasticity rules have been applied to a wide variety of tasks including auto-and heteroassociative memory, pattern recognition, storage and recall of temporal sequences, and function approximation.In 1949, Donald Hebb conjectured that if input from neuron A often contributes to the firing of neuron B, then the synapse from A to B should be strengthened. Hebb suggested that such synaptic modification could proHebb rule duce neuronal assemblies that reflect the relationships experienced during training. The Hebb rule forms the basis of much of the research done on the role of synaptic plasticity in learning and memory. For example, consider applying this rule to neurons that fire together during training due to an association between a stimulus and a response. These neurons would develop strong interconnections, and subsequent activation of some of them by the stimulus could produce the synaptic drive needed to activate the remaining neurons and generate the associated response. Hebbs original suggestion concerned increases in synaptic strength, but it has been generalized to include decreases in strength arising from the repeated failure of neuron A to be involved in the activation of neuron B. General forms of the Hebb rule state that synapses change in proportion to the correlation or covariance of the activities of the pre-and postsynaptic neurons.Experimental work in a number of brain regions, including hippocampus, neocortex, and cerebellum, has revealed activity-dependent processes that can produce changes in the efficacies of synapses that persist for varying amounts of time. indicate amplitudes of field potentials evoked in the CA1 region of a slice of rat hippocampus by stimulation of the Schaffer collateral afferents. In experiments such as this, field potential amplitudes are used as a measure of synaptic strength. In A wealth of data is available on the underlying cellular basis of activitydependent synaptic plasticity. For instance, the postsynaptic concentration of calcium ions appears to play a critical role in the induction of both LTP and LTD. However, we will not consider mechanistic models. Rather, we study synaptic plasticity at a functional level, attempting to relate the impact of synaptic plasticity on neurons and networks to the basic rules governing its induction.Studies of plasticity and learning involve analyzing how synapses are affected by activity over the course of a training period. In this and the following chapters, we consider three types of training procedures. In unsupervised learning, a network responds to unsupervised learning a series of inputs during training solely on the basis of its intrinsic connections and dynamics. The network then self-organizes in a manner that depends on the synaptic plasticity rule being applied and on the nature of the inputs presented during training. We consider unsupervised learning in a more general setting called density estimation in chapter 10.In supervised learning, which we consider in the last section of this chapsupervised learning ter, a desired set of input-output relationships is imposed on the network by a "teacher" during training. Networks that perform particular tasks can be constructed in this way by letting a modification rule adjust their synapses until the desired computation emerges as a consequence of the training process. This is an alternative to explicitly specifying the synaptic weights, as was done in chapter 7. In this case, finding a biologically plausible teaching mechanism may not be a concern if the question being addressed is whether any weights can be found that allow a network to implement a particular function. In more biologically plausible examples of supervised learning, one network acts as the teacher for another network.In chapter 9, we discuss a third form of learning, reinforcement learning, that is intermediate between these cases. In reinforcement learning, the reinforcement learning network output is not constrained by a teacher, but evaluative feedback about network performance is provided in the form of reward or punishment. This can be used to control the synaptic modification process.In this chapter we largely focus on activity-dependent synaptic plasticity of the Hebbian type, meaning plasticity based on correlations of pre-and postsynaptic firing. To ensure stability and to obtain interesting results, we often must augment Hebbian plasticity with more global forms of synaptic modification that, for example, scale the strengths of all the synapses onto a given neuron. These can have a major impact on the outcome of development or learning. Non-Hebbian forms of synaptic plasticity, such as those non-Hebbian plasticity that modify synaptic strengths solely on the basis of pre-or postsynaptic firing, are likely to play important roles in homeostatic, developmental, and learning processes. Activity can also modify the intrinsic excitability and response properties of neurons. Models of such intrinsic plasticity show that neurons can be remarkably robust to external perturbations if they adjust their conductances to maintain specified functional characteristics. Intrinsic and synaptic plasticity can interact in interesting ways. For example, shifts in intrinsic excitability can compensate for changes in the level of input to a neuron caused by synaptic plasticity. It is likely that all of these forms of plasticity, and many others, are important elements of both the stability and the adaptability of nervous systems.In this chapter, we describe and analyze basic correlation-and covariancebased synaptic plasticity rules in the context of unsupervised learning, and discuss their extension to supervised learning. As an example, we discuss the development of ocular dominance in cells of the primary visual cortex, and the formation of the map of ocular dominance preferences across the cortical surface. In the models we discuss, synaptic strengths are characterized by synaptic weights, defined as in chapter 7.Increasing synaptic strength in response to activity is a positive feedback process. The activity that modifies synapses is reinforced by Hebbian plasticity, which leads to more activity and further modification. Without appropriate adjustments of the synaptic plasticity rules or the imposition of constraints, Hebbian modification tends to produce uncontrolled growth of synaptic strengths.The easiest way to control synaptic strengthening is to impose an upper limit on the value that a synaptic weight can take. Such an upper limit is supported by LTP experiments. It also makes sense to prevent weights from changing sign, because the plasticity processes we are modeling cannot change an excitatory synapse into an inhibitory synapse or vice versa. We therefore impose the constraint, which we call a saturation constraint, that all excitatory synaptic weights must lie between 0 and a maximum synaptic saturation value w max , which is a constant. The simplest implementation of saturation is to set any weight that would cross a saturation bound due to application of a plasticity rule to the limiting value.Uncontrolled growth is not the only problem associated with Hebbian plasticity. Synapses are modified independently under a Hebbian rule, which can have deleterious consequences. For example, all of the synaptic weights may be driven to their maximum allowed values w max , causing the postsynaptic neuron to lose selectivity to different patterns of input. The development of input selectivity typically requires competition between different synapses, so that some are forced to weaken when others synaptic competition become strong. We discuss a variety of synaptic plasticity rules that introduce competition between synapses. In some cases, the same mechanism that leads to competition also stabilizes growth of the synaptic weights. In other cases, it does not, and saturation constraints must also be imposed.Rules for synaptic modification take the form of differential equations describing the rate of change of synaptic weights as a function of the preand postsynaptic activity and other possible factors. In this section, we give examples of such rules. In later sections, we discuss their computational implications.In the models of plasticity that we study, the activity of each neuron is described by a continuous variable, not by a spike train. As in chapter 7, we use the letter u to denote the presynaptic level of activity and v to denote the postsynaptic activity. Normally, u and v represent the firing rates of the pre-and postsynaptic neurons, in which case they should be restricted to nonnegative values. Sometimes, to simplify the analysis, we ignore this constraint. An activity variable that takes both positive and negative values can be interpreted as the difference between a firing rate and a fixed background rate, or between the firing rates of two neurons being treated as a single unit. Finally, to avoid extraneous conversion factors in our equations, we take u and v to be dimensionless measures of the corresponding neuronal firing rates or activities. For example, u and v could be the firing rates of the pre-and postsynaptic neurons divided by their maximum or average values.In the first part of this chapter, we consider unsupervised learning as applied to a single postsynaptic neuron driven by N u presynaptic inputs with activities represented by u b for b  1, 2, . . . , N u , or collectively by the vector u. In unsupervised learning, the postsynaptic activity v is evoked directly by the presynaptic activity u. We describe v using a linear version of the firing-rate model discussed in chapter 7,where  r is a time constant that controls the firing-rate response dynamics. Recall that w b is the synaptic weight that describes the strength of the synapse from presynaptic neuron b to the postsynaptic neuron, and w is the vector formed by all N u synaptic weights. The individual synaptic weight vector w weights can be either positive, representing excitation, or negative, representing inhibition. Equation 8.1 does not include any nonlinear dependence of the firing rate on the total synaptic input, not even rectification. Using such a linear firing-rate model considerably simplifies the analysis of synaptic plasticity. The restriction to nonnegative v either will be imposed by hand or, sometimes, will be ignored to simplify the analysis.The processes of synaptic plasticity are typically much slower than the dynamics characterized by equation 8.1. If, in addition, the stimuli are presented slowly enough to allow the network to attain its steady-state activity during training, we can replace the dynamic equation 8.1 bywhich instantaneously sets v to the asymptotic steady-state value determined by equation 8.1. This is the equation we primarily use in our analysis of synaptic plasticity in unsupervised learning. Synaptic modification is included in the model by specifying how the vector w changes as a function of the pre-and postsynaptic levels of activity. The complex time course of plasticity seen in figure 8.1 is simplified by modeling only the longer-lasting changes.The simplest plasticity rule that follows the spirit of Hebbs conjecture takes the formwhere  w is a time constant that controls the rate at which the weights change. This equation, which we call the basic Hebb rule, implies that sibasic Hebb rule multaneous pre-and postsynaptic activity increases synaptic strength. If the activity variables represent firing rates, the right side of this equation can be interpreted as a measure of the probability that the pre-and postsynaptic neurons both fire spikes during a small time interval.Synaptic plasticity is generally modeled as a slow process that gradually modifies synaptic weights over a time period during which the components of u take a variety of different values. Each different set of u values is called an input pattern. The direct way to compute the weight changes induced by a series of input patterns is to sum the small changes caused by each of them separately. A convenient alternative is to average over all of the different input patterns and compute the weight changes induced by this average. As long as the synaptic weights change slowly enough, the averaging method provides a good approximation of the weight changes produced by the set of input patterns.In this chapter, we use angle brackets to denote averages over the ensemble of input patterns presented during training. The Hebb rule of equation 8.3, when averaged over the inputs used during training, becomes averaged Hebb ruleIn unsupervised learning, v is determined by equation 8.2, and if we replace v with w  u, we can rewrite the averaged plasticity rule as correlation-based rulewhere Q is the input correlation matrix given by input correlation matrix.5 is called a correlation-based plasticity rule because of the presence of the input correlation matrix.Whether or not the pre-and postsynaptic activity variables are restricted to nonnegative values, the basic Hebb rule is unstable. To show this, we consider the square of the length of the weight vector, w 2  w  w  b w 2 b . Taking the dot product of equation 8.3 with w, and noting that dw 2 dt  2w  dwdt and that w  u  v, we find that  w dw 2 dt  2v 2 , which is always positive. Thus, the length of the weight vector grows continuously when the rule 8.3 is applied. To avoid unbounded growth, we must impose an upper saturation constraint. A lower limit is also required if the activity variables are allowed to be negative. Even with saturation, the basic Hebb rule fails to induce competition between different synapses.Sometimes, synaptic modification is modeled as a discrete rather than continuous process, particularly if the learning procedure involves the sequential presentation of inputs. In this case, equation 8.5 is replaced by a discrete updating rulewhere  is a parameter, analogous to the learning rate 1 w in the continuous rule, that determines the amount of modification per application of the rule.If, as in Hebbs original conjecture, u and v are interpreted as representing firing rates, the basic Hebb rule describes only LTP. Experiments, such as the one shown in where  v is a threshold that determines the level of postsynaptic activpostsynaptic threshold  v ity above which LTD switches to LTP. As an alternative to equation 8.8, we can impose the threshold on the input rather than output activity, and writeHere,    u is a vector of thresholds that determines the levels of presynaptic presynaptic threshold    u activities above which LTD switches to LTP. It is also possible to combine these two rules by subtracting thresholds from both the u and v terms, but this has the undesirable feature of predicting LTP when pre-and postsynaptic activity levels are both low.A convenient choice for the thresholds is the average value of the corresponding variable over the training period. In other words, we set the threshold in equation 8.8 to the average postsynaptic activity,  v  v , or the threshold vector in equation 8.9 to the average presynaptic activity vector,    u  u . As we did for equation 8.5, we use the relation v  w  u and average over training inputs to obtain an averaged form of the plasticity rule. When the thresholds are set to their corresponding activity averages, equations 8.8 and 8.9 both produce the same averaged rule, input covariance matrix C  w dw dt  C  w , Because of the presence of the covariance matrix in equation 8.10, equations 8.8 and 8.9 are known as covariance rules. covariance rules Although they both average to give equation 8.10, the rules in equations 8.8 and 8.9 have their differences. Equation 8.8 modifies synapses only if they have nonzero presynaptic activities. When v   v , this produces an effect called homosynaptic depression. In contrast, equation 8.9 homosynaptic and heterosynaptic depression reduces the strengths of inactive synapses if v  0. This is called heterosynaptic depression. Note that maintaining  v  v in equation 8.8 requires changing  v as the weights are modified. In contrast, the threshold in equation 8.9 is independent of the weights and does not need to be changed during the training period to keep    u  u .Even though covariance rules include LTD and thus allow weights to decrease, they are unstable because of the same positive feedback that makes the basic Hebb rule unstable. For either rule 8.8 with  v  v or rule 8.9 with    u  u ,  w dw 2 dt  2v. The time average of the right side of this equation is proportional to the variance of the output, v 2  v 2 , which is positive except in the trivial case when v is constant. Also similar to the case of the Hebb rule is the fact that the covariance rules are noncompetitive, but competition can be introduced by allowing the thresholds to slide, as described below.The covariance-based rule of equation 8.8 does not require any postsynaptic activity to produce LTD, and rule 8.9 can produce LTD without presynaptic activity. As in equation 8.8,  v acts as a threshold on the postsynaptic activity that determines whether synapses are strengthened or weakened.If the threshold  v is held fixed, the BCM rule, like the basic Hebbian rule, is unstable. Synaptic modification can be stabilized against unbounded growth by allowing the threshold to vary. The critical condition for stability is that  v must grow more rapidly than v as the output activity grows large. In one instantiation of the BCM rule with a sliding threshold,  v acts sliding threshold as a low-pass filtered version of v 2 , as determined by the equationHere   sets the time scale for modification of the threshold. This is usually slower than the presentation of individual presynaptic patterns, but faster than the rate at which the weights change, which is determined by  w . With a sliding threshold, the BCM rule implements competition between synapses because strengthening some synapses increases the postsynaptic firing rate, which raises the threshold and makes it more difficult for other synapses to be strengthened or even to remain at their current strengths.The BCM rule stabilizes Hebbian plasticity by means of a sliding threshold that reduces synaptic weights if the postsynaptic neuron becomes too active. This amounts to using the postsynaptic activity as an indicator of the strengths of synaptic weights. A more direct way to stabilize a Hebbian plasticity rule is to add terms that depend explicitly on the weights. This typically leads to some form of weight normalization, which corresponds to the idea that postsynaptic neurons can support only a fixed total synaptic weight, so increases in some weights must be accompanied by decreases in others.Normalization of synaptic weights involves imposing some sort of global constraint. Two types of constraints are typically used. If the synaptic weights are nonnegative, their growth can be limited by holding the sum of all the weights of the synapses onto a given postsynaptic neuron to a constant value. An alternative, which also works for weights that can be either positive or negative, is to constrain the sum of the squares of the weights instead of their linear sum. In either case, the constraint can be imposed either rigidly, requiring that it be satisfied at all times during the training process, or dynamically, requiring only that it be satisfied asymptotically at the end of training. We discuss one example of each type: a rigid scheme for imposing a constraint on the sum of synaptic weights, and a dynamic scheme for constraining the sum over their squares. Dynamic constraints can be applied in the former case and rigid constraints in the latter, but we restrict our discussion to two widely used schemes. We discuss synaptic normalization in connection with the basic Hebb rule, but the results we present can be applied to covariance rules as well. Weight normalization can drastically alter the outcome of a training procedure, and different normalization methods may lead to different outcomes.The sum over synaptic weights that is constrained by subtractive normalization can be written as w b  n  w where n is an N u -dimensional vector with all its components equal to 1. This sum can be constrained by replacing equation 8.3 with Hebb rule with subtractive normalizationNote that n  u is simply the sum of all the inputs. This rule imposes what is called subtractive normalization because the same quantity is subtracted from the change to each weight, whether that weight is large or small. Subtractive normalization imposes the constraint on the sum of weights rigidly because it does not allow the Hebbian term to change n  w. To see this, we take the dot product of equation 8.14 with n to obtainThe last equality follows because n  n  N u . Hebbian modification with subtractive normalization is nonlocal in that it requires the sum of all the input activities, n  u, to be available to the mechanism that modifies each synapse. It is not obvious how such a rule could be implemented biophysically.Subtractive normalization must be augmented by a saturation constraint that prevents weights from becoming negative. If the rule 8.14 attempts to drive any of the weights below 0, the saturation constraint prevents this change. At this point, the rule is not applied to any saturated weights, and its effect on the other weights is modified. Both modifications can be achieved by setting the components of the vector n corresponding to any saturated weights to 0 and the factor of N u in equation 8.14 equal to the sum of the components of this modified n vector. Without any upper saturation limit, this procedure often results in a final outcome in which all the weights but one have been set to 0. To avoid this, an upper saturation limit is also typically imposed. Hebbian plasticity with subtractive normalization is highly competitive because small weights are reduced by a larger proportion of their sizes than are large weights.A constraint on the sum of the squares of the synaptic weights can be imposed dynamically by using a modification of the basic Hebb rule known as the Oja rule where  is a positive constant. This rule involves only information that is local to the synapse being modified, but its form is based more on theoretical arguments than on experimental data. The normalization it imposes is called multiplicative because the amount of modification induced by the second term in equation 8.16 is proportional to w.The stability of the Oja rule can be established by taking the dot product of equation 8.16 with the weight vector w to obtainThis indicates that w 2 will relax over time to the value 1, which obviously prevents the weights from growing without bound, proving stability. It also induces competition between the different weights, because when one weight increases, the maintenance of a constant length for the weight vector forces other weights to decrease.Experiments have shown that the relative timing of pre-and postsynaptic action potentials plays a critical role in determining the sign and amplitude of the changes in synaptic efficacy produced by activity. Simulating the spike-timing dependence of synaptic plasticity requires a spiking model. However, an approximate model can be constructed on the basis of firing rates. The effect of pre-and postsynaptic timing can be included in a synaptic modification rule by including a temporal difference marked by open symbols, the presynaptic spike occurred either 10 or 100 ms before the postsynaptic neuron fired an action potential. The traces marked by solid symbols denote the reverse ordering in which the presynaptic spike occurred either 10 or 100 ms after the postsynaptic spike. Separations of 100 ms had no long-lasting effect. In contrast, the 10 ms delays produced effects that built up to a maximum over a 5-to-10-minute period and lasted for the duration of the experiment. Pairing a presynaptic action potential with a postsynaptic action potential 10 ms later produced LTP, whereas the reverse ordering generated LTD.  If H is positive for positive  and negative for negative , the first term on the right side of this equation represents LTP, and the second, LTD. The solid curve in Rules in which synaptic plasticity is based on the relative timing of preand postsynaptic action potentials still require saturation constraints for stability, but, in spiking models, they can generate competition between synapses without further constraints or modifications. This is because different synapses compete to control the timing of postsynaptic spikes. Synapses that are able to evoke postsynaptic spikes rapidly get strengthened. These synapses then exert a more powerful influence on the timing of postsynaptic spikes, and they tend to generate spikes at times that lead to the weakening of other synapses that are less capable of controlling postsynaptic spike timing.We now consider the computational properties of the different synaptic modification rules we have introduced in the context of unsupervised learning. Unsupervised learning provides a model for the effects of activity on developing neural circuits and the effects of experience on mature networks. We separate the discussion of unsupervised learning into cases involving a single postsynaptic neuron and cases in which there are multiple postsynaptic neurons.A major area of research in unsupervised learning concerns the development of neuronal selectivity and the formation of cortical maps. Chapters 1 and 2 provided examples of the selectivities of neurons in various cortical areas to features of the stimuli to which they respond. In many cases, neuronal selectivities are arranged across the cortical surface in an orderly and regular pattern known as a cortical map. The patterns of concortical map nection that give rise to neuronal selectivities and cortical maps are established during development by both activity-independent and activitydependent processes. A conventional view is that activity-independent mechanisms control the initial targeting of axons, determine the appropriate layer for them to innervate, and establish a coarse order or patterning in their projections. Other activity-independent and activity-dependent mechanisms then refine this order and help to create and preserve neuronal selectivities and cortical maps.Although the relative roles of activity-independent and activity-dependent processes in cortical development are the subject of extensive debate, developmental models based on activity-dependent plasticity have played an important role in suggesting key experiments and successfully predicting their outcomes. A detailed analysis of the more complex patternforming models that have been proposed is beyond the scope of this book. Instead, in this and later sections, we give a brief overview of some different approaches and the results that have been obtained. In this section, we consider the case of ocular dominance.Ocular dominance refers to the tendency of input to neurons in the adult ocular dominance primary visual cortex of many mammalian species to favor one eye over the other. This is especially true for neurons in layer 4, which receive extensive innervation from the LGN. Neurons dominated by one eye or the other occupy different patches of cortex, and areas with left-or right-eye ocular dominance alternate across the cortex in fairly regular bands known as ocular dominance stripes. In a later section, we discuss how this cortical ocular dominance stripes map can arise from Hebbian plasticity.Equation Any N u -dimensional vector can be represented using the eigenvectors as a basis, so we can write The exponential factors in 8.20 all grow over time, because the eigenvalues   are positive for all  values. For large t, the term with the largest value of   becomes much larger than any of the other terms and dominates the sum for w. This largest eigenvalue has the label   1, and its corresponding eigenvector e 1 is called the principal eigenvector. Thus, for large t, w  e 1 to a good approximation, provided that principal eigenvector w  e 1  0. After training, the response to an arbitrary input vector u is well approximated by v  e 1  u .  The proportionality sign in equation 8.21 hides the large multiplicative factor exp that is a consequence of the positive feedback inherent in Hebbian plasticity. One way to limit growth of the weight vector in equation 8.5 is to impose a saturation constraint. This can have significant effects on the outcome of Hebbian modification, including, in some cases, preventing the weight vector from ending up proportional to the principal eigenvector. Another way to eliminate the large exponential factor in the weights is to use the Oja rule, 8.16, instead of the basic Hebb rule. The weight vector generated by the Oja rule, in the example we have discussed, approaches w  e 1  12 as t  . In other words, the Oja rule gives a weight vector that is parallel to the principal eigenvector, but normalized to a length of 1 12 rather than growing without bound.Finally, we examine the effect of including a subtractive constraint, as in equation 8.14. Averaging equation 8.14 over the training inputs, we find averaged Hebb rule with subtractive constraintIf we once again express w as a sum of eigenvectors of Q, we find that the growth of each coefficient in this sum is unaffected by the extra term in equation 8.22, provided that e   n  0. However, if e   n  0, the extra term modifies the growth. In our discussion of ocular dominance, we consider a case in which the principal eigenvector of the correlation matrix is proportional to n. In this case, Q  e 1 nN  0, so the projection in the direction of the principal eigenvector is unaffected by the synaptic plasticity rule. Furthermore, e   n  0 for   2 because the eigenvectors of the correlation matrix are mutually orthogonal, which implies that the evolution of the remaining eigenvectors is unaffected by the constraint. As a result,Thus, ignoring the effects of any possible saturation constraints, the synaptic weight matrix tends to become parallel to the eigenvector with the second largest eigenvalue as t  .In summary, if weight growth is limited by some form of multiplicative normalization, as in the Oja rule, the configuration of synaptic weights produced by Hebbian modification typically will be proportional to the principal eigenvector of the input correlation matrix. When subtractive normalization is used and the principal eigenvector is proportional to n, the eigenvector with the next largest eigenvalue provides an estimate of the configuration of final weights, again up to a proportionality factor. If, however, saturation constraints are used, as they must be in a subtractive scheme, this can invalidate the results of a simplified analysis based solely on these eigenvectors If applied for a long enough time, both the basic Hebb rule to the principal eigenvector of the correlation matrix of the inputs used during training. Any unit that obeys equation 8.2 characterizes the state of its N u inputs by a single number v, which is equal to the projection of u onto the weight vector w. Intuition suggests, and a technique known as principal component analysis formalizes, that setting the projection direction w principal components analysis PCA proportional to the principal eigenvector e 1 is often the optimal choice if a set of vectors is to be represented by, and reconstructed from, a set of single numbers through a linear relation. For example, if e 1 is normalized so that e 1   1, the vectors vw with v  w  u and w  e 1 provide the best estimates that can be generated from single numbers of the set of input vectors u used to construct Q. Furthermore, the fact that projection along this direction maximizes the variance of the outputs that result can be interpreted using information theory. The entropy of a Gaussian distributed random variable with variance  2 grows with increasing variance as log 2 . For Gaussian input statistics, and output that is corrupted by noise that is also Gaussian, maximizing the variance of v by a Hebbian rule maximizes not only the output entropy but also the amount of information v carries about u. In chapter 10, we further consider the computational significance of finding the direction of maximum variance in the input data set, and we discuss the relationship between this and general techniques for extracting structure from input statistics. As an example of a developmental model of ocular dominance at the single neuron level, we consider the highly simplified case of a single layer 4 cell that receives input from just two LGN afferents. One afferent is associated with the right eye and has activity u R , and the other is from the left eye and has activity u L . Two synaptic weights w  describe the strengths of these projections, and the output activity v is determined byThe weights in this model are constrained to nonnegative values. Initially, the weights for the right-and left-eye inputs are taken to be approximately equal. Ocular dominance arises when one of the weights is pushed to 0 while the other remains positive.We can estimate the results of a Hebbian developmental process by considering the input correlation matrix. We assume that the two eyes are statistically equivalent, so the correlation matrix of the right-and left-eye inputs takes the formThe subscripts S and D denote same-and different-eye correlations. The eigenvectors are e 1   2 and e 2   2 for this correlation matrix, and their eigenvalues are  1  q S  q D and  2  q S  q D .If the right-and left-eye weights evolve according to equation 8.5, it is straightforward to show that the eigenvector combinations w   w R  w L and w   w R  w L obey the uncoupled equationsPositive correlations between the two eyes are likely to exist after eye opening has occurred. This means that q S  q D  q S  q D , so, according to equations 8.26, w  grows more rapidly than w  . Equivalently, e 1   2 is the principal eigenvector. The basic Hebbian rule thus predicts a final weight vector proportional to e 1 , which implies equal innervation from both eyes. This is not the observed outcome of cortical development.   2. If this is positive, w R increases and w L decreases if it is negative, w L increases and w R decreases. Eventually, the decreasing weight will hit the saturation limit of 0, and the other weight will stop increasing due to the normalization constraint. At this point, total dominance by one eye or the other has been achieved. This simple model shows that ocular dominance can arise from Hebbian plasticity if there is sufficient competition between the growth of the left-and right-eye weights.Hebbian models can also account for the development of the orientation selectivity displayed by neurons in primary visual cortex. The model of Hubel and Wiesel for generating an orientation-selective simple cell response by summing linear arrays of alternating ON and OFF LGN inputs was presented in chapter 2. The necessary pattern of LGN inputs can arise from Hebbian plasticity on the basis of correlations between the responses of different LGN cells and competition between ON and OFF units. Such a model can be constructed by considering a simple cell receiving input from ON-center and OFF-center cells of the LGN, and applying Hebbian  As in the case of ocular dominance, the development of orientation selectivity can be partially analyzed on the basis of properties of the correlation matrix driving Hebbian development. The critical feature required to produce orientation-selective receptive fields is the growth of components proportional to eigenvectors that are not spatially uniform or rotationally invariant. Application of a Hebbian rule without constraints leads to growth of a uniform component resulting in an unstructured receptive field. Appropriate constraints can eliminate growth of this component, producing spatial structured receptive fields. The development of receptive fields that are not rotationally invariant, and that therefore exhibit orientation selectivity, relies on nonlinear aspects of the model and is therefore difficult to study analytically. For this reason, we simply present simulation results.Neurons in primary visual cortex receive afferents from LGN cells centered over a finite region of the visual field. This anatomical constraint can be included in developmental models by introducing what is called an arbor function. The arbor function, which is often taken to be Gausarbor function sian, characterizes the density of innervation from different visual locations to the cell being modeled. As a simplification, this density is not altered during the Hebbian developmental process, but the strengths of synapses within the arbor are modified by the Hebbian rule. The outcome is oriented receptive fields of a limited spatial extent. Unlike correlation-or covariance-based rules, temporal Hebbian rules allow changes of synaptic strength to depend on the temporal sequence of activity across the synapse. This allows temporal Hebbian rules to exhibit a phenomenon called trace learning, where the term trace refers to the histrace learning tory of synaptic activity.We can approximate the final result of applying a temporal plasticity rule by integrating equation 8.18 from t  0 to a large final time t  T, assuming that w  0 initially, and shifting the integration variable to obtainThe approximation comes from ignoring both small contributions associated with the end points of the integral and the change in v produced during training by the modification of w. Equation 8.27 shows that temporally dependent Hebbian plasticity depends on the correlation between the postsynaptic activity and the presynaptic activity temporally filtered by the function H.Equation To study the effect of plasticity on multiple neurons, we introduce the network of figure 8.6, in which N v output neurons receive input through N u feedforward connections and from recurrent interconnections. A vector v represents the activities of the multiple output units, and the feedforward synaptic connections are described by a matrix W, with the element W ab feedforward weight matrix W giving the strength and sign of the synapse from input unit b to output unit a. The strength of the recurrent connection from output unit a  to output unit a is described by element M aa  of the recurrent weight matrix recurrent weight matrix M M.The activity vector for the output units of the network in figure 8.6 is determined by a linear version of the recurrent model of chapter 7,Provided that the real parts of the eigenvalues of M are less than 1, this equation has a stable fixed point with a steady-state output activity vector determined by.29 can be solved by defining the matrix inversewhere I is the identity matrix, to obtainIt is important for different output neurons in a multi-unit network to be selective for different aspects of the input, or else their responses will be redundant. For the case of a single cell, competition between different synapses could be used to ensure that synapse-specific plasticity rules do not modify all of the synapses onto a postsynaptic neuron in the same way. For multiple output networks, fixed or plastic linear or nonlinear recurrent interactions can be used to ensure that the units do not all develop the same selectivity. In the following sections, we consider three different patterns of plasticity: plastic feedforward and fixed recurrent synapses, plastic feedforward and recurrent synapses, and, finally, fixed feedforward and plastic recurrent synapses.Ocular dominance stripes can arise in a Hebbian model with plastic feedforward but fixed recurrent synapses. In the single-cell model of ocular dominance considered previously, the ultimate ocular preference of the output cell depends on the initial conditions of its synaptic weights. A multiple-output version of the model without any recurrent connections would therefore generate a random pattern of selectivities across the cortex if it started with random weights. where Q  uu is the input autocorrelation matrix. Equation 8.31 has the same form as the single unit equation 8.5, except that both K and Q affect the growth of W.We consider a highly simplified model of the development of ocular dominance maps that considers only a single direction across the cortex and a single point in the visual field. The index a denoting the identity of a given output unit also represents the location of that unit on the cortex. This linking of a to locations on the cortical surface allows us to interpret the results of the model in terms of a cortical map.Writing w   w R  w L and w   w R  w L , the equivalent of equation 8.26 for these sum and difference vectors isAs in the single-cell model of ocular dominance, subtractive normalization, which holds the value of w  fixed while leaving the growth of w  unaffected, eliminates the tendency for the cortical cells to become binocular. Then only the equation for w  is relevant, and the growth of w  is dominated by the principal eigenvector of K. The sign of the component w   a determines whether the neuron in the region of the cortex corresponding to the label a is dominated by the right eye or the left eye. Oscillations in the signs of the components of w  translate into ocular dominance stripes.We assume that the connections between the output neurons are translation invariant, so that K aa   K depends only on the separation between the cortical cells a and a  . Note that it is more convenient to consider the form of the function K than to discuss the connections between output neurons directly. We use a convenient trick to remove edge effects, which is to impose periodic boundary conditions that require the activities of the units with a  0 and a  N v to be identical. This provides a reasonable model of a patch of the cortex away from regional boundaries. In some cases, edge effects can impose important constraints on the overall structure of maps, but we do not analyze this here.In the case of periodic boundary conditions, the eigenvectors of K have the formfor   0, 1, 2, . . . , N v 2, and with a phase parameter  that can take any value from 0 to 2. The eigenvalues are given by the discrete Fourier shows the result of a simulation of Hebbian development of an ocular dominance map for a one-dimensional line across cortex consisting of 512 units. In this simulation, constraints that limit the growth of synaptic weights have been included, but these do not dramatically alter the conclusions of our analysis.In this section and the next, we consider models that deal with Hebbian learning and development on a more abstract level. Although inspired by features of neural circuitry, these models are less directly linked to neurons and synapses. For example, the model discussed in this section considers We have plotted this as a function of the distance between the cortical locations corresponding to the indices a and a  , rather than of a  a  . The dotted line is the principal eigenvector plotted on the same scale.K, the Fourier transform of K. This is also given by the difference of two Gaussians. As in A, we use cortical distance units and plotK in terms of the spatial frequency k rather than the integer index .the effects of excitation and inhibition by modeling competitive and cooperative aspects of how activity is generated in, and spread across, the cortex. The advantage of this approach is that it allows us to deal with complex, nonlinear features of Hebbian models in a more controlled and manageable way. As we have seen, competition between neurons is an important element in the development of realistic patterns of selectivity. Linear recurrent connections can produce only a limited amount of differentiation among network neurons, because they induce fairly weak competition between output units. As detailed in chapter 7, recurrent connections can lead to much stronger competition if the interactions are nonlinear. The model discussed in this section allows strongly nonlinear competition nonlinear competition to arise in a Hebbian setting.As mentioned in the previous paragraph, the model we discuss represents the effect of cortical processing in two somewhat abstract stages. One stage, modeling the effects of long-range inhibition, involves competition among all the cortical cells for feedforward input in a scheme related to that used in chapter 2 for contrast saturation. The second stage, modeling shorter-range excitation, involves cooperation in which neurons that receive feedforward input excite their neighbors.In the first stage, the feedforward input for unit a, and that for all the other units, is fed through a nonlinear function to generate a competitive measure of the local excitation generated at location a,The activities and weights are all assumed to be positive. The parameter  controls the degree of competition. For large , only the largest feedforward input survives. The case   1 is quite similar to the linear recurrent connections of the previous section. In the cooperative stage, the local excitation of equation 8.34 is distributed across the cortex by recurrent connections, producing a level of activity in unit a given by Using the outputs of equation 8.35 in conjunction with a Hebbian rule for the feedforward weights is called competitive Hebbian learning. The comcompetitive Hebbian learning petition between neurons implemented by this scheme does not ensure competition among the synapses onto a given neuron, so some mechanism such as a normalization constraint is still required. For these models, the outcome of training cannot be analyzed simply by considering eigenvectors of the covariance or correlation matrix because the activation process is nonlinear. Rather, higher-order statistics of the input distribution are important. Nonlinear competition can lead to strong differentiation of output units.An example of the use of competitive Hebbian learning is shown in figure 8.9, in the form of a one-dimensional cortical map of ocular dominance with inputs arising from LGN neurons with receptive fields covering an extended region of the visual field. This example uses competitive Hebbian plasticity with nondynamic multiplicative weight normalization. Two weight matrices, W R and W L , corresponding to right-and left-eye inputs, characterize the connectivity of the model. These are shown separately in It is possible to analyze the structure shown in Models of cortical map formation can get extremely complex when multiple neuronal selectivities, such as retinotopic location, ocular dominance, and orientation preference, are considered simultaneously. To deal with this, a class of more abstract models, called competitive feature-based models, has been developed. These use a general approach similar to the competitive Hebbian models discussed in the previous section. Featurebased models are not directly related to the biophysical reality of neuronal firing rates and synaptic strengths, but they provide a compact description of map development.In a feedforward model, the selectivity of neuron a is determined by the feedforward weights W ab for all values of b, describing how this neuron is connected to the input units of the network. The input units are driven by the stimulus and their responses reflect various stimulus features. Thus, selectivity in these models is determined by how the synaptic weights transfer the selectivities of the input units to the output units. The idea of a feature-based model is to simplify this by directly relating the output unit selectivities to the corresponding features of the stimulus. In featurebased models, the index b is not used to label different input units, but rather to label different features of the stimulus. N u is thus equal to the number of parameters being used to characterize the stimulus. Also, the input variable u b is set to the parameter used to characterize feature b of the stimulus. Similarly, W ab does not describe the coupling between input unit b and output unit a, but instead it represents the selectivity of output unit a to stimulus feature b. For example, suppose b  1 represents the location of a visual stimulus, and b  2 represents its ocularity, the difference in strength between the left-and right-eye inputs. Then, u 1 and u 2 would be the location coordinate and the ocularity of the stimulus, and W a1 and W a2 would be the preferred stimulus location and the preferred eye for neuron a. Map development in such a model is studied by noting how the appearance of various stimulus features and neuronal selectivities affects the matrix W. By associating the index a with cortical location, the structure of the final matrix W that arises from a plasticity rule predicts how selectivities are mapped across the cortex.The activity of a particular output unit in a feature-based model is determined by how closely the stimulus being presented matches its preferred stimulus. The weights W ab for all b values determine the preferred stimulus features for neuron a, and we assume that the activation of neuron a is high if the components of the input u b match the components of W ab . A convenient way to achieve this is to express the activation for unit a as, which has its maximum at u b  W ab for all b, and falls off as a Gaussian function for less perfect matches of the stimulus to the selectivity of the cell. The parameter  b determines how selective the neuron is for characteristic b of the stimulus.The Gaussian expression for the activation of neuron a is not used directly to determine its level of activity. Rather, as in the case of competitive Hebbian learning, we introduce a competitive activity variable for cortical site a,In addition, some cooperative mechanism must be included to keep the maps smooth, which means that nearby neurons should, as far as possible, have similar selectivities. The two algorithms we discuss, the selforganizing map and the elastic net, differ in how they introduce this second element.The self-organizing map spreads the activity defined by equation 8.36 to self-organizing map nearby cortical sites through equation 8.35, v a  a  M aa  z a  . This gives cortical cells a and a  similar selectivities if they are nearby, because v a and v a  affect one another through local recurrent excitation. The elastic net sets the activity of unit a to the result of equation 8.36, v a  z a , which genelastic net erates competition. Smoothness of the map is ensured not by spreading this activity, as in the self-organizing map, but by including an additional term in the plasticity rule that tends to make nearby selectivities the same.Hebbian development of the selectivities characterized by W is generated by an activity-dependent rule. In general, Hebbian plasticity adjusts the weights of activated units so that they become more responsive to, and selective for, input patterns that excite them. Feature-based models achieve the same thing by modifying the selectivities W ab so they more closely feature-based learning rule match the input parameters u b when output unit a is activated by u. For the case of the self-organized map, this is achieved through the averagedThe elastic net modification rule is similar, but an additional term is included to make the maps smooth, because smoothing is not included in the rule that generates the activity in this case. The elastic net plasticity rule is elastic net rule  We previously alluded to the problem of redundancy among multiple output neurons that can arise from feedforward Hebbian modification. The Oja rule of equation 8.16 for multiple output units, which takes the form  One way to reduce redundancy in a linear model is to make the linear recurrent interactions of equation 8.29 plastic rather than fixed, using an anti-Hebbian modification rule. As the name implies, anti-Hebbian plasticity causes synapses to decrease in strength when there is simultaneous pre-and postsynaptic activity. The recurrent interactions arising from an anti-Hebb rule can prevent different output units from representing the same eigenvector. This occurs because the recurrent interactions tend to make output units less correlated by canceling the effects of common feedforward input. Anti-Hebbian modification is believed to be the predominant form of plasticity at synapses from parallel fibers to Purkinje cells in the cerebellum, although this may be a special case because Purkinje cells inhibit rather than excite their targets. A basic anti-Hebb rule for M aa  can be created simply by changing the sign on the right side of equation 8.3. However, just as Hebbian plasticity tends to make weights increase without bound, anti-Hebbian modification tends to make them decrease to 0, and to avoid this it is necessary to useto modify the off-diagonal components of M. Here,  is a positive constant. For suitably chosen  and  M , the combination of rules 8.39 and 8.40 produces a stable configuration in which the rows of the weight matrix W are different eigenvectors of the correlation matrix Q, and all the elements of the recurrent weight matrix M are ultimately set to 0. The minus sign in the term v embodies the anti-Hebbian modification. This term is nonlocal because the change in the weight of a given synapse depends on the total feedforward input to the postsynaptic neuron, not merely on the input at that particular synapse. The term I  M prevents the weights from going to 0 by pushing them toward the identity matrix I. Unlike 8.40, this rule requires the existence of autapses, synapses that a neuron makes onto itself.If the Goodall plasticity rule converges and stops changing M, the right side of equation 8.41 must vanish on average, which requiresMultiplying both sides by K and using equation 8.30, we findThis means that the outputs are decorrelated and also indicates histogram equalization in the sense, discussed in chapter 4, that all the elements of v have the same variance. Indeed, the Goodall algorithm can be used to implement the decorrelation and whitening discussed in chapter 4. Because the anti-Hebb and Goodall rules are based on linear models, they are capable of removing only second-order redundancy, meaning redundancy characterized by the covariance matrix. In chapter 10, we consider models that are based on eliminating higher orders of redundancy as well.Temporal Hebbian rules have been used in the context of multi-unit networks to store information about temporal sequences. To illustrate this, we consider a network with the architecture of figure 8.6. We study the effect of time-dependent synaptic plasticity, as given by equation 8.18, on the recurrent synapses of the model, leaving the feedforward synapses constant.Suppose that before training the average response of output unit a to a stimulus characterized by a parameter s is given by the tuning curve f a, The thick solid curve is the response tuning curve of the neuron that initially had the thin solid tuning curve after a training period involving a time-dependent stimulus. The tuning curve increases in amplitude, asymmetrically broadens, and shifts as a result of temporally asymmetric Hebbian plasticity. The shift shown corresponds to a training stimulus with a positive rate of change, that is, one that moves rightward on this plot as a function of time. The corresponding shift in the tuning curve is to the left. The shift has been calculated using more neurons and tuning curves than are shown in this plot. Location of place field centers while a rat traversed laps around a closed track. Over sequential laps, the place fields shifted backward relative to the direction the rat moved. The effect of this type of modification on the tuning curve in the middle of the array Asymmetric enlargements and backward shifts of neural response tuning curves similar to those predicted from temporally asymmetric LTP and LTD induction have been seen in recordings of hippocampal place cells in rats made by In unsupervised learning, inputs are imposed during a training period and the output is determined by the network dynamics, using the current values of the weights. This means that the network and plasticity rule must uncover patterns and regularities in the input data by themselves. In supervised learning, both a set of inputs and the corresponding desired outputs are imposed during training, so the network is essentially given the answer.Two basic problems addressed in supervised learning are storage, which means learning the relationship between the input and output patterns provided during training, and generalization, which means being able to provide appropriate outputs for inputs that were not presented during training but are similar to those that were. The main tasks we consider within the context of supervised learning are classification of inputs into two categories and function approximation, in which the output of a network unit is trained to approximate a specified function of the input. Understanding generalization in such settings has been a major focus of theoretical investigations in statistics and computer science but lies outside the scope of our discussion.In supervised learning, paired input and output samples are presented during training. We label these pairs by u m and v m for m  1 . . . N S , where the superscript is a label and does not signify either a component of u or a power of v. For a feedforward network, an averaged Hebbian plasticity rule for supervised learning can be obtained from equation 8.4 by averaging across all the input-output pairs,As in the unsupervised Hebbian learning case, the synaptic modification process depends on the input-output cross-correlation vu . However, for cross-correlation supervised learning, the output v  v m is imposed on the network rather than being determined by it.Unless the cross-correlation is 0, equation 8.44 never stops changing the synaptic weights. The methods introduced to stabilize Hebbian modification in the case of unsupervised learning can be applied to supervised learning as well. However, stabilization is easier in the supervised case, because the right side of equation 8.44 does not depend on w. Therefore, the growth is only linear rather than exponential in time, making a simple multiplicative synaptic weight decay term sufficient for stability. This is introduced by writing the supervised learning rule as supervised learning with decayfor some positive constant . Asymptotically, equation 8.45 makes w  vu , that is, the weights become proportional to the input-output crosscorrelation.We discuss supervised Hebbian learning in the case of a single output unit, but the results can be generalized to multiple outputs.The perceptron is a nonlinear map that classifies inputs into one of two perceptron categories. It thus acts as a binary classifier. To make the model consistent binary classifier when units are connected in a network, we also require the inputs to be binary. We can think of the two possible states as representing units that are either active or inactive. As such, we would naturally assign them the values 1 and 0. However, the analysis is simpler if, instead, we require the inputs u a and output v to take the two values 1 and 1.The output of the perceptron is based on a modification of the linear rule of equation 8.2 toThe threshold  determines the dividing line between values of w  u that generate 1 and 1 outputs. The supervised learning task for the perceptron is to place each of N S input patterns u m into one of two classes designated by the desired binary output v m . How well the perceptron performs this task depends on the nature of the classification. The weight vector and threshold define a subspace of dimension N u  1 that cuts the N u -dimensional space of input vectors into two regions. It is possible for a perceptron to classify inputs perfectly only if a hyperplane exists that divides the input space into one half-space containing all the inputs corresponding to v  1, and another half-space containing all those for v  1. This condition is called linear separability. An instructive case linear separability to consider is when each component of each input vector and the associated output values are chosen randomly and independently, with equal probabilities of being 1 and 1. For large N u , the maximum number of random associations that can be described by a perceptron for typical examples of this type is 2N u .For linearly separable inputs, a set of weights exists that allows the perceptron to perform perfectly. However, this does not mean that a Hebbian modification rule can construct such weights. A Hebbian rule based on equation 8.45 with   N u N S constructs the weight vectorTo see how well such weights allow the perceptron to perform, we compute the output generated by one input vector, u n , chosen from the training set. For this example, we set   0. Nonzero threshold values are considered later in the chapter.With   0, the value of v for input u n is determined solely by the sign of w  u n . Using the weights of equation 8.47, we findIf we set m n v m u m  u n N u   n, then v n u n  u n N u  v n because 1 2  2  1, so we can writeSubstituting this expression into equation 8.46 to determine the output of the perceptron for the input u n , we see that the term  n acts as a source of noise, interfering with the ability of the perceptron to generate the correct answer v  v n .We can think of  n as a sample from a probability distribution over values of . are chosen randomly with equal probabilities of being 1 or 1. Including the dot product, the right side of the expression N u  n  m n v m u m  u n defining  n is the sum ofN u terms, each of which is equally likely to be 1 or 1. For large N u and N S , the central limit theorem implies that the distribution of  values is Gaussian with mean 0 and varianceN u . This suggests that the perceptron with Hebbian weights should work well if the number of input patterns being learned is significantly less than the number of input vector components. We can make this more precise by noting from equations 8.46 with   0 and equation 8.49 that, for v n  1, the perceptron will give the correct answer if 1   n  . Similarly, for v n  1, the perceptron will give the correct answer if    n  1. If v n has probability 12 of taking either value, the probability of the perceptron giving the correct answer is 12 times the integral of the Gaussian distribution from 1 to  plus 12 times its integral from  to 1. Combining these two terms, we findThis result is plotted in In chapter 1, we studied examples in which the firing rate of a neuron was given by a function of a stimulus parameter, namely, the response tuning curve. When such a relationship exists, we can think of the neuronal firing rate as representing the function. Populations of neurons that respond to a stimulus value s, by firing at average rates f b, can similarly represent an entire set of functions. However, a function h that is not equal to any of the single neuron tuning curves must be represented by combining the responses of a number of units. This can be done using the network shown in figure 8.13. The average steady-state activity level of the output unit in this network, in response to stimulus value s, is given by equationNote that we have replaced u with f where f is the vector with components f b. The network presented in chapter 7 that performs coordinate transformation is an example of this type of function approximator.In equation 8.51, the input tuning curves f act as a basis for representing the output function h, and for this reason they are called basis functions. Different sets of basis functions can be used to represent a given basis functions set of output functions. A set of basis functions that can represent any member of a class of functions using a linear sum, as in equation 8.51, is called complete for this class. For the basis sets typically used in mathcompleteness ematics, such as the sines and cosines in a Fourier series, the weights in equation 8.51 are unique. When neural tuning curves are used to expand a function, the weights tend not to be unique, and the set of input functions is called overcomplete. In this chapter, we assume that the basis functions overcomplete are held fixed, and only the weights are adjusted to improve output performance. It is also interesting to consider methods for learning the best basis functions for a particular application. One way of doing this is by applying an algorithm called backpropagation, which develops the basis functions guided by the output errors of the network. Other methods, which we consider in chapter 10, involve unsupervised learning.Suppose that the function-representation network of figure 8.13 is pro-vided with a sequence of N S sample stimuli, s m for m  1, 2, . . . , N S , and the corresponding function values h, during a training period. To make v match h as closely as possible for all m, we minimize the errorWe have made the replacement v  w  f in this equation and have used the bracket notation for the average over the training samples. Equations for the weights that minimize this error, called the normal equations, normal equations are obtained by setting its derivative with respect to the weights to 0, yielding the conditionThe supervised Hebbian rule of equation 8.45, applied in this case, ultimately sets the weight vector to w  fh . These weights must satisfy the normal equations 8.53 if they are to optimize function approximation. There are two circumstances under which this occurs. The obvious one is when the input units are orthogonal across the training stimuli, ff  I. In this case, the normal equations are satisfied with   1. However, this condition is unlikely to hold for most sets of input tuning curves. An alternative possibility is that for all pairs of stimuli s m and s n in the training set,for some constant c. This is called a tight frame condition. If it is satisfied, tight frame the weights given by supervised Hebbian learning with decay can satisfy the normal equations. To see this, we insert the weights w  fh  into equation 8.53 and use 8.54 to obtain An essential limitation of supervised Hebbian rules is that synaptic modification does not depend on the actual performance of the network. An alternative learning strategy is to start with an initial guess for the weights, compare the output v in response to input u m with the desired output v m , and change the weights to improve the performance. Two important error-correcting modification rules are the perceptron rule, which applies to binary classification, and the delta rule, which can be applied to function approximation and many other problems.Suppose that the perceptron of equation To verify that the perceptron learning rule makes appropriate weight adjustments, we note that it implies that To learn a set of input pattern classifications, the perceptron learning rule is applied to each one repeatedly, either sequentially or in a random order. For fixed  w , the perceptron learning rule of equation 8.56 is guaranteed to find a set of weights w and threshold  that solve any linearly separable problem. This is proved in the appendix.The function approximation task with the error function E of equation 8.52 can be solved using an error-correcting scheme similar in spirit to the perceptron learning rule, but designed for continuous rather than binary outputs. A simple but extremely useful version of this is the gradient descent procedure, which modifies w according to gradient descent If  w is too large or w is very near to a point where  w E  0 0 0, E can increase instead. We assume that  w is small enough so that E decreases at least until w is very close to a minimum. If E has many minima, gradient descent will lead to only one of them, and not necessarily the one with the lowest value of E. In the case of function approximation using basis functions as in equation 8.51, gradient descent finds a value of w that satisfies the normal equations, and therefore constructs an optimal function approximator, because the error function of equation 8.52 has only one minimum.For function approximation, the error E in equation 8.52 is a sum over the set of examples. As a result,  w E also involves a sum,  The output firing rate after weight modification using the delta rule with 20 sample points. The output firing rate after weight modification using the delta rule with 100 sample points.Gaussian tuning curves drives an output neuron so that it quite accurately represents a sine function. In the next section, we discuss a particular example of this in the case of the Boltzmann machine, and we show how learning rules intended for supervised learning can sometimes be used for unsupervised learning as well.In chapter 7, we presented the Boltzmann machine, a stochastic network with binary units. One of the key innovations associated with the Boltzmann machine is a synaptic modification rule that has a sound foundation in probability theory. We start by describing the case of supervised learning, although the underlying theory is similar for both supervised and unsupervised cases with the Boltzmann machine.Recall from chapter 7 that the Boltzmann machine produces a stochastic output v from an input u through a process called Gibbs sampling. This has two main consequences. First, rather than being described by an equation such as 8.2, the input-output relationship of a stochastic network is described by a distribution Pvu W, which is the probability that input u generates output v when the weight matrix of the network is W. Second, supervised learning in a deterministic network involves the development of an input-output relationship that matches, as closely as possible, a set of samples for m  1, 2, . . . , N S . Such a task does not make sense for a model that has a stochastic rather than deterministic relationship between its input and output activities. Instead, stochastic networks are appropriate for representing statistical aspects of the relationship between two sets of variables. For example, suppose that we drew sample pairs from a joint probability distribution P A natural supervised learning task for a stochastic network is to make the input-output distribution of the network, Pvu W, match as closely as possible, the probability distribution P . This is the Gibbs sampling procedure discussed in chapter 7 applied to the feedforward Boltzmann machine. Because there are no recurrent connections, the states of the output units are independent, and they can all be sampled simultaneously. Analogous to the discussion in chapter 7, this procedure gives rise to a conditional probability distribution Pvu W for v given u that can be written as likelihood maximization A learning rule that performs gradient ascent of the log likelihood can be derived by changing the weights by an amount proportional to the derivative of the logarithmic term in equation 8.64 with respect to the weights.As in the discussion of the delta rule, it is more convenient to use a stochastic gradient ascent rule, choosing an index m at random to provide a Monte Carlo sample from the average of equation 8.64, and changing W ab according to the derivative with respect to this sample,  Supervised learning can also be implemented in a Boltzmann machine with recurrent connections. When the output units are connected by a symmetric recurrent weight matrix M, the energy function is for the probability that v a  1. Repeated sampling is required to assure that the network relaxes to the equilibrium distribution of equation The Boltzmann machine was originally introduced in the context of unsupervised rather than supervised learning. In the supervised case, we try to make the distribution Pvu W match the probability distribution P We consider the unsupervised Boltzmann machine without recurrent connections. In addition to the distribution of equation 8.62 for v, given a specific input u, the energy function of the Boltzmann machine can be used to define a distribution over both u and v defined byThis can be used to construct a distribution for u alone by summing over the possible values of v,The goal of unsupervised learning for the Boltzmann machine is to make this distribution match, as closely as possible, the distribution of inputs P The unsupervised learning rule can be extended to include recurrent connections by following the same general procedure.We presented a variety of forms of Hebbian synaptic plasticity, ranging from the basic Hebb rule to rules that involve multiplicative and subtractive normalization, constant or sliding thresholds, and spike-timing effects. Two important features, stability and competition, were emphasized. We showed how the effects of unsupervised Hebbian learning could be estimated by computing the principal eigenvector of the correlation matrix of the inputs used during training. Unsupervised Hebbian learning can be interpreted as a process that produces weights that project the input vector onto the direction of maximal variance in the training data set. In some cases, this requires an extension from correlation-based to covariance-based rules. We used the principal eigenvector approach to analyze Hebbian models of the development of ocular dominance and its associated map in primary visual cortex. Plasticity rules based on the dependence of synaptic modification on spike timing were shown to implement temporal sequence and trace learning.Forcing multiple outputs to have different selectivities requires them to be connected, either through fixed weights or by weights that are themselves plastic. In the latter case, anti-Hebbian plasticity can ensure decorrelation of multiple output units. We also considered the role of competition and cooperation in models of activity-dependent development, and described two examples of feature-based models, the self-organizing map and the elastic net.Finally, we considered supervised learning applied to binary classification and function approximation, using supervised Hebbian learning, the perceptron learning rule, and gradient descent learning through the delta rule. We also treated contrastive Hebbian learning for the Boltzmann machine, involving Hebbian and anti-Hebbian updates in different phases.For convenience, we take  w  1 and start the perceptron learning rule with w  0 0 0 and   0. Then, under presentation of the sample m, the changes in the weights and threshold are given by and allow the perceptron to categorize correctly, for which we require the conditionv m   for some   0 and for all m.Consider the cosine of the angle between the current weights and threshold w,  and the solution w  ,    w  w     w 2  2   w,  , To show this, we consider the change in  due to one step of perceptron learning during which w and  are modified because the current weights generated the wrong response. When an incorrect response is generated,The inequality follows from the condition imposed on w  and   as providing a solution of the categorization problem. Assuming that  is initially positive and iterating this result over n steps in which the weights change, we find thatSimilarly, over one learning step in which some change is made,The first term on the right side is always negative when an error is made, and if we define D to be the maximum value of u m  2 over all the training samples, we findAfter n nontrivial learning iterations starting from w,  2  0, we therefore havePutting together equations 8.76 and 8.79, we find, after n nontrivial training steps,To ensure that  1, we must have n  2 . Therefore, after a finite number of weight changes, the perceptron learning rule must stop changing the weights, and the perceptron must classify all the patterns correctly.Hebbs original proposal about learning set the stage for many of the subsequent investigations. We followed the treatments of Hebbian, BCM, anti-Hebbian, and trace learning found in . We followed Descriptions of relevant data on the patterns of responsivity across cortical areas and the development of these patterns include We described Hebbian models for the development of ocular dominance and orientation selectivity due to The capacity of a perceptron for random associations is computed in The ability of animals to learn appropriate actions in response to particular stimuli on the basis of associated rewards or punishments is a focus of behavioral psychology. The field is traditionally separated into classical and instrumental conditioning. In classical conclassical and instrumental conditioning ditioning, the reinforcers are delivered independently of any actions taken by the animal. In instrumental conditioning, the actions of the animal determine what reinforcement is provided. Learning about stimuli or actions solely on the basis of the rewards and punishments associated with them is called reinforcement learning. Reinforcement learning is minimally supervised because animals are not reinforcement learning told explicitly what actions to take in particular situations, but must work this out for themselves on the basis of the reinforcement they receive.We begin this chapter with a discussion of aspects of classical conditioning and the models that have been developed to account for them. We first discuss various pairings of one or more stimuli with presentation or denial of a reward, and present a simple learning algorithm that summarizes the results. We then present an algorithm, called temporal difference learning, that leads to predictions of both the presence and the timing of rewards delivered after a delay following stimulus presentation. Two neural systems, the cerebellum and the midbrain dopamine system, have been particularly well studied from the perspective of conditioning. The cerebellum has been studied in association with eyeblink conditioning, a paradigm in which animals learn to shut their eyes just in advance of disturbances, such as puffs of air, that are signaled by cues. The midbrain dopaminergic system has been studied in association with reward learning. We focus on the latter, together with a small fraction of the extensive behavioral data on conditioning.There are two broad classes of instrumental conditioning tasks. In the first class, which we illustrate with an example of foraging by bees, the reinforcement is delivered immediately after the action is taken. This makes learning relatively easy. In the second class, the reward or punishment depends on an entire sequence of actions and is partly or wholly delayed until the sequence is completed. Thus, learning the appropriate action at delayed rewards each step in the sequence must be based on future expectation, rather than immediate receipt, of reward. This makes learning more difficult. Despite the differences between classical and instrumental conditioning, we show how to use the temporal difference model we discuss for classical conditioning as the heart of a model of instrumental conditioning when rewards are delayed.For consistency with the literature on reinforcement learning, throughout this chapter, the letter r is used to represent a reward rather than a firing rate. Also, for convenience, we consider discrete actions such as a choice between two alternatives, rather than a continuous range of actions. We also consider trials that consist of a number of discrete events and use an integer time variable t  0, 1, 2, . . . to indicate steps during a trial. We therefore also use discrete weight update rules rather than learning rules described by differential equations.Classical conditioning involves a wide range of different training and testing procedures and a rich set of behavioral phenomena. The basic procedures and results we discuss are summarized in table 9.1. Rather than going through the entries in the table at this point, we introduce a learning algorithm that serves to summarize and structure these results.In the classic Pavlovian experiment, dogs are repeatedly fed just after a bell is rung. Subsequently, the dogs salivate whenever the bell sounds, as unconditioned stimulus and response if they expect food to arrive. The food is called the unconditioned stimulus. Dogs naturally salivate when they receive food, and salivation is thus called the unconditioned response. The bell is called the conditioned stimulus because it elicits salivation only under the condition that there has conditioned stimulus and response been prior learning. The learned salivary response to the bell is called the conditioned response. We do not use this terminology in the following discussion. Instead, we treat those aspects of the conditioned responses that mark the animals expectation of the delivery of reward, and build models of how these expectations are learned. We therefore refer to stimuli, rewards, and expectation of reward.The Rescorla-Wagner rule . The expected reward, denoted by v, is stimulus u expected reward v expressed as this stimulus variable multiplied by a weight w, weight w v  wu .The value of the weight, w, is established by a learning rule designed to minimize the expected squared error between the actual reward r and the prediction v, 2 . The angle brackets indicate an average over the presentations of the stimulus and reward, either or both of which may be stochastic. As we saw in chapter 8, stochastic gradient descent in the form of the delta rule is one way of minimizing this error. This results in the trial-by-trial learning rule known as the Rescorla-Wagner rule, Rescorla-Wagner rule w  w  u with   r  v .Here  is the learning rate, which can be interpreted in psychological terms associability as the associability of the stimulus with the reward. The crucial term in this learning rule is the prediction error, . In a later section, we interpret the activity of dopaminergic cells in the ventral tegmental area as encoding a form of this prediction error. If  is sufficiently small and u  1 on every trial, the rule ultimately makes w fluctuate about the equilibrium value w  r , at which point the average value of  is 0.The filled circles in figure 9.1 show how learning progresses according to acquisition the Rescorla-Wagner rule during the acquisition and extinction phases of Pavlovian conditioning. In this example, the stimulus and reward were extinction both initially presented on each trial, but later the reward was removed. The weight approaches the asymptotic limit w  r exponentially during the rewarded phase of training, and exponentially decays to w  0 during the unrewarded phase. Experimental learning curves are generally more sigmoidal in shape. There are various ways to account for this discrepancy, the simplest of which is to assume a nonlinear relationship between the expectation v and the behavior of the animal.The Rescorla-Wagner rule also accounts for aspects of the phenomenon of partial reinforcement, in which a reward is associated with a stimulus partial reinforcement only on a random fraction of trials. Behavioral measures of the ultimate association of the reward with the stimulus in these cases indicate that it is weaker than when the reward is always presented. This is expected from the delta rule, because the ultimate steady-state average value of w  r is smaller than r in this case. The open squares in figure 9.1 show what happens to the weight when the reward is associated with the stimulus 50% of the time. After an initial rise from 0, the weight varies randomly around an average value of 0.5.To account for experiments in which more than one stimulus is used in association with a reward, the Rescorla-Wagner rule must be extended to include multiple stimuli. This is done by introducing a vector of binary variables u, with each of its components representing the presence or abstimulus vector u sence of a given stimulus, together with a vector of weights w. The exweight vector w pected reward is then the sum of each stimulus parameter multiplied by its corresponding weight, written compactly as a dot product,Minimizing the prediction error by stochastic gradient descent in this case gives the delta learning rule, delta rule w  w  u with   r  v .Various classical conditioning experiments probe the way that predictions are shared between multiple stimuli A standard way to induce inhibitory conditioning is to use trials in which inhibitory conditioning one stimulus is shown in conjunction with the reward in alternation with trials in which that stimulus and an additional stimulus are presented in the absence of reward. In this case, the second stimulus becomes a conditioned inhibitor, predicting the absence of reward. This can be demonstrated by presenting a third stimulus that also predicts reward, in conjunction with the inhibitory stimulus, and showing that the net prediction of reward is reduced. It can also be demonstrated by showing that subsequent learning of a positive association between the inhibitory stimulus and reward is slowed. Inhibition emerges naturally from the delta rule. Trials in which the first stimulus is associated with a reward result in a positive value of w 1 . Over trials in which both stimuli are presented together, the net prediction v  w 1  w 2 comes to be 0, so w 2 is forced to be negative.A further example of the interaction between stimuli is overshadowing. If two stimuli are presented together during training, the prediction overshadowing of reward is shared between them. After application of the delta rule, v  w 1  w 2  r. However, the prediction is often shared unequally, as if one stimulus is more salient than the other. Overshadowing can be encompassed by generalizing the delta rule so that the two stimuli have different learning rates, reflecting unequal associabilities. Weight modification stops when   0, at which point the faster growing weight will be larger than the slower growing weight. Various, more subtle effects come from having not only different but also modifiable learning rates, but these lie beyond the scope of our account.The Rescorla-Wagner rule, binary stimulus parameters, and linear reward prediction are obviously gross simplifications of animal learning behavior. Yet they summarize and unify an impressive amount of classical conditioning data and are useful, provided their shortcomings are fully appreciated. As a reminder of this, we point out one experiment, secondary conditioning, that cannot be encompassed within this scheme.Secondary conditioning involves the association of one stimulus with a resecondary conditioning ward, followed by an association of a second stimulus with the first stimulus. This causes the second stimulus to evoke expectation of a reward with which it has never been paired. The delta rule cannot account for the positive expectation associated with the second stimulus. Indeed, because the reward does not appear when the second stimulus is presented, the delta rule would cause w 2 to become negative. In other words, in this case, the delta rule would predict inhibitory, not excitatory, secondary conditioning. Secondary conditioning is related to the problem of delayed rewards in instrumental conditioning that we discuss later in this chapter.Secondary conditioning raises the important issue of keeping track of the time within a trial in which stimuli and rewards are present. This is evident because a positive association with the second stimulus is reliably established only if it precedes the first stimulus in the trials in which they are paired. If the two stimuli are presented simultaneously, the result may be inhibitory rather than secondary conditioning.We measure time within a trial using a discrete time variable t, which falls in the range 0  t  T. The stimulus u, the prediction v, and the reward r are all expressed as functions of t.In addition to associating stimuli with rewards and punishments, animals can learn to predict the future time within a trial at which reinforcement will be delivered. We might therefore be tempted to interpret v as the reward predicted to be delivered at time step t. However, The brackets denote an average over trials. This quantity is useful for optimization, because it summarizes the total expected worth of the current state. To approximate v, we generalize the linear relationship used for classical conditioning, equation 9.3. For the case of a single timedependent stimulus u, we writeThis is just a discrete time version of the sort of linear filter used in chapters 1 and 2.Arranging for v to predict the total future reward would appear to require a simple modification of the delta rule we have discussed previously, w  w  u , with  being the difference between the actual and predicted total future reward,    r  v. However, there is a problem with applying this rule in a stochastic gradient descent algorithm. Computation of  requires knowledge of the total future reward on a given trial. Although r is known at time t, the succeeding r, r . . . have yet to be experienced, making it impossible to calculate . A possible solution is suggested by the recursive formulaThe temporal difference model of prediction is based on the observation that v provides an approximation of the average value of the last term in equation 9.8, so we can writeReplacing the sum in the equation    r  v by this approximation gives the temporal difference learning rule, temporal difference rule w  w  u with   r  v  v . The name of the rule comes from the term v  v, which is the difference between two successive estimates.  is usually called the temporal difference error. Under a variety of circumstances, this rule is likely to converge to make the correct predictions. As the peak in  moves backward from the time of the reward to the time of the stimulus, weights w for   100, 99, . . . successively grow. This gradually extends the prediction of future reward, v, from an initial transient at the time of the reward to a broad plateau extending from the time of the stimulus to the time of the reward. Eventually, v predicts the correct total future reward from the time of the stimulus onward, and predicts the time of the reward delivery by dropping to 0 at the time when the reward is delivered. The exact shape of the ridge of activity that moves from t  200 to t  100 over the course of trials in figure 9.2A is sensitive to a number of factors, including the learning rate, and the form of the linear filter of equation 9.6.Unlike the delta rule, the temporal difference rule provides an account of secondary conditioning. Suppose an association between stimulus s 1 and a future reward has been established, as in The prediction error  plays an essential role in both the Rescorla-Wagner and temporal difference learning rules, and we might hope to find a neural signal that represents this quantity. One suggestion is that the activity of dopaminergic neurons in the ventral tegmental area in the midbrain ventral tegmental area VTA plays this role.There is substantial evidence that dopamine is involved in reward learndopamine ing. Drugs of addiction, such as cocaine and amphetamines, act partly by increasing the longevity of the dopamine that is released onto target structures such as the nucleus accumbens. Other drugs, such as morphine and heroin, also affect the dopamine system. Further, dopamine delivery is important in self-stimulation experiments. Rats will compulsively press levers that cause current to be delivered through electrodes into various areas of their brains. One of the most effective self-stimulation sites is the medial forebrain ascending bundle, which is an axonal pathway. Stimulating this pathway is likely to cause increased delivery of dopamine to the nucleus accumbens and other areas of the brain, because the bundle contains many fibers from dopaminergic cells in the VTA projecting to the nucleus accumbens.In a series of studies by Schultz and his colleagues The similarity between the responses of the dopaminergic neurons and the quantity  suggests that their activity provides a prediction error for reward, i.e. an ongoing difference between the amount of reward that is delivered and the amount that is expected. Something similar to the temporal difference learning rule could be realized in a neural system if the dopamine signal representing  acts to gate and regulate the plasticity associated with learning. We discuss this possibility further in a later section.In classical conditioning experiments, rewards are directly associated with stimuli. In more natural settings, rewards and punishments are associated with the actions an animal takes. Animals develop policies, or plans of policy action, that increase reward. In studying how this might be done, we consider two different cases. In static action choice, the reward or punishment immediately follows the action taken. In sequential action choice, rewards may be delayed until several actions are completed.As an example of static action choice, we consider bees foraging among flowers in search of nectar. We model an experiment in which single bees forage under controlled conditions among blue and yellow artificial flowforaging ers. In actual experiments, the bees learn within a single session about the reward characteristics of the blue and yellow flowers. All else being equal, they preferentially land on the color of flower that delivers more reward. This preference is maintained over multiple sessions. However, if the reward characteristics of the flowers are interchanged, the bees quickly swap their preferences.We treat a simplified version of the problem, ignoring the spatial aspects of sampling, and assuming that a model bee is faced with repeated choices between two different flowers. If the bee chooses the blue flower on a trial, it receives a quantity of nectar r b drawn from a probability density pr b . If it chooses the yellow flower, it receives a quantity r y , drawn from a probability density pr y . The task of choosing between the flowers is a form of stochastic two-armed bandit problem, two-armed bandit and is formally equivalent to many instrumental conditioning tasks.The model bee has a stochastic policy, which means that it chooses blue stochastic policy and yellow flowers with probabilities that we write as P There are only two possible actions in the example we study, but the extension to multiple actions, a  1, 2, . . . , N a , is straightforward. In this case, a vector m of parameters controls the decision process, and the probability action value vector m Pa of choosing action a isWe consider two simple methods of solving the bee foraging task. In the first method, called the indirect actor, the bee learns to estimate the expected nectar volumes provided by each flower by using a delta rule. It then bases its action choice on these estimates. In the second method, called the direct actor, the choice of actions is based directly on maximizing the expected average reward.. Between trials 15 and 16, the delivery characteristics of the flowers were swapped. To apply the foraging model we have been discussing to the experiment shown in Furthermore, we can include an arbitrary parameter r in both these terms, because it cancels out. Thus, In stochastic gradient ascent, the changes in the parameter m b are determined such that, averaged over trials, they end up proportional to  r m b . We can derive a stochastic gradient ascent rule for m b from equation 9.19 in two steps. First, we interpret the two terms on the right side as changes associated with the choice of blue and yellow flowers. This accounts for the factors P the average change in m b is proportional to  r m b . Note that m b is changed even when the bee chooses the yellow flower. We can summarize this learning rule as The learning rule of equations 9.20 and 9.21 performs stochastic gradient ascent on the average reward, whatever the value ofr. Different values ofr lead to different variances of the stochastic gradient terms, and thus different speeds of learning. A reasonable value forr is the mean reward under the specified policy or some estimate of this quantity. The direct actor learning rule can be extended to multiple actions, a  1, 2, . . . , N a , by using the multidimensional form of the softmax distribution. In this case, when action a is taken, m a  for all values of a  is updated according to In the previous section, we considered ways that animals might learn to choose actions on the basis of immediate information about the consequences of those actions. A significant complication that arises when reward is based on a sequence of actions is illustrated by the maze task shown in figure 9.7. In this example, a hungry rat has to move through a maze, starting from point A, without retracing its steps. When it reaches one of the shaded boxes, it receives the associated number of food pellets and is removed from the maze. The rat then starts again at A. The task is to optimize the total reward, which in this case entails moving left at A and right at B. It is assumed that the animal starts knowing nothing about the structure of the maze or about the rewards.If the rat started from point B or point C, it could learn to move right or left, using the methods of the previous section, because it experiences an immediate consequence of its actions in the delivery or nondelivery of food. The difficulty arises because neither action at the actual starting point, A, leads directly to a reward. For example, if the rat goes left at A and also goes left at B, it has to figure out that the former choice was good but the latter was bad. This is a typical problem in tasks that involve delayed rewards. The reward for going left at A is delayed until after the rat also goes right at B.There is an extensive body of theory in engineering, called dynamic programming, as to how systems of any sort can come to select appropriate dynamic programming actions in optimizing control problems similar to the maze task. An important method on which we focus is called policy iteration. Our reinforcement learning version of policy policy iteration iteration maintains and improves a stochastic policy, which determines the actions at each decision point through action values and the softmax distribution of equation 9.12. Policy iteration involves two elements. One, called the critic, uses temporal difference critic learning to estimate the total future reward that is expected when starting from A, B, or C, when the current policy is followed. The other element, called the actor, maintains and improves the policy. Adjustment of the actor action values at point A is based on predictions of the expected future rewards associated with points B and C that are provided by the critic. In effect, the rat learns the appropriate action at A, using the same methods of static action choice that allow it to learn the appropriate actions at B and C. However, rather than using an immediate reward as the reinforcement signal, it uses the expectations about future reward that are provided by the critic.As we mentioned when discussing the direct actor, a stochastic policy is a way of assigning a probability distribution over actions to each location. The location is specified by a variable u that takes the values A, B, or C, and a twocomponent action value vector m is associated with each location. The components of the action vector m control the probability of taking a left or a right turn at u through the softmax distribution of equation 9.12.The immediate reward provided when action a is taken at location u is written as r a. For the maze of figure 9.7, this takes values 0, 2, or 5, depending on the values of u and a. The predicted future reward expected at location u is given by v  w. This is an estimate of the total award that the rat expects to receive, on average, if it starts at the point u and follows its current policy through to the end of the maze. The average is taken over the stochastic choices of actions specified by the policy. In this case, the expected reward is simply equal to the weight. The learning procedure consists of two separate steps: policy evaluation, in which w is adjusted to improve the predictions of future reward, and policy improvement, in which m is adjusted to increase the total reward. and uses temporal difference learning to determine the expected total future reward starting from each location. Suppose that, initially, the rat has no preference for turning left or right, that is, m  0 for all u, so the probability of left and right turns is 12 at all locations. By inspection of the possible places the rat can go, we find that the values of the states areThese values are the average total future rewards that will be received during exploration of the maze when actions are chosen using the random policy. The temporal difference learning rule of equation 9.10 can be used to learn them. If the rat chooses action a at location u and ends up at location u  , the temporal difference rule modifies the weight w by critic learning rule In our earlier description of temporal difference learning, we included the possibility that the reward delivery might be stochastic. Here, that stochasticity is the result of a policy that makes use of the information provided by the critic. In the appendix, we discuss a Monte Carlo interpretation of the terms in the temporal difference learning rule that justifies its use. In policy improvement, the expected total future rewards at the different locations are used as surrogate immediate rewards. Suppose the rat is about to take action a at location u and move to location u  . The expected worth to the rat of that action is the sum of the actual reward received and the rewards that are expected to follow, which is r a  v. For simplicity, we assume that the rat receives the reward for location u at the same time it decides to move on to location u  . The direct actor scheme of equation 9.22 uses the difference r a r between a sample of the worth of the action and a reinforcement comparison term, which might be the average value over all the actions that can be taken. Policy improvement uses r a  v as the equivalent of the sampled worth of the action, and v as the average value across all actions that can be taken at u. The difference between these is   r a  v  v, which is exactly the same term as in policy evaluation. The policy improvement or actor learning rule is then actor learning rulefor all a  , where Pa   u is the probability of taking action a  at location u given by the softmax distribution of equation 9.12 with action value m a .To look at this more concretely, consider the temporal difference error starting from location u A, using the true values of the locations given by equation 9.23. Depending on the action,  takes the two valuesThe learning rule of equation 9.25 increases the probability that the action with   0 is taken and decreases the probability that the action with   0 is taken. This increases the chance that the rat makes the correct turn at A in the maze of figure 9.7.As the policy changes, the values, and therefore the temporal difference terms, change as well. However, because the values of all locations can increase only if we choose better actions at those locations, this form of policy improvement inevitably leads to higher values and better actions. This monotonic improvement of the expected future rewards at all locations is proved formally in the dynamic programming theory of policy iteration for a class of problems called Markov decision Markov decision problems problems, as discussed in the appendix.Strictly speaking, policy evaluation should be complete before a policy is improved. It is also most straightforward to improve the policy completely before it is re-evaluated. A convenient alternative is to interleave partial policy evaluation and policy improvement steps. This is called the actor-critic algorithm. The full actor-critic model for solving sequential action tasks includes three generalizations of the maze learner that we have presented. The first involves additional information that may be available at the different locations. If, for example, sensory information is available at a location u, we associate a state vector u with that location. The vector u state vector u parameterizes whatever information is available at location u that might help the animal decide which action to take. For example, the state vector might represent a scent of food that the rat might detect in the maze task. When a state vector is available, v, which is the value at location u, can depend on it. The simplest dependence is provided by the linear form v  w  u, similar to the input-output relationship used in linear feedforward network models. The learning rule for the critic is then generalized to include the information provided by the state vector, . unary representation We must also modify the actor learning rule to make use of the information provided by the state vector. This is done by generalizing the action value vector m to a matrix M, called an action matrix. M has as many columns action matrix M as there are components of u and as many rows as there are actions. Given input u, action a is chosen at location u with the softmax probability of equation 9.12, but using component a of the action value vector, for all a  , with  given again as in equation 9.24. This is called a three-term covariance learning rule.We can speculate about the biophysical significance of the three-term covariance rule by interpreting  aa  as the output of cell a  when action a is chosen and interpreting u as the input to that cell. Compared with the Hebbian covariance rules studied in chapter 8, learning is gated by a third term, the reinforcement signal . It has been suggested that the dorsal striatum, which is part of the basal ganglia, dorsal striatum is involved in the selection and sequencing of actions. Terminals of axons basal ganglia projecting from the substantia nigra pars compacta release dopamine onto synapses within the striatum, suggesting that they might play such a gating role. The activity of these dopamine neurons is similar to that of the VTA neurons discussed previously as a possible substrate for .The second generalization is to the case that rewards and punishments received soon after an action are more important than rewards and punishments received later. One natural way to accommodate this is a technique called exponential discounting. In computing the expected future reward, discounting this amounts to multiplying a reward that will be received  time steps after a given action by a factor   , where 0    1 is the discounting factor. The smaller , the stronger the effect. Discounting has a major influence on the optimal behavior in problems for which there are many steps to a goal. Exponential discounting can be accommodated within the temporal difference framework by changing the prediction error  to   r a  v  v ,  is the state vector used at time step t of a trial. Such generalized temporal difference learning can be achieved by computing new state vectors, defined by the recursive relatio As an example of generalized reinforcement learning, we consider the water maze task. This is a navigation problem in which rats are placed in a large pool of milky water and have to swim around until they find a small platform that is submerged slightly below the surface of the water. The opaqueness of the water prevents them from seeing the platform directly, and their natural aversion to water motivates them to find the platform. After several trials, the rats learn the location of the platform and swim directly to it when placed in the water. The action with the highest probability for each location in the maze. Lower path plots: Actual paths taken by the model rat from random starting points to the platform, indicated by a small circle. A slight modification of the actor learning rule was used to enforce generalization between spatially similar actions. We discussed reinforcement learning models for classical and instrumental conditioning, interpreting the former in terms of learning predictions about total future rewards and the latter in terms of optimization of those rewards. We introduced the Rescorla-Wagner or delta learning rule for classical conditioning, together with its temporal difference extension, and indirect and direct actor rules for instrumental conditioning given immediate rewards. Finally, we presented the actor-critic version of the dynamic programming technique of policy iteration, evaluating policies using temporal difference learning and improving them using the direct actor learning rule, based on surrogate immediate rewards from the evaluation step.In the appendix, we show more precisely how temporal difference learning can be seen as a Monte Carlo technique for performing policy iteration.  The task for a system or animal facing a Markov decision problem, starting in state u at time 0, is to choose a policy, denoted by M, that maximizes the expected total future reward,where u  u, actions a are determined on the basis of the state u according to policy M, and the notation u,M implies taking an expectation over the actions and the states to which they lead, starting at state u and using policy M. , but, by influencing the state of the sys-tem, also the subsequent rewards. It would seem that the animal would have to consider optimizing whole sequences of actions, the number of which grows exponentially with time. This says that maximizing reward at u requires choosing the action a that maximizes the sum of the mean immediate reward r a and the average of the largest possible values of all the states u  to which a can lead the system, weighted by their probabilities.The actor-critic algorithm is a form of a dynamic programming technique called policy iteration. Policy iteration involves interleaved steps of policy evaluation and policy improvement. Evaluation of policy M requires working out the values for all states u. We call these values v M, to reflect explicitly their dependence on the policy. Each value is analogous to the quantity in 9.5. Using the same argument that led to the Bellman equation, we can derive the recursive formula is used as a sampled approximation to the discrepancy v M  v, which is an appropriate error measure for training v to equal v M. Note that there is an interaction between the stochasticity in the reinforcement learning versions of policy evaluation and policy improvement. This means that it is not known whether the two together are guaranteed to converge. One could perform temporal difference policy evaluation until convergence before attempting policy improvement, and this would be sure to work.Dickinson, Our description of the temporal difference model of classical conditioning in this chapter is based on Sutton and Sutton  Barto. The treatment of static action choice comes from Narendra  Thatachar The kernel representation of the weight between a stimulus and reward can be seen as a form of a serial compound stimulus The response selectivities of individual neurons, and the way they are distributed across neuronal populations, define how sensory information is represented by neural activity. Sensory information is typically represented in multiple brain regions, the visual system being a prime example, with the nature of the representation shifting progressively along the sensory pathway. In previous chapters, we discussed how such representations can be generated by neural circuitry and developed by activitydependent plasticity. In this chapter, we study neural representations from a computational perspective, asking what goals are served by particular representations, and how appropriate representations might be developed on the basis of input statistics.Constructing new representations of, or re-representing, sensory input is re-representation important because sensory receptors often deliver information in a form that is unsuitable for higher-level cognitive tasks. For example, roughly 10 8 photoreceptors provide a pixelated description of the images that appear on our retinas. A list of the membrane potentials of each of these photoreceptors provides a bulky and awkward representation of the visual world, from which it would be difficult to identify directly the face of a friend or a familiar object. Instead, the information provided by photoreceptor outputs is processed in a series of stages involving increasingly sophisticated representations of the visual world. In this chapter, we consider how these more complex and useful representations can be constructed.The key to building useful representations for vision lies in determining the structure of visual images and capturing the constraints imposed on images by the natural world. The set of possible pixelated activities arising from natural scenes is richly structured and highly constrained, because images are not generated randomly, but arise from well-defined objects, such as rocks, trees, and people. We call these objects the "causes" of the images. In representational learning, we seek to identify causes by analyzing the statistical structure of visual images and building a model, called the generative model, that is able to reproduce this structure. Idengenerative model tification of the causes of particular images is performed by a second model, called the recognition model, that is conrecognition model structed on the basis of the generative model. This procedure is analogous to analyzing an experiment by building a model of the processes thought to underly it, and using the model as a basis for extracting interesting features from the accumulated experimental data.We follow the convention of previous chapters and use the variables u and v to represent the input and output of the models we consider. In some models, causes may be associated with a vector v, rather than a single number v.In terms of these variables, the ultimate goal of the models we consider recognition is recognition, in which the model tells us something about the causes v underlying a particular input u. Recognition can be either deterministic or deterministic recognition probabilistic. In a model with deterministic recognition, the output v is the models estimate of the cause underlying input u. In probabilistic recognition, the model estimates the probability that different values of probabilistic recognition v are associated with input u. In either case, the output is taken as the models re-representation of the input.We consider models that infer causes in an unsupervised manner. This means that the existence and identity of any underlying causes must be deduced solely from two sources of information. One is the set of general assumptions and guesses, collectively known as heuristics, concerning the heuristics nature of the input data and the causes that might underly them. These heuristics determined the general form of the generative model. The other source of information is the statistical structure of the input data. In the absence of supervisory information or even reinforcement, causes are judged by their ability to explain and reproduce the statistical structure of the inputs they are designed to represent. The process is analogous to judging the validity of a model by comparing simulated data generated by it with the results of a real experiment. The basic idea is to use assumed causes to generate synthetic input data from a generative model. The statistical structure of the synthetic data is then compared with that of the real input data, and the parameters of the generative model are adjusted until the two are as similar as possible. If the final statistical match is good, the causes are judged trustworthy, and the model can be used as a basis for recognition.Representational learning is a large and complex subject with a terminology and methodology that may be unfamiliar to many readers. Section 10.1 follows two illustrative examples to provide a general introduction. This should give the reader a basic idea of what representational learning attempts to achieve and how it works. Section 10.2 covers more technical aspects of the approach, and 10.3 surveys a number of examples. and synthetic data points generated by the optimal generative model. The square indicates a new input point that is assigned to either cluster A or cluster B with probabilities computed from the recognition model. Many processes can generate clustered data. For example, u 1 and u 2 might be characterizations of the voltage recorded on an extracellular electrode in response to an action potential. Interpreted in this way, these data suggest that we are looking at spikes produced by two neurons, which are the underlying causes of the two clusters seen in figure 10.1A. A more compact and causal description of the data can be provided by a single output variable v that takes the value A or B for each data point, representing which of the two neurons was responsible for a particular action potential. Directly reading the output of such a model would be an example of deterministic recognition, with v  A or B providing the models estimate of which neuron produced the spike associated with input u. We consider, instead, a model with probabilistic recognition that estimates the probability that the spike with input data u was generated by either neuron A or neuron B.In this example, we assume from the start that there are two possible, mutually exclusive causes for the data points, the two neurons A and B. By making this assumption, which is part of the heuristics underlying the generative model, we avoid the problem of identifying the number of pos-sible causes. Probabilistic methods can be used to make statistical inferences about the number of clusters in a data set, but they lie beyond the scope of this text.To illustrate the concept of a generative model, we construct one for the data in figure 10.1A. The general form of the model is determined by the heuristics, assumptions about the nature of the causes and the way they generate inputs. However, the model has parameters that can be adjusted to fit the actual data that are observed. We begin by introducing parameters  A and  B that represent the proportions of action potentials generated by each mixing proportions of the neurons. These might account for the fact that one of the neurons has a higher firing rate than the other, for example. The parameter  v , with v  A or B, specifies the probability Pv G  that a given spike was generated by neuron v in the absence of any knowledge about the input u associated with that spike.over causes. The symbol G stands for all the parameters used to characterize the generative model. At this point, G consists of the two parameters parameters G  A and  B , but more parameters will be added as we proceed. We start by assigning  A and  B random values consistent with the constraint that they must sum to 1.To continue the construction of the generative model, we need to assume something about the distribution of u values arising from the action potentials generated by each neuron. An examination of figure 10.1A suggests that Gaussian distributions might be appropriate. The probability density of u values given that neuron v fired is p This describes the probability of cause v and input u both being produced by the generative model.As mentioned previously, the choice of a particular structure for a generative model reflects our notions and prejudices concerning the nature of the causes that underlie input data. Usually, the heuristics consist of biases toward certain types of representations, which are imposed through the choice of the prior distribution pv G . For exfactorial coding ample, we may want the identified causes to be mutually independent sparse coding or sparse, or of lower dimensionality reduction dimension than the input data. Many heuristics can be formalized using the information theoretic ideas we discuss in chapter 4.Once the optimal generative model has been constructed, the culmination of representational learning is recognition, in which new input data are interpreted in terms of the causes established by the generative model. In probabilistic recognition models, this amounts to determining the probability that cause v is associated with input u, Pvu G , which is called the posterior distribution over causes or the recognition distribution.recognition distributionIn the model of During our discussion of generative models, we skipped over the process by which the parameters G are refined to optimize the match between synthetic and real input data. There are various ways of doing this. In this chapter, we use an approach called expectation maximization. The general theory of EM is discussed in detail in the EM following section but, as an introduction to the method, we apply it here to the example of figure 10.1. Recall that the problem of optimizing the generative model in this case involves adjusting the mixing proportions, means, and variances of the two Gaussian distributions until the clusters of synthetic data points in figure 10.1B and 10.1C match the clusters of actual data points in figure 10.1A.To optimize the match between synthetic and real input data, the parameters g v and v , for vA and B, of the Gaussian distributions of the generative model should equal the means and variances of the data points associated with each cluster in figure 10.1A. If we knew which cluster each input point belonged to, it would be a simple matter to compute these means and variances and construct the optimal generative model. Similarly, we could set  v , the prior probability of a given spike being a member of cluster v, equal to the fraction of data points assigned to that cluster. Of course, we do not know the cluster assignments of the input points that would amount to knowing the answer to the recognition problem. However, we can make an informed guess about which point belongs to which cluster using the recognition distribution computed from equation 10.3. In other words, the recognition distribution Pvu G  provides us with our best current guess about the cluster assignment, and this can be used in place of the actual knowledge about which neuron produced which spike.G  is thus used to assign the data point u to cluster v in a probabilistic manner. In this context, Pvu G  is also called the responsibility of v for u. responsibilityFollowing this reasoning, the mean and variance of the Gaussian distribution corresponding to cause v are set equal to a weighted mean and variance of all the data points, with the weight for point u equal to the current estimate Pvu G  of the probability that it belongs to cluster v.A similar argument is applied to the mixing proportions, resulting in the equations The angle brackets indicate averages over all the input data points. The factors of  v dividing the last two expressions correct for the fact that the number of points in cluster v is expected to be  v times the total number of input data points, whereas the full averages denoted by the brackets involve dividing by the total number of data points.The full EM algorithm consists of two phases that are applied in alternation. In the E phase, the responsibilities Pvu G  are cal- The data of figure 10.1A consist of two separated clusters of points, so a cause v that takes only two different values is appropriate. In summary, causal models make use of the following probability distributions. p The process of matching the distribution pu G  produced by the generative model to the actual input distribution pu is a form of density estimation. This technique is discussed in chapter 8 in connection with the density estimation Boltzmann machine. As mentioned in the introduction, the parameters G of the generative model are fitted to the input data by minimizing the discrepancy between the probability density of the input data p where K is a term associated with the entropy of the distribution p As in the case of the Boltzmann machine discussed in chapter 8, equation 10.6 implies that minimizing the discrepancy between pu and pu G  amounts to maximizing the log likelihood that the training data could have been created by the generative model,L is the average log likelihood, and the method is known as maximaximum likelihood density estimation mum likelihood density estimation. A theorem due to Shannon describes circumstances under which the generative model that maximizes the likelihood over input data also provides the most efficient way of coding those data, so density estimation is closely related to optimal coding.Although stochastic gradient ascent can be used to adjust the parameters of the generative model to maximize the likelihood in equation 10.7, the EM algorithm discussed in the introduction is an alternative procedure that is often more efficient. We applied this algorithm, on intuitive grounds, to the examples of figures 10.1 and 10.3, but we now present a more general and rigorous discussion. This is based on the connection of EM with maximization of the function Because the Kullback-Leibler divergence is never negative, The negative of F is related to the free energy used in statistical physics.free energy F Expressions 10.10, 10.11, and 10.12 are critical to the operation of EM. The two phases of EM are concerned with separately maximizing F with respect to one of its two arguments, keeping the other one fixed. When F increases, this increases a lower bound on the log likelihood of the input data. In the M phase, F is increased with respect to G , keeping Q constant. For the generative models considered as examples in the previous section, it is possible to maximize F with respect to G in a single step. For other generative models, this may require multiple steps that perform gradient ascent on F . In the E phase, F is increased with respect to Q, keeping G constant. From equation 10.10, we see that increasing F by changing Q is equivalent to reducing the average Kullback-Leibler divergence between Qv u and P One advantage of EM over likelihood maximization through gradient methods is that large steps toward the maximum can be taken during each M cycle of modification. Of course, the log likelihood may have multiple maxima, in which case neither gradient ascent nor EM is guaranteed to find the globally optimal solution.If the causal model being considered is invertible, the E step of EM simply consists of solving equation 10.3 for the recognition distribution, and setting Q equal to the resulting Pvu G , as in equation 10.12. This maximizes F with respect to Q by setting the Kullback-Leibler term in equation 10.10 to 0, and it makes the function F equal to L, the average log likelihood of the data points. However, the EM algorithm for maximizing F is not exactly the same as likelihood maximization by gradient ascent of F . This is because the function Q is held constant during the M phase while the parameters of the generative model are modified. Although F is equal to L at the beginning of the M phase, exact equality ceases to be true as soon as the parameters are modified, making Pvu G  different from Q. F is equal to L again only after the update of Q during the following E phase. At this point, L must have increased since the last E phase, because F has increased. This shows that the log likelihood increases monotonically during EM until the process converges.For the example of figure 10.1, the joint probability over causes and inputs isand thusThe E phase amounts to computing P  If the generative model is noninvertible, the E phase of the EM algorithm is more complex than simply setting Q equal to Pvu G , because it is not practical to compute the recognition distribution exactly. The steps taken during the E phase depend on whether the approximation to the inverse of the model is deterministic or probabilistic, although the basic argument is the same in either case.Deterministic recognition results in a prediction v of the cause underlying input u. The M phase of EM consists, as always, of maximizing this expression with respect to G . During the E phase we try to find the function v that maximizes F . Because v is varied during the optimization procedure, the approach is sometimes called a variational method. The E and variational method M steps make intuitive sense we are finding the input-output relationship that maximizes the probability that the generative model would have simultaneously produced the input u and cause v.The alternative to using a deterministic approximate recognition model is to treat Qv u as a full probability distribution over v for each input example u. In some cases, W has separate parameters for each possible input u. This means that each input has a separate approximate recognition distribution which is individually tailored, subject to the inherent simplifying assumptions, to its own causes. The mean-field approximation to the Boltzmann machine discussed in chapter 7 is an example of this type.It is not necessary to maximize F completely with respect to W and then G during successive E and M phases. Instead, gradient ascent steps that modify W and G by small amounts can be taken in alternation, in which case the E and M phases effectively overlap.. Nevertheless, a good generative model should be obtained if the lower bound is tight.In this section, we present a number of models in which representational learning is achieved through density estimation. The mixture of Gaussians and factor analysis models that we have already mentioned are examples of invertible generative models with probabilistic recognition. K-means is a limiting case of mixture of Gaussians with deterministic recognition, and principal components analysis is a limiting case of factor analysis with deterministic recognition. We consider two other models with deterministic recognition: independent components analysis, which is invertible and sparse coding, which is noninvertible. Our final example, the Helmholtz machine, is noninvertible with probabilistic recognition. The Boltzmann machine, discussed in chapters 7 and 8, is an additional example that is closely related to the causal models discussed here. We summarize and interpret general properties of representations derived from causal models at the end of the chapter. The table in the appendix summarizes the generative and recognition distributions and the learning rules for all the models we discuss.The model applied in the introduction to the data in is a Gaussian distribution with mean g and variances for the individual components equal to . The function F for this model is given by an expression similar to equation 10.14, leading to the M-phase learning rules given in the appendix. Once the generative model has been optimized, the recognition distribution is constructed from equation 10.3 asA special case of mixture of Gaussians can be derived in the limit that the variances of the Gaussians are equal and tend toward 0, v   0. We discuss this limit for two clusters, as in figure 10.1. When is extremely small, the recognition distribution Pvu G  of equation 10.19 degenerates because it takes essentially two values, 0 or 1, depending on whether u is closer to one cluster or the other. This provides a deterministic, rather than a probabilistic, classification of u. In the degenerate case, EM consists of choosing two random values for the centers of the two cluster distributions, and then repeatedly finding all the inputs u that are closest to a given center g v , and then moving g v to the average of these points. This is called the K-means algorithm. The mixing proportions  v do not play an important role for the K-means algorithm. New input points are recognized as belonging to the clusters to which they are closest.The model used in The expression det  indicates the value of the determinant of . In factor analysis, is taken to be diagonal,  diag, with all the diagonal elements strictly positive, so its inverse is simplyBecause is diagonal, the individual components of v are mutually independent. Thus, any correlations between the components of u must arise from the mean values G  v of the generative distribution. To be well specified, the model requires v to have fewer dimensions than u. In terms of heuristics, factor analysis seeks a relatively small number of independent causes that account, in a linear manner, for collective Gaussian structure in the inputs.The recognition distribution for factor analysis has the Gaussian form In this case, we can understand the goal of density estimation in an additional way. By direct calculation, as in equation 10.1, the marginal distribution for u is In the same way that setting the parameters v to 0 in the mixture of Gaussians model leads to the K-means algorithm, setting all the variances in factor analysis to 0 leads to another well-known method, principal components analysis with respect to g, without changing the expression for v. Here, K is a term independent of g and . In this expression, the only term that depends on g is proportional to u  gv 2 . Minimizing this in the M phase produces a new value of g given by For principal components analysis, we can say more about the value of g at convergence. We consider the case g 2  1 because we can always multiply g and divide v by the same factor to make this true without affecting the dominant term in F Here, we have used expression 10.24 for v. Minimizing 10.27 with respect to g, subject to the constraint g 2  1, gives the result that g is the eigenvector of the covariance matrix uu with maximum eigenvalue. This is just the principal component vector and is equivalent to finding the vector of unit length with the largest possible average projection onto u.Note that there are ways other than EM for finding eigenvectors of this matrix.The argument we have given shows that principal components analysis is a degenerate form of factor analysis. This is also true if more than one factor is considered, although maximizing F constrains the projections G  u and therefore is sufficient only to force G to represent the principal components subspace of the data. The same subspace emerges from full factor analysis provided that the variances of all the factors are equal, even when they are nonzero. Both principal components analysis and factor analysis produce a mar-is non-Gaussian, the best that these models can do is to match the mean and covariance of p The v values in response to input in factor and principal components analysis tend to be Gaussian distributed. If we attempt to relate such causal variables to the activities of cortical neurons, we find a discrepancy, because the activity distributions of cortical cells in response to natural inputs are not Gaussian. A sparse representation over a large population of neurons might more naturally be defined as one in which each input is encoded by a small number of the neurons in the population. Unfortunately, identifying this form of sparseness experimentally is difficult.Sparse coding can arise in generative models that have sparse prior distributions over causes. Unlike factor analysis and principal components analysis, sparse coding does not stress minimizing the number of representing units, and sparse representations may require large numbers of units. This is not a disadvantage for modeling the visual system because representations in visual areas are indeed greatly expanded at various steps along the pathway. For example, there are around 40 cells in primary visual cortex for each cell in the visual thalamus. Downstream processing can benefit greatly from sparse representations because, for one thing, they minimize interference between different patterns of input.Because they employ Gaussian priors, factor analysis and principal components analysis do not generate sparse representations. The mixture of Gaussians model is extremely sparse because each input is represented by a single cause. This may be reasonable for relatively simple input patterns, but for complex stimuli such as images, we seek something between these extremes. where K is a term that is independent of G and v. For convenience in discussing the EM procedure, we further take  1 and do not allow it to vary. Similarly, we assume that  in equation 10.31 is predetermined and held fixed. Then, G consists only of the matrix G.The E phase of EM involves maximizing F with respect to v for every u. This leads to the conditionsThe prime on g indicates a derivative. One way to solve this equation is to let v evolve over time according to the equation After v has been determined during the E phase of EM, a delta rule is used during the M phase to modify G and improve the generative model. The full learning rule is given in the appendix. The delta rule follows from maximizing F Applying the sparse coding model to inputs coming from the pixel intensities of small square patches of monochrome photographs of natural scenes leads to selectivities that resemble those of cortical simple cells. Before studying this result, we need to specify how the selectivities of generative models, such as the sparse coding model, are defined. The selectivities of sensory neurons are typically described by receptive fields, as in chapter 2. For a causal model, one definition of a receptive field for unit a is the set of inputs u for which v a is likely to take large values. However, it may be impossible to construct receptive fields by averaging over these inputs in nonlinear models, such as sparse coding models. Furthermore, generative models are most naturally characterized by projective fields rather than projective field receptive fields. The projective field associated with a particular cause v a can be defined as the set of inputs that it frequently generates. This consists of all the u values for which Puv a  G  is sufficiently large when v a is large. For the model of Projective fields for the Olshausen and Field model trained on natural scenes are shown in In a generative model, projective fields are associated with the causes underlying the visual images presented during training. The fact that the causes extracted by the sparse coding model resemble Gabor patches within the visual field is somewhat strange from this perspective. It is difficult to conceive of images as arising from such low-level causes, instead of causes couched in terms of objects within the images, for example. From the perspective of good representation, causes that are more like objects and less like Gabor patches would be more useful. To put this another way, although the prior distribution over causes biased them toward mutual independence, the causes produced by the recognition model in response to natural images are not actually independent. This is due to the structure in images arising from more complex objects than bars and gratings. It is unlikely that this high-order structure can be extracted by a model with only one set of causes. It is more natural to think of causes in a hierarchical manner, with causes at a higher level accounting for structure in the causes at a lower level. The multiple representations in areas along the visual pathway suggests such a hierarchical scheme, but the corresponding models are still in the rudimentary stages of development. As for the case of the mixtures of Gaussians model and factor analysis, an interesting model emerges from sparse coding as  0. In this limit, the generative distribution approaches a  function and always generates u  G  v. Under the additional restriction that there are as many causes as inputs, the approximation we used for the sparse coding model of making the recognition distribution deterministic becomes exact, and the recognition distribution that maximizes F is where K is independent of G. Under the conventional EM procedure, we would maximize this expression with respect to G, keeping W fixed. However, the normal procedure fails in this case, because the minimum of the right side of equation 10.36 occurs at G  W 1 , and W is being held fixed, so G cannot change. This is an anomaly of coordinate ascent in this particular limit.Fortunately, it is easy to fix this problem, because we know that W  G 1 provides an exact inversion of the generative model. Therefore, instead of holding W fixed during the M phase of an EM procedure, we keep W  G 1 at all times as we change G. This sets F equal to the average log likelihood, and the process of optimizing with respect to G is equivalent to likelihood maximization. Because W  G 1 , maximizing with respect to W is equivalent to maximizing with respect to G, and it turns out that this is easier to do. Therefore, we set W  G 1 in equation 10.36, which causes the first term to vanish, and write the remaining terms as the log likelihood expressed as a function of W instead of G, Direct stochastic gradient ascent on this log likelihood can be performed using the update rule This algorithm is called independent components analysis. Just as the sparse coding network is a nonlinear generalization of factor analysis, independent components analysis is a nonlinear generalization of principal components analysis that attempts to account for non-Gaussian features of the input distribution. The generative model is based on the assumption that u  G  v. Some other technical conditions must be satisfied for independent components analysis to extract reasonable causes. Specifically, the prior distributions over causes p The independent components algorithm was suggested by One advantage independent components analysis has over other sparse coding algorithms is that, because the recognition model is an exact inverse of the generative model, receptive as well as projective fields can be constructed. Just as the projective field for v a can be represented by the matrix elements G ab for all b values, the receptive field is given by W ab for all b.To illustrate independent components analysis, figure 10.8 shows an example of its application to the sounds created by tapping a tooth while adjusting the shape of the mouth to reproduce a  Learning in the Helmholtz machine proceeds by using stochastic sampling to replace the weighted sums in equations 10.45 and 10.46. In the M phase, an input u from P The two phases of learning are sometimes called wake and sleep because wake-sleep algorithm learning in the first phase is driven by real inputs u from the environment, while learning in the second phase is driven by values v and u "fantasized" by the generative model. This terminology is based on slightly different principles from the wake and sleep phases of the Boltzmann machine discussed in chapter 8. The sleep phase is only an approximation of the actual E phase, and general conditions under which learning converges appropriately are not known.Because of the widespread significance of coding, transmitting, storing, and decoding visual images such as photographs and movies, substantial effort has been devoted to understanding the structure of this class of inputs. As a result, visual images provide an ideal testing ground for representational learning algorithms, allowing us to go beyond evaluating the representations they produce solely in terms of the log likelihood and qualitative similarities with cortical receptive fields.Most modern image processing techniques are based on multi-resolution decompositions. In such decompositions, images are represented by the activity of a population of units with systematically varying spatial frequency and orientation preferences, centered at various locations on the image. The outputs of the representational units are generated by filters that act as receptive fields and are partially localized in both space and spatial frequency. The filters usually have similar underlying forms, but they are cast at different spatial scales and centered at different locations for the different units. Systematic versions of such representations, in forms such as wavelets, are important signal processing tools, and there is an extensive body of theory about their representational and coding qualities. Representation of sensory information in separated frequency bands at different spatial locations has significant psychophysical consequences as well.The projective fields of the units in the sparse coding network shown in Many multi-resolution decompositions, with a variety of computational and representational properties, can be expressed as linear transformations v  W  u, where the rows of W describe filters, such as those illustrated in figure 10.10A. One reason for using multi-resolution decompositions is that they offer efficient ways of encoding visual images, whereas raw values of input pixels provide an inefficient encoding. This is illustrated by the dot-dashed line in By contrast, the solid line in In making these statements about the distributions of activities, we are equating the output distribution of a filter applied at many locations on a single image with the output distribution of a filter applied at a fixed location on many images. This assumes spatial translational invariance of the ensemble of visual images.Images represented by multi-resolution filters can be further compressed by retaining only approximate values of the filter outputs. This is called lossy coding and may consist of reporting filter outputs as integer multilossy coding ples of a basic unit. Making the multi-resolution code for an image lossy by coarsely quantizing the outputs of the highest spatial frequency filters generally has quite minimal perceptual consequences, while saving substantial coding cost. This fact illustrates the important point that trying to build generative models of all aspects of visual images may be unnecessarily difficult, because only certain aspects of images are actually relevant. Unfortunately, abstract principles are unlikely to tell us what information in the input can safely be discarded independent of details of how the representations are to be used.Sparse representations often have more output units than input units. Such representations, called overcomplete, are the subject of substantial work in multi-resolution theory. Many reasons have been suggested for overcompleteness, although none obviously emerges from the requirement of fitting good probabilistic models to input data.One interesting idea comes from the notion that the task of manipulating representations should be invariant to the groups of symmetry transformations of the input, which, for images, include rotation, translation, and scaling. Complete representations are minimal, and so do not densely sample orientations. This means that the operations required to manipulate images of objects presented at angles not directly represented by the filters are different from those required at the represented angles. When a representation is overcomplete in such a way that different orientations are represented roughly equally, as in primary visual cortex, the computational operations required to manipulate images are more uniform as a function of image orientation. Similar ideas apply across scale, so that the operations required to manipulate large and small images of the same object are likewise similar. However, it is impossible to generate representations that satisfy all these constraints perfectly.In more realistic models that include noise, other rationales for overcompleteness come from considering population codes, in which many units redundantly report information about closely related quantities so that uncertainty can be reduced. Despite the ubiquity of overcomplete population codes in the brain, there are few representational learning models that produce them satisfactorily. The coordinated representations required to construct population codes are often incompatible with other heuristics such as factorial or sparse coding.One of the failings of multi-resolution decompositions for coding is that the outputs are not mutually independent. This makes encoding each of  The interdependence shown in figure 10.12 suggests a failing of sparse coding to which we have alluded before. Although the prior distribution for sparse coding stipulates independent causes, the causes identified as underlying real images are not independent. The dependence apparent in The most important dependencies as far as causal models are concerned are those induced by the presence in images of objects with large-scale coordinated structure. Finding and building models of these dependencies is the goal for more sophisticated, hierarchical representational learning schemes aimed ultimately at object recognition within complex visual scenes.We have presented a systematic treatment of exact and approximate maximum likelihood density estimation as a way of fitting probabilistic generative models and thereby performing representational learning. Recognition models, which are the statistical inverses of generative models, specify the causes underlying an input and play a crucial role in learning. We discussed the expectation maximization algorithm applied to invertible and noninvertible models, including the use of deterministic and probabilistic approximate recognition models and a lower bound on the log likelihood.We presented a variety of models for continuous inputs with discrete, continuous, or vector-valued causes. These include mixture of Gaussians, Kmeans, factor analysis, principal components analysis, sparse coding, and independent components analysis. We also described the Helmholtz machine and discussed general issues of multi-resolution representation and coding.  If e is an eigenvector, e is also an eigenvector for any nonzero value of . We can use this freedom to normalize eigenvectors so that e  1. If two eigenvectors, say e 1 and e 2 , have the same eigenvalues  1   2 , they are termed degenerate. Then, e 1  e 2 is also an eigenvector with the same degeneracy eigenvalue, for any  and  that are not both 0. Apart from such degeneracies, an N by N matrix can have at most N eigenvectors, although some matrices have fewer. If W has N nondegenerate eigenvalues, the eigenvectors e 1 , . . . , e N are linearly independent, meaning that  c  e   0 0 0 linear independence only if the coefficients c   0 for all . These eigenvectors can be used to represent any N component vector v through the relationc  e  ,with a unique set of coefficients c  . They are thus said to form a basis for the set of vectors v. basisThe eigenvalues and eigenvectors of symmetric matrices have special properties, and for the remainder of this section, we consymmetric matrix sider this case. The eigenvalues of a symmetric matrix are real, and the eigenvectors are real and orthogonal. This means that, if they are normalized to unit length, the eigenvectors satisfy orthonormal eigenvectors e   e     .To derive this result we note that, if W is a symmetric matrix, we can write e   W  W  e     e  . Therefore, allowing the matrix to act in both directions, we find e   W  e     e   e     e   e  . If      , this requires e   e   0. For orthogonal and normalized eigenvectors, the coefficients in equation A.14 take the values The transformation to and back from a diagonal form is extremely useful because computations with diagonal matrices are easy. Defining L  diag, we find, for example, that Despite its name, the Dirac  function is not a properly defined function, but rather the limit of a sequence of functions. In this limit, the  function approaches 0 everywhere except where its argument is 0, and there it grows without bound. The infinite height and infinitesimal width of this function are matched so that its integral is 1. Thus, dt   1 , The sequence of functions used to construct the  function as a limit is not unique. In essence, any function that integrates to 1 and has a single peak that gets continually narrower and taller as the limit is taken can be used. For example, the  function can be expressed as the limit of a square pulse  Comparing equations A.31 and A.32, we see that the eigenvalue for this eigenfunction is   d K exp . A functional analog of expanding a vector using eigenvectors as a basis is the inverse Fourier transform, which expresses a function in an expansion using complex exponential eigenfunctions as a basis. The analog of equation A.16 for determining the coefficient functions of this expansion is the Fourier transform.As outlined in the previous section, Fourier transforms provide a useful representation for functions when they are acted upon by translationinvariant linear operators.The Fourier transform of a function f is a complex function of a real argument  given by This equation implies a periodic continuation of f n outside the range 0  n  N t , so that f nN t  f n for all n. Consult the references in the bibliography for an analysis of the properties of the discrete Fourier transform and the quality of its approximation to the continuous Fourier transform. Note in particular that there is a difference between the discretetime Fourier transform, which is the Fourier transform of a signal that is inherently discrete, and the discrete Fourier transform, given above, which is based on a finite number of samples of an underlying continuous function. If f is band-limited, meaning thatf  0 for   , the sampling theorem states that f is sampling theorem completely determined by regular samples spaced at intervals 1.Fourier transforms of functions of more than one variable involve a direct extension of the equations given above to multi-dimensional integrals. For example,f.The properties of multi-dimensional transforms are similar to those of onedimensional transforms.An operation frequently encountered in the text is minimizing a quadratic form. In terms of vectors, this typically amounts to finding the matrix W that makes the product W  v closest to another vector u when averaged over a number of presentations of v and u. The function to be minimized is the average squared error W  v  u 2 , where the brackets denote averaging over all the different samples v and u. Often, when a function f has to be minimized or maximized with respect to a vector v, there is an additional constraint on v that requires another function g to be held constant. The standard way of dealing with this situation is to find the extrema of the function f  g where  is a free parameter called a Lagrange multiplier. Once this is done, the value Lagrange multiplier of  is determined by requiring g to take the specified value. This procedure can appear a bit mysterious when first encountered, so we provide a rather extended discussion. for all a  c. The derivatives of f and g are functions of v, so these equations can be solved to determine where the extremum point is located.In the above derivation, we have singled out component c for special treatment. We have no way of knowing until we get to the end of the calculation whether the particular c we chose leads to a simple or a complex set of final equations. The clever idea of the Lagrange multiplier is to notice that the whole problem is symmetric with respect to the different components of v. Before, we had to say that these equations held only for a  c because c was treated differently. Now, however, notice that the above equation when a is set to c is algebraically equivalent to the definition in equation A.57. Thus, we can say that equation A.58 applies for all a, and this provides a symmetric formulation of the problem of finding an extremum that often results in simpler algebra.The final realization is that equation A.58 for all a is precisely what we would have derived if we had set out in the first place to find an extremum of the function f  g and forgotten about the constraint entirely.Of course this lunch is not completely free. From equation A.58, we derive a set of extremum points parameterized by the undetermined variable .To fix , we must substitute this family of solutions back into g The most general differential equation we consider takes the form dv dt  f , We focus most of our analysis on fixed point solutions, which are also called equilibrium points. For v  to be a time-independent solution equilibrium point of equation A.59, we must have f  0. General solutions of equation A.59 when f is nonlinear cannot be constructed, but we can use linear techniques to study the behavior of v near a fixed point v  . If f is linear, the techniques we use and the solutions we obtain as approximations in the nonlinear case are exact. Near the fixed point v  , we write v  v      Stability of the fixed point v  requires the real parts of all the eigenvalues to be negative. In this case, the point v  is a stable fixedpoint attractor of the system, meaning that v will approach v  if it attractor starts from any point in the neighborhood of v  . If any real part is positive, the fixed point is unstable. Almost any v initially unstable fixed point in the neighborhood of v  will move away from that neighborhood. If f is linear, the exponential growth of v  v   never stops in this case. For a nonlinear f , equation A.66 determines what happens only in the neighborhood of v  , and the system may ultimately find a stable attractor away from v  , either a fixed point, a limit cycle, or a chaotic attractor. In all these cases, the mode for which the real part of   takes the largest value dominates the dynamics as t  . If this real part is equal to 0, the fixed point is called marginally stable.As mentioned previously, the analysis presented above as an approximation for nonlinear differential equations near a fixed point is exact if the original equation is linear. In the text, we frequently encounter linear equations of the form In some cases, we consider discrete rather than continuous dynamics defined over discrete steps n  1, 2, . . . If this is greater than 1 for any value of , v   as n  . If it is less than 1 for all , v  0 0 0 in this limit.Biophysical models of single cells involve equivalent circuits composed of resistors, capacitors, and voltage and current sources. We review here basic results for such circuits. Resistance is measured in ohms 1 ohm is the resistance through which 1 ampere of current causes a voltage drop of 1 volt.A capacitor stores charge across an insulating medium, and the voltage across it V C  V 1  V 2 is related to the charge it stores, Q C , by In the circuit of figure A.1D, we have added an external source passing the current I e . For this circuit, Kirchhoffs and Ohms laws tells us that I e  I 1  I 2  VR 1  VR 2 . This indicates how resistors add in parallel, V  I e R 1 R 2 .  where V   E  RI e and   RC. This shows exponential relaxation from the initial potential V to the equilibrium potential V  at a rate governed by the time constant  of the circuit.For the case I e  I cos, once an initial transient has decayed to 0, we find V  E  RI cos  1   2  2 , Probability distributions and densities are discussed extensively in the text. Here, we present a slightly more formal treatment. At the heart of probability theory lie two objects: a sample space, , and a measure. We begin by considering the simplest case of a finite sample space. Here, each element  of the full sample space can be thought of as one of the possample space probability measure sible outcomes of a random process, for example, one of the 6 5 possible results of rolling five dice. The measure assigns a number   to each outcome , and these must satisfy 0     1 and     1.We are primarily interested in random variables. A random variable is a mapping from a random outcome  to a space such as the space of integers. An example is the number of ones that appear when five dice are rolled. Typically, a capital letter, such as S, is used for the random variable, and the corresponding lowercase letter, s in this case, is used for a particular value it might take. The probability that S takes the value s is then written as The notation S refers to the value of S generated by the random event labeled by , and the sum is over all events for which S  s. Here, we use to denote the variance of the Gaussian distribution, which is more often written as  2 . The asterisks in the entries for the Cauchy distribution reflect the fact that it has such heavy tails that the integrals defining its mean and variance do not converge. Nevertheless,  and 1 2 play similar roles, and are called location and scale parameters respectively.The Gaussian distribution is particularly important because of the central limit theorem. of feedforward and recurrent input as the source of orientation selectivity in primary visual cortex. We presented a model of a hypercolumn an extension to multiple hypercolumns is used by  present theoretical results concerning associative networks, in particular their capacity to store information. Associative memory in nonbinary recurrent networks has been studied in particular by Treves and collaborators Rinzel and Ermentrout discusses phase-plane methods, and XPP provides a computer environment for performing phase-plane and other forms of mathematical analysis on neuron and network models. We followed The Boltzmann machine was introduced by This book assumes a familiarity with basic methods of linear algebra, differential equations, and probability theory, as covered in standard texts. Here, we describe the notation we use and briefly sketch highlights of various techniques. The references in the bibliography at the end of this appendix provides further information.An operation O on a quantity z is called linear if, applied to any two instances z 1 and z 2 , O  O  O for any constants  and . In this section, we consider linear operations on vectors and funclinear operator tions. We define a vector v as an array of N numbers.vector v The numbers v a for a  1, 2, . . . , N are called the components of the vector. These are sometimes listed in a single N-row column, it can be periodic function represented by a Fourier series rather than a Fourier integral. That is, Fourier serieswheref k is given byfAs in the case of Fourier transforms, certain conditions have to hold for the series to converge and to be exactly invertible. The Fourier series has properties similar to Fourier transforms, including a convolution theorem and a version of Parsevals theorem. The real and imaginary parts of a Fourier series are often separated, giving the alternative formWhen computed numerically, a Fourier transform is typically based on a certain number, N t , of samples of the function, f n  f for n  0, 1, . . . N t  1. The discrete Fourier transform of these samples is then discrete Fourier transform used as an approximation of the continuous Fourier transform. The discrete Fourier transform is defined as