As part of a renaissance in the genetic analysis of simple organisms in the mid1970s, several investigators recognized that the genetic basis of learning and memory might be effectively studied in the fruit fly, Drosophila melanogaster. In the intervening quarter-century, this approach has yielded some fundamental insights. Although learning and memory has certainly been one of the more difficult problems tackled by Drosophila geneticists, their efforts have been surprisingly successful. A number of genetic mutations have been discovered that to alter learning and memory, and the identification of these genes has provided a valuable framework for studying the cellular mechanisms of these processes.The initial problem in this work was to develop behavioral tests that could identify abnormal learning and/or memory defects in large populations of flies. This challenge was met by Seymour Benzer and his colleagues Chip Quinn and Bill Harris at the California Institute of Technology, who developed the olfactory and visual learning tests that have become the basis for most subsequent analyses of learning and memory in the fruit fly (see These studies led to the identification of an ever-increasing number of single gene mutations that disrupt learning and/or memory in flies. The behavioral and molecular studies of the mutants (given whimsical but descriptive names like dunce, rutabaga, and amnesiac) suggested that a central pathway for learning and memory in the fly is signal transduction mediated by the cyclic nucleotide cAMP. Thus, the gene products of the dunce, rutabaga, and amnesiac loci are, respectively, a phosphodiesterase (which degrades cAMP), an adenylyl cyclase (which converts ATP to cAMP), and a peptide transmitter that stimulates adenylyl cyclase. This conclusion about the importance of cAMP has been confirmed by the finding that genetic manipulation of the CREB transcription factor also interferes with learning and memory in normal flies.These observations in Drosophila accord with conclusions reached in studies of Aplysia and mammals (see text) and have emphasized the importance of cAMP-mediated learning and memory in a wide range of additional species. Performance of normal and mutant flies on an olfactory learning task. The performance of both dunce and rutabaga mutants on this task is diminished by at least 50%. Flies that are mutant at both the dunce and rutabaga locus show a larger decrease in performance, suggesting that the two genes disrupt different but related aspects of learning. Evidence for synaptic plasticity in the mammalian nervous system is widespread; indeed, it is probably safe to conclude that all chemical synapses are capable of plastic change. Synaptic plasticity mechanisms at mammalian synapses, like their invertebrate counterparts, occur on time scales ranging from milliseconds to days, weeks, or longer. The short-term forms of plasticity-those lasting for minutes or less-have been studied in greatest detail at peripheral neuromuscular synapses, the same synapses that proved so valuable for understanding basic mechanisms of synaptic transmission (Chapter 5). Repeated activation of the neuromuscular junction triggers several changes that vary in both direction and duration Synaptic transmission also can be diminished following repeated synaptic activity. Such synaptic depression occurs when many presynaptic action potentials occur in rapid succession and depends on the amount of neurotransmitter that has been released (see During repeated synaptic activity, these various types of plasticity can interact in complex ways. For example, at the neuromuscular synapse, repeated activity first facilitates synaptic transmission, and depletion of synaptic vesicles then allows depression to dominate and weaken the synapse (see Facilitation, depression, and post-tetanic potentiation can briefly modify synaptic transmission. While these mechanisms are probably responsible for many short-lived changes in brain circuitry, they cannot provide the basis for memories or other manifestations of behavioral plasticity that persist for weeks, months, or years. As might be expected, many synapses in the mammalian central nervous system exhibit long-lasting forms of synaptic plasticity that are plausible substrates for more permanent changes in behavior. Because of their duration, these forms of synaptic plasticity are widely believed to be cellular correlates of learning and memory. Thus, a great deal of effort has gone into understanding how they are generated.Some patterns of synaptic activity in the CNS produce a long-lasting increase in synaptic strength known as long-term potentiation (LTP), whereas other patterns of activity produce a long-lasting decrease in synaptic strength, known as long-term depression (LTD). LTP and LTD are broad terms that describe only the direction of change in synaptic efficacy; in fact, different cellular and molecular mechanisms can be involved in producing LTP or LTD at different synapses. In general, these different forms of synaptic plasticity are produced by different histories of activity, and are mediated by different complements of intracellular signal transduction pathways in the nerve cells involved.Time LTP has been most thoroughly studied at excitatory synapses in the mammalian hippocampus, an area of the brain that is especially important in the formation and/or retrieval of some forms of memory (see Chapter 30). In humans, functional imaging shows that the human hippocampus is activated during certain kinds of memory tasks, and that damage to the hippocampus results in an inability to form certain types of new memories. In rodents, hippocampal neurons fire action potentials only when an animal is in certain locations. Such "place cells" appear to encode spatial memories, an interpretation supported by the fact that hippocampal damage prevents rats from developing proficiency in spatial learning tasks (see LTP also exhibits the property of input specificity: When LTP is induced by the stimulation of one synapse, it does not occur in other, inactive synapses that contact the same neuron (see Another important property of LTP is associativity Despite the fact that LTP was discovered more than 30 years ago, its molecular underpinnings were not well understood until recently. A key advance in this effort occurred in the mid-1980s, when it was discovered that antagonists of the NMDA type of glutamate receptor prevent LTP, but have no effect on the synaptic response evoked by low-frequency stimulation of the Schaffer collaterals. At about the same time, the unique biophysical properties of the NMDA receptor were first appreciated. As described in Chapter 6, the NMDA receptor channel is permeable to Ca 2+ , but is blocked by physiological concentrations of Mg 2+ . This property provides a critical insight into how LTP is induced. Thus, during low-frequency synaptic transmission, glutamate released by the Schaffer collaterals binds to both NMDA-type and AMPA/kainate-type glutamate receptors. While both types of receptors bind glutamate, if the postsynaptic neuron is at its normal resting membrane potential, the NMDA channels will be blocked by Mg 2+ ions and no current will flow NMDA receptor thus behaves like a molecular "and" gate: The channel opens (to induce LTP) only when glutamate is bound to NMDA receptors and the postsynaptic cell is depolarized to relieve the Mg 2+ block of the NMDA channel. Thus, the NMDA receptor can detect the coincidence of two events.These properties of the NMDA receptor can account for many of the characteristics of LTP. The specificity of LTP (see Several sorts of observations have confirmed that a rise in the concentration of Ca 2+ in the postsynaptic CA1 neuron, due to Ca 2+ ions entering through NMDA receptors, serves as a second messenger signal that induces LTP. Imaging studies, for instance, have shown that activation of NMDA receptors causes increases in postsynaptic Ca 2+ levels. Furthermore, injection of Ca 2+ chelators blocks LTP induction, whereas elevation of Ca 2+ levels in postsynaptic neurons potentiates synaptic transmission. Ca 2+ induces LTP by activating complicated signal transduction cascades that include protein kinases in the postsynaptic neuron. At least two Ca 2+ -activated protein kinases have been implicated in LTP induction Many synapses in the brain involve small protrusions from dendritic branches known as spines ( Since the earliest description of these structures by Santiago Ram√≥n y Cajal in the late 1800s, dendritic spines have fascinated generations of neuroscientists, inspiring many speculations about their function. One of the earliest conjectures was that the narrow spine neck electrically isolates synapses from the rest of the neuron. Given that the size of spine necks can change, such a mechanism could cause the physiological effect of individual synapses to vary over time, thereby providing a cellular mechanism for forms of synaptic plasticity such as LTP and LTD. However, subsequent measurements of the properties of spine necks indicate that these structures would be relatively ineffective in attenuating the flow of electrical current between spine heads and dendrites.Another theory-currently the most popular functional concept-postulates that spines create biochemical compartments. This idea is based on the supposition that the spine neck could prevent diffusion of biochemical signals from the spine head to the rest of the dendrite. Several observations are consistent with this notion. First, measurements show that the spine neck does indeed serve as a barrier to diffusion, slowing the rate of molecular movement by a factor of 100 or more. Second, spines are found only at excitatory synapses, where it is known that synaptic transmission generates  Nevertheless, there are counterarguments to the hypothesis that spines provide relatively isolated biochemical compartments. For example, it is known that other second messengers, such as IP 3 , can diffuse out of the spine head and into the dendritic shaft. Presumably this difference in diffusion is due to the fact that IP 3 signals last longer than Ca 2+ signals, allowing IP 3 sufficient time to overcome the diffusion barrier of the spine neck. Another relevant point is that postsynaptic Ca 2+ signals are highly localized, even at excitatory synapses that do not have spines. Thus, in at least some instances, spines are neither necessary nor sufficient for localization of synaptic second messenger signaling.A final and less controversial idea is that the purpose of spines is to serve as reservoirs where signaling proteins, such as the downstream molecular targets of Ca 2+ and IP 3 , can be concentrated. Consistent with this possibility, glutamate receptors are highly concentrated on spine heads, and the postsynaptic density comprises dozens of proteins involved in intracellular signal transduction ( If synapses simply continued to increase in strength as a result of LTP, eventually they would reach some level of maximum efficacy, making it difficult to encode new information. Thus, to make synaptic strengthening useful, other processes must selectively weaken specific sets of synapses. Long-term depression (LTD) is such a process. In the late 1970s, LTD was found to occur at the synapses between the Schaffer collaterals and the CA1 pyramidal cells in the hippocampus. Whereas LTP at these synapses requires brief, high-frequency stimulation, LTD occurs when the Schaffer collaterals are stimulated at a low rate-about 1 Hz-for long periods (10-15 minutes). This pattern of  Schaffer collaterals LTP and LTD at the Schaffer collateral-CA1 synapses actually share several key elements. Both require activation of NMDA-type glutamate receptors and the resulting entry of Ca 2+ into the postsynaptic cell. The major determinant of whether LTP or LTD arises appears to be the amount of Ca 2+ in the postsynaptic cell: Small rises in Ca 2+ lead to depression, whereas large increases trigger potentiation. As noted above, LTP is at least partially due to activation of CaMKII, which phosphorylates target proteins. LTD, on the other hand, appears to result from activation of Ca 2+ -dependent phosphatases that cleave phosphate groups from these target molecules (see Chapter 7). Evidence in support of this idea is that phosphatase inhibitors prevent LTD, but have no effect on LTP. The different effects of Ca 2+ during LTD and LTP may arise from the selective activation of protein phosphatases and kinases by low and high levels of Ca 2+ . While the phosphatase substrates important for LTD have not yet been identified, it is possible that LTPSeveral recent observations indicate that postsynaptic glutamate receptors are dynamically regulated at excitatory synapses. Early insight into this process came from the finding that stimulation of some glutamatergic synapses generates no postsynaptic electrical signal when the postsynaptic cell is at its normal resting membrane potential ( Silent synapses are especially prevalent in development and have been found in many brain regions, including the hippocampus, cerebral cortex, and spinal cord. The silence of these synapses is evidently due to the voltage-dependent blockade of NMDA receptors by Mg 2+ (see text and Chapter 6). At the normal resting membrane potential, presynaptic release of glutamate evokes no postsynaptic response at such synapses because their NMDA receptors are blocked by Mg 2+ . However, depolarization of the postsynaptic neuron displaces the Mg 2+ , allowing glutamate release to induce postsynaptic responses mediated by NMDA receptors.Glutamate released at silent synapses evidently binds only to NMDA receptors. How, then, does glutamate release avoid activating AMPA receptors? One possibility is that glutamate released onto neighboring neurons diffuses to synapses on the neuron from which the electrical recording is being made. In this case, the diffusing glutamate may be present at concentrations sufficient to activate the high-affinity NMDA receptors, but not the low-affinity AMPA receptors. A second possibility is that a silent synapse has both AMPA and NMDA receptors, but its AMPA receptors are somehow not functional. Finally, some excitatory synapses may have only and LTD phosphorylate and dephosphorylate the same set of regulatory proteins to control the efficacy of transmission at the Schaeffer collateral-CA1 synapse. Just as LTP at this synapse is associated with insertion of AMPA receptors, LTD is often associated with a loss of synaptic AMPA receptors. This loss probably arises from internalization of AMPA receptors into the postsynaptic cell ( A somewhat different form of LTD is observed in the cerebellum (see Chapter 18). LTD of synaptic inputs onto cerebellar Purkinje cells was first described by Masao Ito and colleagues in Japan in the early 1980s. Purkinje neurons in the cerebellum receive two distinct types of excitatory input: climbing fibers and parallel fibers The initial basis of long-lasting forms of synaptic plasticity in the mammalian CNS, such as LTP and LTD, entails post-translational changes that lead to altered distribution or density of postsynaptic AMPA receptors. Studies in Aplysia, however, showed that while a short-term form of serotonininduced synaptic plasticity also has a post-translational origin, the long-term form of synaptic plasticity requires changes in gene expression (see In summary, behavioral plasticity requires activity-dependent synaptic changes that lead to changes in the functional connections within and among neural circuits. These changes in the efficacy and local geometry of connectivity provide a basis not only for learning, memory, and other forms of plasticity, but also some pathologies. Thus, abnormal patterns of neuronal activity, such as those that occur in epilepsy, can stimulate abnormal changes in synaptic connections that may further increase the frequency and severity of seizures (Box D). Despite the substantial advances in understanding the cellular and molecular bases of some forms of plasticity, how selective changes of synaptic strength encode memories or other complex behavioral modifications in the mammalian brain is simply not known.In addition to these cellular and molecular studies of synaptic plasticity, a good deal is now known about plasticity of adult cortical maps and of the receptive field properties of mature cortical neurons. Until the late 1970s, it was assumed that significant reorganization of cortical circuitry happened primarily during early postnatal development. This conclusion was based on the evidence for critical periods described in the preceding chapter, and on the relative permanence of neural deficits after CNS trauma in adults.This view has to some extent been modified by evidence that topographic maps in the somatic sensory cortex of adult monkeys are actually capable of appreciable reorganization. As described in Chapter 8, the four cortical areas that define the primate somatic sensory cortex (Brodmann's areas 3a, 3b, 1, and 2) each contain a complete topographic representation of the body surface. Jon Kaas and Michael Merzenich took advantage of this arrangement by carefully defining the normal spatial organization of topographic maps in these regions. They then amputated a digit (or cut one of the nerves that innervate the hand) and reexamined topographical maps in the same animals several weeks later. Surprisingly, the somatic sensory cortex had changed: The cortical neurons that had been deprived of their normal peripheral input now responded to stimulation of other parts of the animal's hand Appreciable changes in cortical representation also can occur in response to more physiological changes in sensory or motor experience. For instance, if a monkey is trained to use a specific digit for a particular task that is repeated many times, the functional representation of that digit determinedEpilepsy is a brain disorder characterized by periodic and unpredictable seizures mediated by the rhythmic firing of large groups of neurons. It seems likely that abnormal activity generates plastic changes in cortical circuitry that are critical to the pathogenesis of the disease.The importance of neuronal plasticity in epilepsy is indicated most clearly by an animal model of seizure production called kindling. To induce kindling, a stimulating electrode is implanted in the brain, often in the amygdala (a component of the limbic system that makes and receives connections with the cortex, thalamus, and other limbic structures, including the hippocampus; see Chapter 28). At the beginning of such an experiment, weak electrical stimulation, in the form of a low-amplitude train of electrical pulses, has no discernible effect on the animal's behavior or on the pattern of electrical activity in the brain (laboratory rats or mice have typically been used for such studies). As this weak stimulation is repeated once a day for several weeks, it begins to produce behavioral and electrical indications of seizures. By the end of the experiment, the same weak stimulus that initially had no effect now causes full-blown seizures. This phenomenon is essentially permanent; even after an interval of a year, the same weak stimulus will again trigger a seizure. Thus, repetitive weak activation produces long-lasting changes in the excitability of the brain that time cannot reverse. The word kindling is therefore quite appropriate: A single match can start a devastating fire.The changes in the electrical patterns of brain activity detected in kindled animals resemble those in human epilepsy. The behavioral manifestations of epileptic seizures in human patients range from mild twitching of an extremity to loss of consciousness and uncontrollable convulsions. Although many highly accomplished people have suffered from epilepsy (Alexander the Great, Julius Caesar, Napoleon, Dostoyevsky, and van Gogh, to name a few), seizures of sufficient intensity and frequency can obviously interfere with many aspects of daily life. Moreover, uncontrolled convulsions can lead to excitotoxicity (see Box D in Chapter 6). Up to 1% of the population is afflicted, making epilepsy one of the most common neurological problems.Modern thinking about the causes (and possible cures) of epilepsy has focused on where seizures originate and the mechanisms that make the affected region hyperexcitable. Most of the evidence suggests that abnormal activity in small areas of the cerebral cortex (called foci) provide the triggers for a seizure that then spreads to other synaptically connected regions. For example, a seizure originating in the thumb area of the right motor cortex will first be evident as uncontrolled movement of the left thumb that subsequently extends to other more proximal limb muscles, whereas a seizure originating in the visual association cortex of the right hemisphere may be heralded by complex hallucinations in the left visual field. The behavioral manifestations of seizures therefore provide important clues for the neurologist seeking to pinpoint the abnormal region of cerebral cortex.Epileptic seizures can be caused by a variety of acquired or congenital factors, including cortical damage from trauma, stroke, tumors, congenital cortical dysgenesis (failure of the cortex to grow properly), and congenital vascular malformations. One rare form of epilepsy, Rasmussen's encephalitis, is an autoimmune disease that arises when the immune system attacks the brain, using both humoral (i.e. antibodies) and cellular (lymphocytes and macrophages) agents that can destroy neurons. Some forms of epilepsy are heritable, and more than a dozen distinct genes have been demonstrated to underlie unusual types of epilepsy. However, most forms of familial epilepsy (such as juvenile myoclonic epilepsy and petit mal epilepsy) are caused by the simultaneous inheritance of more than one mutant gene.by electrophysiological mapping can expand at the expense of the other digits No effective prevention or cure exists for epilepsy. Pharmacological therapies that successfully inhibit seizures are based on two general strategies. One approach is to enhance the function of inhibitory synapses that use the neurotransmitter GABA; the other is to limit action potential firing by acting on voltage-gated Na + channels. Commonly used antiseizure medications include carbamazepine, phenobarbital, phenytoin (Dilantin ¬Æ ), and valproic acid. These agents, which must be taken daily, successfully inhibit seizures in 60-70% of patients. In a small fraction of patients, the epileptogenic region can be surgically excised. In extreme cases, physicians resort to cutting the corpus callosum to prevent the spread of seizures (most of the "split-brain" subjects described in Chapter 26 were patients suffering from intractable epilepsy). One of the major reasons for controlling epileptic activity is to prevent the more permanent plastic changes that would ensue as a consequence of abnormal and excessive neural activity. Electroencephalogram (EEG) recorded from a patient during a seizure. The traces show rhythmic activity that persisted much longer than the duration of this record. This abnormal pattern reflects the synchronous firing of large numbers of cortical neurons. (The designations are various positions of electrodes on the head; see Box C in Chapter 27 for additional information about EEG recordings.) These various observations on adult plasticity indicate that normal experience can alter the strength of existing synapses and even elicit some local remodeling of synapses and circuits. More extensive growth and remodeling are stimulated by nervous system injury. As just noted, however, this remodeling rarely results in full restoration of lost function.Traumatic injury, interruption of blood supply, and degenerative diseases all can damage axons in peripheral nerves, or neuronal cell bodies and syn- The map of the digits in the primary somatic sensory cortex prior to training is shown. After several months of "practice," a larger region of the cortex contained neurons activated by the digits used in the task. Note that the specific arrangements of the digit representations are somewhat different from the monkey shown in In contrast, damage to axonal tracts in the adult CNS triggers a very different set of changes. First, the relative distances for specific growth are far longer than they were in the developing brain and spinal cord. Moreover, as axons and their myelin sheaths break down, the remnants are not cleared efficiently and can persist for many weeks, posing a substantial impediment to regeneration. This inhibition appears to reflect the activity of inhibitory signals produced by glia and other cells at the site of injury, including a protein called Nogo that blocks axon extension by interacting with advancing growth cones (see Chapter 22). Nogo is produced primarily by oligodendrocytes, the glia that normally form myelin sheaths around CNS axons. The contributions of Nogo to axon regeneration remains unclear. Blocking its function with specific antibodies can enhance growth of axons in the mature injured CNS; however, genetic inactivation of Nogo (or its receptor) in mice does not result in significantly enhanced axon regeneration in the CNS following injury.To make matters worse, astrocytes reacting to CNS injury express additional inhibitors of axon extension, and the cytokines released by microglia or macrophages as part of the inflammatory response to injury also diminish axon growth. As a consequence, even if a central neuron initiates a genetic program for regeneration, growth cones emerging from the site of a lesion in the adult CNS encounter an array of circumstances that impede continued growth and reestablishment of connections.The contributions of the intrinsic capacity for growth in mature CNS neurons versus the local axonal environment in CNS regeneration was explored in detail by Albert Aguayo and his co-workers at McGill University in the 1980s. They grafted segments of peripheral nerve into sites in the CNS, such as optic nerve, spinal cord, or other locations, and then determined whether neurons were able to regenerate axons through the peripheral grafts. Their studies showed that at least some CNS axons are able to take advantage of the more supportive growth environment of the peripheral nerve, regenerating over distances of many centimeters and in some cases restoring appropriate synaptic connections (Box E).This demonstration that CNS axons can sometimes regenerate successfully into a peripheral nerve graft sparked intensive efforts by many labs to produce a similarly supportive environment for axon growth within the long tracts of the brain or spinal cord. For example, Martin Schwab and his collaborators showed that implanting cells engineered to secrete antibodies against inhibitory proteins, including Nogo, alleviated some of the inhibitory properties of CNS myelin and other cells at the site of axon injury in experimental animals. Another approach was to introduce cells that provide a more supportive environment for regenerating axons in the damaged CNS. Schwann cells, neural stem cells (see next section), and specialized glial cells from the olfactory nerve all can be grown in tissue culture and introduced into the brains or spinal cords of experimental animals, where they modestly improve axon regrowth and, in some cases, may contribute to limited functional recovery.In short, regeneration in adults is held in check by ongoing suppression of genes required for effective axon elongation. Injury to the peripheral nervous system readily induces expression of this genetic program, while interruption of mammalian CNS axons does not. Once CNS neurons have activated these genes, in principle regrowth could be enhanced by removal or neutralization of inhibitory molecules, minimizing local inflammatory responses, and by the introduction of cells that provide a more supportive growth environment. These strategies, however, have not been proven clinically useful, and functional loss after brain and spinal cord injury remains a daunting clinical challenge.It has long been known that mature, differentiated neurons do not divide (see Chapter 21). It does not follow, however, that all the neurons in the adult brain are produced during embryonic development, even though this interpretation has generally been assumed. The merits of this assumption were initially challenged in the 1960s, in experiments indicating that interneurons in a variety of brain regions could be labeled with tritiated thymidine injected in the adult, rather than during early development. This finding suggested that some interneurons-particularly in the olfactory bulb and hippocampus-are generated in the mature rather than in the developing animal. Moreover, a variety of experiments in fish, frogs, and birds indicated a limited generation of new neurons throughout life in these species, especially in animals (like goldfish) where there is significant continuing growth of the entire organism throughout the course of its life. In songbirds, new neurons are able to extend dendrites, generate synaptic and action potentials, and project long axons to establish appropriate connections with other brain nuclei. Production of new neurons is apparent in many parts of the birds' brains, but seems especially prominent in areas involved in song production (see Box B in Chapter 23). These observations showed that the adult brain can generate at least some new nerve cells and incorporate them into neural circuits (see also Chapter 14).The production of new neurons in the mammalian adult brain has now been examined (or re-examined) in mice, rats, monkeys, and humans. In all these cases, new nerve cells in the CNS have been restricted to just two regions of the brain: (1) The granule cell layer of the olfactory bulb; and (2) the dentate gyrus of the hippocampus If differentialted neurons cannot divide (see Chapter 21), how does the adult brain generate these nerve cells? The answer emerged with the discovery that the subventricular zone (a population of cells adjacent to the ventricular space found in the cortical hemispheres and hippocampus that pro-The central nervous system of adult mammals, including humans, recovers only poorly from injury. As indicated in the text, once severed, major axon tracts (such as those in the spinal cord) never regenerate. The devastating consequences of these injuries-e.g., loss of movement and the inability to control basic bodily functions-has led many neuroscientists to seek ways of restoring the connections of severed axons. There is no a priori reason for this biological failure, since "lower" vertebrates-e.g., lampreys, fish, and frogs-can regenerate a severed spinal cord or optic nerve. Even in mammals, the inability to regenerate axonal tracts is a special failing of the central nervous system; peripheral nerves can and do regenerate in adult animals, including humans. Why, then, not the central nervous system?At least a part of the answer to this puzzle apparently lies in the molecular cues that promote and inhibit axon outgrowth. In mammalian peripheral nerves, axons are surrounded by a basement membrane (a proteinaceous extracellular layer composed of collagens, glycoproteins, and proteoglycans) secreted in part by Schwann cells, the glial cells associated with peripheral axons. After a peripheral nerve is crushed, the axons within it degenerate; the basement membrane around each axon, however, persists for months. One of the major components of the basement membrane is laminin, which (along with other growthpromoting molecules in the basement membrane) forms a hospitable environment for regenerating growth cones. The surrounding Schwann cells also react by releasing neurotrophic factors, which further promote axon elongation (see text). This peripheral environment is so favorable to regrowth that even neurons from the central nervous system can be induced to extend into transplanted segments of peripheral nerve. Albert Aguayo and his colleagues at the Montreal General Hospital found that grafts derived from peripheral nerves can act as "bridges" for central neurons (in this case, retinal ganglion cells), allowing them to grow for over a centimeter At present there is only one modestly helpful treatment for CNS injuries such as spinal cord transection. High doses of a steroid, methylprednisolone, immediately after the injury prevents some of the secondary damage to neurons resulting from the initial trauma. Although it may never be possible to fully restore function after such injuries, enhancing axon regeneration, blocking inhibitory molecules and providing additional trophic support to surviving neurons could in principle allow sufficient recovery of motor control to give afflicted individuals a better quality of life than they now enjoy. The best "treatment," however, is to prevent such injuries from occurring, since there is now very little that can be done after the fact. Why the generation of neurons is so restricted in the adult brain is not understood. Nevertheless, the fact that new neurons can be generated in at least a few regions of the adult brain shows that this phenomenon can occur in the adult CNS. The ability of newly generated neurons to integrate into some synaptic circuits adds to the available mechanisms for plasticity in the adult brain. Thus, many investigators have begun to explore the potential use of stem cells for the repair of circuits damaged by traumatic injury or degenerative disease. The adult nervous system exhibits plastic change in a variety of circumstances. Studies of behavioral plasticity in several invertebrates and of the neuromuscular junction suggest that modification of synaptic strength is responsible for much of the ongoing change in synaptic function in adults. Synapses exhibit many forms of plasticity that occur over a broad temporal range. At the shortest times (seconds to minutes), facilitation, post-tetanic potentiation, and depression provide rapid but transient modifications based on alterations in Ca 2+ signaling and synaptic vesicle pools at recently active synapses. Longer-lasting forms of synaptic plasticity such as LTP and LTD are also based on Ca 2+ and other intracellular second messengers. In these more enduring forms of plasticity, protein phosphorylation and changes in gene expression greatly outlast the period of synaptic activity and can yield persistent changes in synaptic strength (hours to days or longer). Different brain regions evidently use one or more of these strategies to learn new behaviors and acquire new memories. Neuronal damage can also induce plastic changes. Peripheral neurons can regenerate axons following damage, though the capacity of CNS axons to regenerate is severely limited. In addition, neural stem cells are present in certain regions of the adult brain, allowing the production of some new neurons in a few brain regions. These various forms of adult plasticity can modify the function of the mature brain and provide some hope for improving the limited ability of the CNS to recover successfully from trauma and neurological disease. The awareness of physical and social circumstances, the ability to have thoughts and feelings (emotions), to be sexually attracted to others, to express these things to our fellow humans by language, and to store such information in memory certainly rank among the most intriguing functions of the human brain. Given their importance in daily life-and for human culture generally-it is not surprising that much of the human brain is devoted to these and other complex mental functions. The intrinsic interest of these aspects of human behavior is unfortunately equaled by the difficulty-both technical and conceptual-involved in unraveling their neurobiological underpinnings. Nonetheless, a good deal of progress has been made in deciphering the structural and functional organization of the relevant brain regions. Especially important has been the steady accumulation of case studies during the last century or more that, by the signs and symptoms resulting from damage to specific brain regions, have indicated much about the primary location of various complex brain functions. More recently, the advent of noninvasive brain imaging techniques has provided a much deeper understanding of some of these abilities in normal human subjects as well as in neurological patients. Finally, complementary electrophysiological experiments in nonhuman primates and other experimental animals have begun to elucidate the cellular correlates of many of these functions. Taken together, these observations have established a rapidly growing body of knowledge about these more complex aspects of the human brain. This general domain of investigation has come to be called "cognitive neuroscience," a field that promises to loom ever larger in the new century.The association cortices include most of the cerebral surface of the human brain and are largely responsible for the complex processing that goes on between the arrival of input in the primary sensory cortices and the generation of behavior. The diverse functions of the association cortices are loosely referred to as cognition, which literally means the process by which we come to know the world. ("Cognition" is perhaps not the best word to indicate this wide range of neural functions, but it has become part of the working vocabulary of neurologists and neuroscientists.) More specifically, cognition refers to the ability to attend to external stimuli or internal motivation; to identify the significance of such stimuli; and to make meaningful responses. Given the complexity of these tasks, it is not surprising that the association cortices receive and integrate information from a variety of sources, and that they influence a broad range of cortical and subcortical targets. Inputs to the association cortices include projections from the primary and secondary sensory and motor cortices, the thalamus, and the brainstem. Outputs from the association cortices reach the hippocampus, the basal ganglia and cerebellum, the thalamus, and other association cortices. Insight into the function of these cortical regions has come primarily from observations of human patients with damage to one or another of these areas. Noninvasive brain imaging of normal subjects, functional mapping at neurosurgery, and electrophysiological analysis of comparable brain regions in non-human primates have generally confirmed clinical deductions. Together, these studies indicate that, among other functions, the parietal association cortex is especially important for attending to stimuli in the external and internal environment, that the temporal association cortex is especially important for identifying the nature of such stimuli, and that the frontal association cortex is especially important for planning appropriate behavioral responses.The preceding chapters have considered in some detail the parts of the brain responsible for encoding sensory information and commanding movements (i.e., the primary sensory and motor cortices). But these regions account for only a fraction (perhaps a fifth) of the cerebral cortex tices in the parietal, temporal, and frontal lobes that make cognition possible.(The extrastriate cortex of the occipital lobe is equally important in cognition; its functions, however, are largely concerned with vision, and much of what is known about these areas has been discussed in Chapter 11.) These other areas of the cerebral cortex are referred to collectively as the association cortices (see Before delving into a more detailed account of the functions of these cortical regions, it is important to have a general understanding of cortical structure and the organization of its canonical circuitry. Most of the cortex that covers the cerebral hemispheres is neocortex, defined as cortex that has six cellular layers, or laminae. Each layer comprises more or less distinctive populations of cells based on their different densities, sizes, shapes, inputs, and outputs. The laminar organization and basic connectivity of the human cerebral cortex are summarized in These generalizations notwithstanding, the connectivity of the association cortices is appreciably different from primary and secondary sensory and motor cortices, particularly with respect to inputs and outputs. For instance, two thalamic nuclei that are not involved in relaying primary motor or sensory information provide much of the subcortical input to the association cortices: the pulvinar projects to the parietal association cortex, while the medial dorsal nuclei project to the frontal association cortex. Several other thalamic nuclei, including the anterior and ventral anterior nuclei, innervate the association cortices as well. Unlike the thalamic nuclei that receive peripheral sensory information and project to primary sensory cortices, the input to these association cortexprojecting nuclei comes from other regions of the cortex. In consequence, the signals coming into the association cortices via the thalamus reflect sensory and motor information that has already been processed in the primary sensory and motor areas of the cerebral cortex, and is being fed back to the association regions. The primary sensory cortices, in contrast, receive thalamic information that is more directly related to peripheral sense organs (see, for example, Chapter 8). Similarly, much of the thalamic input to primary motor cortex is derived from the thalamic nuclei related to the basal ganglia and cerebellum rather than to other cortical regions (see Unit III).A second major difference in the sources of innervation to the association cortices is their enrichment in direct projections from other cortical areas, called corticocortical connections (see Another important source of innervation to the association areas is subcortical, arising from the dopaminergic nuclei in the midbrain, the noradren- Much knowledge about the cerebral cortex is based on descriptions of differences in cell number and density throughout the cortical mantle. Nerve cell bodies, because of their high metabolic rate, are rich in basophilic substances (RNA, for instance), and therefore tend to stain darkly with reagents such as cresyl violet acetate. These Nissl stains (named after F. Nissl, who first described this technique when he was a medical student in nineteenth-century Germany) provide a dramatic picture of brain structure at the histological level.The most striking feature revealed in this way is the distinctive lamination of the cortex in humans and other mammals, as seen in the figure. In humans, there are three to six cortical layers, which are usually designated by roman numerals, with letters for laminar subdivisions (layers IVa, IVb, and IVc in the visual cortex, for example). Each of the cortical laminae in the socalled neocortex (which covers the bulk of the cerebral hemispheres and is defined by six layers) has characteristic functional and anatomical features (see Not all of the cortical mantle is sixlayered neocortex. The hippocampus, for example, which lies deep in the temporal lobe and has been implicated in acquisition of declarative memories (see Chapter 30), has only three or four laminae. The hippocampal cortex is regarded as evolutionarily more primitive, and is therefore called archicortex to distinguish it from the six-layered neocortex. Another, presumably more primitive, type of cortex, called paleocortex (paleo = ancient), generally has three layers and is found on the ventral surface of the cerebral hemispheres and along the parahippocampal gyrus in the medial temporal lobe.The functional significance of different numbers of laminae in neocortex, archicortex, and paleocortex is not known, although it seems likely that the greater number of layers in neocortex reflects more complex information processing than in archi-or paleocortex. The general similarity of neocortical structure across the entire cerebrum clearly suggests that there is a common denominator of cortical operation, although no one has yet deciphered what it is. ergic and serotonergic nuclei in the brainstem reticular formation, and cholinergic nuclei in the brainstem and basal forebrain. These diffuse inputs project to different cortical layers and, among other functions, determine mental state along a continuum that ranges from deep sleep to high alert (see Chapter 27).The general wiring plan for the association cortices is summarized in In 1941, the British neurologist W. R. Brain reported three patients with unilateral parietal lobe lesions in whom the primary problem was varying degrees of attentional difficulty. Brain described their peculiar deficiency in the following way:Though not suffering from a loss of topographical memory or an inability to describe familiar routes, they nevertheless got lost in going from one room to another in their own homes, always making the same error of choosing a right turning instead of a left, or a door on the right instead of one on the left. In each case there was a massive lesion in the right parieto-occipital region, and it is suggested that this ‚Ä¶ resulted in an inattention to or neglect of the left half of external space.The patient who is thus cut off from the sensations which are necessary for the construction of a body scheme may react to the situation in several different ways. He may remember that the limbs on his left side are still there, or he may periodically forget them until reminded of their presence. He may have an illusion of their absence, i.e. they may 'feel absent' although he knows that they are there; he may believe that they are absent but allow himself to be convinced by evidence to the contrary; or, finally, his belief in their absence may be unamenable to reason and evidence to the contrary and so constitute a delusion. W. R. Interestingly, patients with contralateral neglect are not simply deficient in their attentiveness to the left visual field, but to the left sides of objects generally. For example, when asked to cross out lines distributed throughout the visual field, contralateral neglect patients, as expected, tend to bisect more lines on the right side of the field than on the left, consistent with a disruption in attentiveness to the left visual field (see Disruptions in spatial frames of reference are also associated with lesions of the parietal cortex that are more dorsal and medial than those typically associated with classical neglect. Such damage often presents as a triad of visuospatial deficits known as Balint's syndrome (named after an AustrianHungarian neurologist). These three signs are: an inability to perceive parts of a complex visual scene as a whole (called simultanagnosia); deficits in visually guided reaching (optic ataxia); and difficulty in voluntary scanning of visual scenes (ocular apraxia). In contrast to classical neglect, optic ataxia and ocular apraxia typically remit when movements are guided by non-visual cues. These observations suggest that the parietal cortex participates in the construction of spatial representations that can guide both attention and movement. Clinical evidence from patients with lesions of the association cortex in the temporal lobe indicates that one of the major functions of this part of the brain is the recognition and identification of stimuli that are attended to, particularly complex stimuli. Thus, damage to either temporal lobe can result in difficulty recognizing, identifying, and naming different categories of objects. These disorders, collectively called agnosias (from the Greek for "not knowing"), are quite different from the neglect syndromes. As noted, patients with right parietal lobe damage often deny awareness of sensory information in the left visual field (and are less attentive to the left sides of objects generally), despite the fact that the sensory systems are intact (an individual with contralateral neglect syndrome typically withdraws his left arm in response to a pinprick, even though he may not admit the arm's existence). Patients with agnosia, on the other hand, acknowledge the presence of a stimulus, but are unable to report what it is. These latter disorders have both a lexical aspect (a mismatching of verbal or other cognitive symbols with sensory stimuli; see Chapter 26) and a mnemonic aspect (a failure to recall stimuli when confronted with them again; see Chapter 30).One of the most thoroughly studied agnosias following damage to the temporal association cortex in humans is the inability to recognize and identify faces. This disorder, called prosopagnosia (prosopo, from the Greek for "face" or "person"), was recognized by neurologists in the late nineteenth century and remains an area of intense investigation. After damage to the inferior temporal cortex, typically on the right, patients are often unable to identify familiar individuals by their facial characteristics, and in some cases cannot recognize a face at all. Nonetheless, such individuals are perfectly aware that some sort of visual stimulus is present and can describe particular aspects or elements of it without difficulty.An example is the case of L.H., a patient described by the neuropsychologist N. L. Etcoff and colleagues. (The use of initials to identify neurological patients in published reports is standard practice.) This 40-year-old minister and social worker had sustained a severe head injury as the result of an automobile accident when he was 18. After recovery, L.H. could not recognize familiar faces, report that they were familiar, or answer questions about faces from memory. He was nonetheless able to lead a fairly normal and productive life. He could still identify other common objects, could discriminate subtle shape differences, and could recognize the sex, age, and even the "likability" of faces. Moreover, he could identify particular people by non-facial cues such as voice, body shape, and gait. The only other category of visual stimuli he had trouble recognizing was animals and their expressions, though these impairments were not as severe as for human faces. Noninvasive brain imaging showed that L.H.'s prosopagnosia was the result of damage to the right temporal lobe.More recently, imaging studies in normal subjects have confirmed that the inferior temporal cortex mediates face recognition and that nearby regions are responsible for categorically different recognition functions In general, lesions of the right temporal cortex lead to agnosia for faces and objects, whereas lesions of the corresponding regions of the left temporal cortex tend to result in difficulties with language-related material. (Recall that the primary auditory cortex is on the superior aspect of the temporal lobe; as described in the following chapter, the cortex adjacent to the auditory cortex in the left temporal lobe is specifically concerned with language.)The lesions that typically cause recognition deficits are in the inferior temporal cortex in or near the so-called fusiform gyrus; those that cause languagerelated problems in the left temporal lobe tend to be on the lateral surface of the cortex. Consistent with these conclusions, direct cortical stimulation in subjects whose temporal lobes are being mapped for neurosurgery (typically removal of an epileptic focus) may have a transient prosopagnosia as a consequence of this abnormal activation of the relevant regions of the right temporal cortex.Prosopagnosia and related agnosias involving objects are specific instances of a broad range of functional deficits that have as their hallmark the inability to recognize a complex sensory stimulus as familiar, and to identify and name that stimulus as a meaningful entity in the environment. Depending on the laterality, location, and size of the lesion in temporal cortex, agnosias can be as specific as for human faces, or as general as an inability to name most familiar objects.The functional deficits that result from damage to the human frontal lobe are diverse and devastating, particularly if both hemispheres are involved. This broad range of clinical effects stems from the fact that the frontal cortex has a wider repertoire of functions than any other neocortical region (consistent with the fact that the frontal lobe in humans and other primates is the largest of the brain's lobes and comprises a greater number of cytoarchitectonic areas).The particularly devastating nature of the behavioral deficits after frontal lobe damage reflects the role of this part of the brain in maintaining what is normally thought of as an individual's "personality." The frontal cortex inte- grates complex perceptual information from sensory and motor cortices, as well as from the parietal and temporal association cortices. The result is an appreciation of self in relation to the world that allows behaviors to be planned and executed normally. When this ability is compromised, the afflicted individual often has difficulty carrying out complex behaviors that are appropriate to the circumstances. These deficiencies in the normal ability to match ongoing behavior to present or future demands are, not surprisingly, interpreted as a change in the patient's "character."The case that first called attention to the consequences of frontal lobe damage was that of Phineas Gage, a worker on the Rutland and Burlington Railroad in mid-nineteenth-century Vermont. In that era, the conventional way of blasting rock was to tamp powder into a hole with a heavy metal rod. Gage, the popular and respected foreman of the crew, was undertaking this procedure one day in 1848 when his tamping rod sparked the powder, setting off an explosion that drove the rod, which was about a meter long and 4 or 5 centimeters in diameter, through his left orbit (eye socket), destroying much of the frontal part of his brain in the process (see the illustration on page 612). Gage, who never lost consciousness, was promptly taken to a local doctor who treated his wound. An infection set in, presumably destroying additional frontal lobe tissue, and Gage was an invalid for several months. Eventually he recovered and was to outward appearances well again. Those who knew Gage, however, were profoundly aware that he was not the "same" individual that he had been before. A temperate, hardworking, and altogether decent person had, by virtue of this accident, been turned into an inconsiderate, intemperate lout who could no longer cope with normal social intercourse or the kind of practical planning that had allowed Gage the social and economic success he enjoyed before.The physician who looked after Gage until his death in 1863 summarized his impressions of Gage's personality as follows: Another classic case of frontal lobe deficits was that of a patient followed for many years by the neurologist R. M. Brickner during the 1920s and 30s. Joe A., as Brickner referred to his patient, was a stockbroker who at age 39 underwent bilateral frontal lobe resection because of a large tumor. After the operation, Joe A. had no obvious sensory or motor deficits; he could speak and understand verbal communication and was aware of people, objects, and temporal order in his environment. He acknowledged his illness and retained a high degree of intellectual power, as judged from an ongoing ability to play an expert game of checkers. Nonetheless, Joe A.'s personality had undergone a dramatic change. This formerly restrained, modest man became boastful of professional, physical, and sexual prowess, showed littleThe consequences of frontal lobe destruction have been all too well documented by a disturbing yet fascinating episode in twentieth-century medical practice. During the period from 1935 through the 1940s, neurosurgical destruction of the frontal lobe (frontal lobotomy or leukotomy) was a popular treatment for certain mental disorders. More than 20,000 of these procedures were performed, mostly in the United States.Enthusiasm for this approach to mental disease grew from the work of Egas Moniz, a respected Portuguese neurologist, who, among other accomplishments, did pioneering work on cerebral angiography before becoming the leading advocate of psychosurgery. Moniz recognized that the frontal lobes were important in personality structure and behavior, and concluded that interfering with frontal lobe function might alter the course of mental diseases such as schizophrenia and other chronic psychiatric disorders. He also recognized that destroying the frontal lobe would be relatively easy to do and, with the help of Almeida Lima, a neurosurgical colleague, introduced a simple surgical procedure for indiscriminately destroying most of the connections between the frontal lobe and the rest of the brain (see In the United States, the neurologist Walter Freeman at George Washington University School of Medicine, in collaboration with neurosurgeon James Watts, became an equally strong advocate of this approach. Freeman devoted his life to treating a wide variety of mentally disturbed patients in this way. He popularized a form of the procedure that could be carried out under local anesthesia and traveled widely across the United States to demonstrate this technique and encourage its use.Although it is easy in retrospect to be critical of this zealotry in the absence of either evidence or sound theory, it is important to remember that effective psychotropic drugs were not then available, and patients suffering from many of the disorders for which leukotomies were done were confined under custodial conditions that were at best dismal, and at worst brutal. Rendering a patient relatively tractable, albeit permanently altered in personality, no doubt seemed the most humane of the difficult choices that faced psychiatrists and others dealing with such patients in that period.With the advent of increasingly effective psychotropic drugs in the late 1940s and the early 1950s, frontal lobotomy as a psychotherapeutic strategy rapidly disappeared, but not before Moniz was awarded the Nobel Prize for Physiology or Medicine in 1949. The history of this instructive episode in modern medicine has been compellingly told by Eliot Valenstein, and his book on the rise and fall of psychosurgery should be read by anyone contemplating a career in neurology, neurosurgery, or psychiatry. restraint in conversation, and was unable to match the appropriateness of what he said to his audience. Like Gage, his ability to plan for the future was largely lost, as was much of his earlier initiative and creativity. Even though he retained the ability to learn complex procedures, he was unable to return to work and had to rely on his family for support and care.The effects of widespread frontal lobe damage documented by these case studies encompass a wide range of cognitive disabilities, including impaired restraint, disordered thought, perseveration (i.e., repetition of the same behavior), and the inability to plan appropriate action. Recent studies of patients with focal damage to particular regions of the frontal lobe also suggest that some of the processes underlying these deficits may be localized anatomically, with working memory functions (see Chapter 30) situated more dorsolaterally and planning and social restraint functions located more ventromedially. Some of these functions can be clinically assessed using standardized tests such as the Wisconsin Card Sorting Task for planning (see Box C), the delayed response task for working memory, and the "go-nogo" task for inhibition of inappropriate responses. All these observations are consistent with the idea that the common denominator of the cognitive functions subserved by the frontal cortex is the selection, planning, and execution of appropriate behavior, particularly in social contexts.Sadly, the effects of damage to the frontal lobes have also been documented by the many thousands of frontal lobotomies ("leukotomies") performed in the 1930s and 40s as a means of treating mental illness (Box B). The rise and fall of this "psychosurgery" provides a compelling example of the frailty of human judgment in medical practice, and of the conflicting approaches of neurologists, neurosurgeons, and psychiatrists in that era to the treatment of mental disease.These clinical and pathological observations clearly indicate distinct cognitive functions for the parietal, temporal, and frontal lobes. They do not, however, provide much insight into how the nervous system represents this information in nerve cells and their interconnections. The apparent functions of the association cortices implied by clinical observations stimulated a number of informative electrophysiological studies in non-human primates, particularly macaque (usually rhesus) monkeys.As in humans, a wide range of cognitive abilities in monkeys are mediated by the association cortices of the parietal, temporal, and frontal lobes An example is neurons apparently related to the attentive functions of the parietal cortex. These particular studies of cellular electrophysiology and behavior take advantage of the fact that monkeys can be trained to selectively attend to particular objects or events and report their experience in a variety of nonverbal ways, typically by looking at a response target or manipulating a joystick. Thus, attention-sensitive neurons can be identified by changes in neuronal activity associated with simultaneous changes in the attentive behavior of the animal. As might be expected from the clinical evidence in humans, neurons in specific regions of the parietal cortex of the rhesus monkey are activated when the animal attends to a target but not when the same stimulus is ignored In another study, monkeys were rewarded with different amounts of fruit juice (a highly desirable treat) for attending to each of a pair of simultaneously illuminated targets In keeping with human deficits of recognition following temporal lobe lesions, neurons with responses that correlate with the recognition of specific The animal is seated in a chair and gently restrained. Several weeks before data collection begins, a recording well is placed through the skull using a sterile surgical technique. For electrophysiological recording experiments, a tungsten microelectrode is inserted through the dura and arachnoid, and into the cortex. The screen and the response bar in front of the monkey are for behavioral testing. In this way, individual neurons can be monitored while the monkey performs specific cognitive tasks. stimuli are present in the temporal cortex of rhesus monkeys In principle, it is unlikely that such "face cells" are tuned to specific faces or objects, and no cells have so far been found that are selective for a particular face. However, it is not hard to imagine that populations of neurons differently responsive to various features of faces or other objects could act in concert to enable the recognition of such complex sensory stimuli. In fact, recent studies Although the baseline level of activity of the neuron being studied here remains unchanged when the monkey ignores a visual target (left), firing rate increases dramatically when the monkey attends to the same stimulus (right). The histograms indicate action potential frequency per unit time. (C) When given a choice of where to attend, the monkey pays increasing attention to a particular visual target when more fruit juice reward can be expected for doing so (left), and the firing rate of a parietal neuron under study increases accordingly In confirmation of the human clinical evidence about the function of the frontal association cortices, neurons that appear to be specifically involved in planning have been identified in the frontal cortices of rhesus monkeys. The behavioral test used to study cells in the monkey frontal cortex is called the delayed response task V