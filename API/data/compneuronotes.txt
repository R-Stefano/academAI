Copyright Office of Scientific and Technical InformationThese notes have three main objectives: to present the major concepts in the field of computational neuroscience, to present the basic mathematics that underlies these concepts, and to give the reader some idea of common approaches taken by computational neuroscientists when combining and. Most books on computational neuroscience take one of two approaches. In the first approach, the text is designed for computational students with an interest in neuroscience. A reader must already have significant mathematical knowledge in order to comfortably read the text. In the other approach, the text is designed for a broad audience and the mathematics is separated from the presentation of the biology. The main narrative focuses on the underlying concepts discussed within the field of computational neuroscience. Details regarding the necessary mathematical definitions and concepts are reserved for appendices, presented either at the end of the book or at the end of each chapter. The basic thinking behind this approach is that the key contributions of computational neuroscience are conceptual, and do not rely on a deep understanding of the underlying mathematics. Separating the math allows the ideas to be presented to a wide audience, many of whom do not have extensive computational training. If some readers are interested in the mathematical details, they can find them in the appendices.In contrast to these approaches, these notes present the neuroscience and mathematical concepts simultaneously. Why The primary reason is the following. I believe that the most important insights gained from applying computational techniques to understanding the nervous system result from the process of translating biological facts into mathematical facts and vice versa. To convey the value of translating ideas back and forth, it is crucial that the math and the neuroscience be presented together. However, an integrated presentation brings with it a host of difficulties. For example, the notes must be presented in two competing voices -the best presentation of the underlying concepts does not always coincide with the best presentation of the mathematics. Therefore the notes sometimes jump from sections that make conceptual sense to those that make mathematical sense and back again. At each of these transitions, there is always a danger of losing the reader. A second major disadvantage is that the amount of mathematics presented must be severely limited. These notes have been written for a one semester course in computational neuroscience. It would be foolish to try to cram 2-3 semesters of college mathematics on top of the conceptual material to be presented.Overcoming these difficulties requires a degree of discipline on the part of the reader, particularly those readers that have a limited mathematical background. First and foremost, be patient The clearest way to present mathematical concepts is often very abstract. One begins by defining the mathematical objects to be studied, and then goes on to describe the operations to be performed on these objects. These definitions and operations are often confusing at first, since it is only after working some problems and examples that the key concepts become clear. Thus, contrary to the way it appears in text books, mathematics is almost always learned in cycles. It is natural for concepts to be unclear at first clarity usually comes only after revisiting ideas multiple times. This is why working problems is crucial for learning mathematical concepts. In working through a problem set, ideas get applied to a number of examples, requiring them to be revisited a number of times.A second admonition: if your understanding of the mathematics presented in these notes is less than rock solid, dont worry Im not expecting you to learn to pass the final exam in a linear algebra course or to know the ins and outs of information theory. Im only expecting you to learn enough of the mathematics to understand what is going on, not necessarily how to do it yourself. I cannot emphasize this point strongly enough. When dealing with mathematical concepts there often seems to be an expectation that anything other than a crystal clear understanding is somehow a failure. I encourage students to judge their success in this course relative to the completeness of their knowledge in other survey courses. For example, one hardly expects to remember everything from the mass of facts presented in a typical introductory neuroscience course.That said, I am expecting people to do some mathematics. While it is possible to grasp many of the key concepts in computational neuroscience after only a hand-waving explanation of the underlying mathematics, how these concepts are shaped by their mathematical roots would remain hidden. Since it is important to appreciate the limitations of computational neuroscience as well as its strengths, one has to grapple with some mathematics.In an attempt to organize the material and present it from multiple points of view, I have subdivided the material in a number of ways. The main narrative of the notes is organized according to the competing requirements of presenting the mathematics and the neuroscience.To help keep the reader on track, information that isnt strictly necessary to the main narrative has been separated out into "examples" and "asides." Working through the examples is crucial for adding some flesh to the skeleton of the main ideas. Asides are included to flesh out the presentation even more, although some of them might be considered excess fat by some. Most examples carry the label "biological," "mathematical," or "network." Network examples tend to be in between the other two categories -theyre not particularly close to the biology nor do they illustrate purely mathematical idea. These categories are rather loose. Asides are either "biological," "notational," or "historical." I have a particular fondness for the historical asides, since I feel it is important to get a sense of the historical arc of the main ideas.Since formulating precise definitions is one of the most important aspects of mathematics, important definitions are separated from the main text and given a number. Crucial bits of mathematics are separated into subsections on "Mathematical Formalism." These sections allow the presentation of important mathematical concepts using a more formal mathematical style. Finally, all important terms are presented in bold the first time they are used.At the end of most sections, I have included a number of problems and "key concepts." Key concepts are mostly just pointers to the parts of the section that are most important for understanding the main ideas. These can be used as a quick review, or as starting points for class discussion -if you dont understand one of the key concepts, you should definitely ask for clarification Problems come in two types. Roughly half have been marked "" for easy. If the problem seems obvious and you think I must be asking for something deeper, youre wrong. These are meant to check that you really did understand the definition in the first place. The other problems should be a bit more challenging and are meant to give you some practice with the main ideas. The problems are a bit uneven and there probably should be more of them. Writing good problems is one of the most difficult aspects of writing a text book.Much of the above is what Id like to have happen. These notes will fall short of these ideals, particularly as the semester wears on. Well see how I can keep up. And speaking of text books, there is a possibility that something based on these notes may be published some day. If you could read them with a pen in hand and make editorial comments that would be much appreciated. Im particularly interested in hearing about sections that seem confusing or that could be fleshed out a bit more. Any feedback is appreciated.Finally, these notes assume a working knowledge of basic neuroscience -essentially the material covered in NACS 641 or a similar systems neuroscience course. If you feel that any of the biology needs more explanation, see some of the references below or ask me.Good question Using computers to simulate and model brain function. Applying techniques from computational fields to understand the brain. Trying to understand the computations performed by the brain.The term computational neuroscience covers a dizzying array of approaches to understanding the nervous system, and to achieve any coherence in constructing a course called "computational neuroscience" requires vast subsections of the field to be excluded. A number of different schemes have been used to divide up the field. Many of these fall under the phrase "levels of analysis." One division according to level of analysis is biological: the brain can be studied at a hierarchy of scales ranging from the cellular and molecular level to the level of small localized circuits in the brain to the level of large-scale brain circuits involving multiple neural subsystems A second class of scheme is that proposed by David Yet another scheme that falls under the "levels of analysis" rubric is the distinction between "top-down" and "bottom-up" approaches to understanding the brain. A top-down approach starts at the level of cognitive phenomena and tries to reach "down" to connect these phenomena to specific events taking place in the brain. The bottom-up approach starts with biological knowledge about brain cells and circuits and tries to determine how these mechanisms support complex mental phenomena. Each of these schemes for subdividing the field has advantages and disadvantages. In many cases, they can be brought into rough alignment. The topics addressed by these notes, can be best localized in terms of the biology hierarchy -almost all the topics explored fall at the level of neurons or local circuits. The notes are generally "middle-out" although they probably fall closer to the bottom-up rather than the top-down approaches. A truly complete survey of computational neuroscience would probably treat two separate clusters of ideas that are dealt with here in only a cursory manner. The first tradition traces its roots back to by far the most successful model in all of neuroscience: the Hodgkin-Huxley model These notes focus on two basic mathematical approaches taken by computational neuroscientists to broaden our understanding of the relationship between neural activity and behavior. The first centers around concepts of probability theory. These concepts are crucial to making progress in understanding how the brain encodes, performs computations on, and then decodes information gathered from the world outside the cranium. The second approach centers around the idea of a state space. This is a somewhat abstract notion in which a list of the many variables describing a given neural system are viewed as a single point in some "space." For example, one could assume that a list of the activity level of all the nerve cells within a given region of the brain gives a reasonable characterization of the "state" of that brain region. The set of all possible combinations of firing rates represents the "space" in which such a state lives, and any particular state is represented as a point in that space. The most important contribution of this idea is that the internal variables describing many individual components of a system are combined into a single conceptual "object." It is then an easy conceptual leap to think of one state affecting another, or to view changes in the system over time as mapping out a path in state space, all without getting enmeshed in the particular changes in each of the parts. Computationally, tools exist for analyzing many such systems, particularly if they are linear. Thus, the opening chapters of these notes will focus on basic notions in linear algebra, although well address certain nonlinearities as well.Another possible way to divide the topics in the course is to separate the computations performed by the brain "as it is", vs. trying to understand how the brain changes to optimize behavior. The first topic of the "activation dynamics" of the brain can be separated into two further questions: the question of trying to quantify what is the relationship between neural activity and events in the outside world, and the question of what are the biological mechanisms that underly neural computation. The question of plasticitylearning can also be subdivided into a search for the computational goals vs. the nature of the underlying neural mechanisms.Basic Concepts and ModelsA natural starting point for course in computational neuroscience is the ground breaking experiments carried out in the 1920s by E.D. Adrians results are so ingrained in the culture of neuroscience, that it is difficult to appreciate just how important these Nobel prize-winning results Second in importance to the discovery that the shape of the action potential did not appear to carry significant information was Adrians finding that as the magnitude or intensity of stimulation was increased, the sensory neurons produced action potentials at an increasing rate. Thus, at least to a first approximation, the nervous system seemed to use firing rate to encode information about the world. This rate encoding hypothesis has dominated neuroscience ever since. However, it is quite possible that information is encoded in the pattern of spike timing rather than their overall rate of production. When diving into the debate over "rate codes" and "temporal codes" things get pretty murky rather quickly. We will discuss some of these issues later in some detail, but to start we will adopt the perspective that firing rates or "activity levels" are the basic parameters that govern the neural code. Adrians third basic finding was that neural responses adapt, i.e. the initial presentation of a stimulus causes a neuron to produce spikes at a certain rate, but as the neuron "adapts" to this stimulus its firing rate slows At the most basic level, all experiments designed to uncover the neural code can be described as presenting a bunch of stimuli and recording  the neural responses. Although we will describe some systematic methods below, most often the stimulus set is arrived by a combination of educated guesses and trial and error.The most common form of presenting the results is in the form of a response function -a graph where the mean number of spikes is represented as a function of the particular parameter value associated with each stimulus. For example, the left part of figure 2.4 shows the contrast response function for a cell in the primary visual cortex, and the right plot shows an orientation tuning curve. MORE.  So far weve discussed how to assign a rate function to a given spike train. When performing theoretical computations related to neural coding or when making models of neural systems it is not uncommon to find oneself in the position of having to specify how spikes are produced by a neuron firing at a given spike rate. We will explore some of the biological issues related to spike generation in chapter , but for now we introduce two simple phenomenological models of spike generation. It is useful to think of these two models as coming from "inverting" the definitions of the ISI and PSTH rates. We then introduce a more realistic model that we will explore in more detail in chapter 5.Consider a neuron firing at a constant rate, say r  20 spikessec. Then one would expect that the PSTH collected from many trials, would be constant, i.e. in any small time period t, the neuron will produce rt spikes when averaged over many trials. From this perspective, we can think of the rate at any given time as proportional to the probability of producing a spike at that time. Moreover, in combining spikes from many different trials, the PSTH "averages out" information regarding the dependency of one spike on the next in a given trial. By assuming no dependency, one can construct a simple and useful formal model of spike production, the Poisson neuron. A Poisson neuron produces a random sequence of action potentials that have two important properties: The probability of producing a spike at any given time is give by rate function r. Spikes are produced independently, i.e. the probability of producing a spike is not affected by the presence or absence of other spikes.The last property is an mathematical idealization of the biology. For example, spikes have a finite width and so after the start of one spike means that there cant be another spike for at least this duration. Moreover, after each spike real neurons have a refractory period where the probability of generating a spike is reduced. MORE.Mathematical Aside. This model derives its name from the mathematical concept of a Poisson process.A stochastic point process is a mathematical process that generates a sequence of events with some probabilistic structure. A Poisson process is a stochastic point process that has the properties 1 and 2 above. One can show that the probability of getting n spikes in any given interval equal to e   n n.Consider a neuron firing at a constant rate, say 20 spikessec. From the perspective of the ISI rate, a constant rate would mean a constant interspike interval. A simple model of a neuron that has this property is the perfect integrator: the model neuron adds up its inputs and then produces a spike whenever this integrated input reaches some threshold value, at which time the process starts over. For obvious reasons, this type of model is referred to as an integrate-and-fire neuron. The rate at which the input arrives is proportional to the slope of the integrated input -the greater the input, the faster that the model returns to threshold and the greater the firing rate. EXPLAIN FIG.The above models are phenomenological models for how a neuron might produce spiking output at a given rate. They dont pretend to model how actual neurons convert synaptic input into trains  There is a particular view of brain function that is implicit in many discussions of neural coding. In fact, this viewpoint dominates much of neuroscience. In this view, organisms are optimized to extract information presented to them by their environment and, based on this information, select behaviors that optimize their chance of survival. This point of view is depicted in figure 2.9. Information first enters the brain through sensory receptors. Information is extracted via a series of processing stages. Based on this information, and perhaps information recalled from memory, the organism makes a decision to act. Finally, the details of an appropriate motor command are computed in another series of processing stages, eventually leading to behavior. This picture is the starting point for a number of the most basic controversies about how we view the actions of the nervous system. For example, by depicting various aspects of brain  function as separate, this picture implies a modular view of the nervous system. Controversies related to the localization of brain function have raged since the days of the phrenologists in the nineteenth century, and continue in fights over the interpretation and importance of the latest brain imaging studies. Another criticism of that can be leveled against the dominant view, is the fact that information flows only in one direction, originating with the stimulus and ending with the response. Since it allows for complex interaction with internal representations and memory, the dominant viewpoint escapes some of the criticisms leveled at the strict stimulus-response paradigm adopted by the behaviorist school in the early to mid twentieth century. Nevertheless, the action in figure 2.9 starts with the stimulus and ends with a response. This assumption has often been criticized as a much too passive view of an organisms role in the world. Some have argued that action should be emphasized to a much greater extent, with the behavior of the animal determining to a large degree which stimuli are experienced. This viewpoint is reflected in current trends toward "active perception." Taking a more extreme view, one could reasonably argue that the motor end of the picture is fundamental -animals have evolved to act. Sensory input is of course important, but instead of looking to the world to determine the origin of behavior, perhaps on should view sensory stimuli as making adjustments to an animals ongoing behavioral repertoire. These issues touch on two of the basic dichotomies encountered while studying the brain: the degree to which is behavior innately specified vs. learned, and the degree to which a given pattern of brain activity is driven by the external stimulus vs. internal brain processes. Much of the first part of these notes concerns itself with the topic of neural coding. The phrase "understanding the neural code" has great intuitive appeal, but nailing down exactly what such an understanding would entail can be a slippery proposition. Taken at its broadest interpretation, saying that one is interested "cracking the neural code" is no more specific than saying that one is interested in understanding how the brain works. While more exact definitions could be attempted, I will associate "neural coding" with attempts to analyze brain function focusing on the question of neural representation, i.e. how the brain represents and transforms information about objects in the environment.There are many ongoing and often heated debates related to issues of neural coding. Often these are presented as eitheror debates between mutually exclusive notions of the neural code. Does the brain use a firing rate code or a temporal code Does it use a population code or a local code Are population codes generally based on coarse coding or sparse coding principles As one digs a bit deeper, these clear dichotomies often get rather murky, and their relevance for understanding the brain can get lost. One important source of confusion is that the relevance of the different coding paradigms often depends to a great deal on the particular experiment.MORE... For a discussion of many neural codes, see These results raised a number of questions. Most fundamentally they raised the question of why we experience the world as a continuous flowing scene of continuous objects, when the information that enters the brain is a complicated pattern of discrete impulses. Moreover, the impulses caused by visual stimuli look identical to those caused by somatosensory stimuli. How come one set of impulses leads to the experience of seeing and the other to the experience of touch The obvious answer to the second question is that the visual experiences are caused by impulses arising from the eye and touch sensations are caused by impulses originating in the skin. To quote Adrian, "the quality of the sensation seems to depend on the path which the impulses must travel, for apart from this there is little to distinguish the message from different receptors" The rewiring experiments do not really contradict the labelled line idea, since it is the entire path that constitutes a labelled line. Presumably, neural pathways "downstream" of the auditory cortex were also rewired so that the previously auditory neurons connected with the "decision" and "motor" circuits guiding visual behavior. However, these experiments do point to the possibility of a circularity in Adrians labelled line argument. For if no fixed pathway can be associated with visual perception, then one cannot use the pathway argument to delineate "visual" from "auditory" spikes. Turning the argument on its head, one could argue that the visual pathway should be defined as the set of neurons whose activation contributes to visual perception. But this begs the question of how neural activity leads to perception, and leads straight to the formidable philosophical question of the relation of mind and brain. Adrian was quite frank in acknowledging that his experiments didnt really have much to say about this age-old problem.   Spike Rates and Receptive FieldsWe will start by exploring different ways of assigning spike rates to a given take a series of action potentials, or spike train. In particular, suppose we have a list of action potential occurrence times, t 1 , t 2 , . . . , t i , . . .. How do we come up with a meaningful definition of the rate at which these spikes are produced Certainly any notion of spike rate should have the units of spikes per unit time, i.e. it should somehow be calculated as a fraction:spike rate   of spikes period of timeThe most obvious way to define rate is to decide on fixed a period of time, or time window, and count the number of spikes. For example, if one is interested in how a visual neuron responds to bars of different orientations, one can present such stimuli for a fixed period of time, and count spikes during the period in which the stimulus was shown. In other experiments, one is interested in how the spike rate changes over time. A simple way to calculate the rate in this case is to divide the experiment into short segments or "bins," and count the number of spikes in each time bin or window. One difficulty with this approach is that one can get different answers depending on where the spikes fall relative to the edges of the time bins. These "edge effects" can be eliminated by using a sliding window, i.e. letting the window slide along the time axis, assigning the rate at any time to be the number of spikes in a window centered at that time divided by the length of the window. The location of the edges then depend on the spike times themselves. Note that this procedure is exactly equivalent to the procedure of placing a "window" centered on each spike whose height is equal to 1 divided by the window width, and determining the rate by summing up the heights of all the windows overlapping that point in time. Using a sliding window allows one to consider windows of arbitrary shapes. Often, one takes a smooth window, eliminating the sudden jumps in firing rate.In choosing a window to define the spike rate, one faces a fundamental tradeoff in estimating the underlying rate: variability can be reduced by using large windows and averaging over many spikes, but this averaging smoothes over rapid fluctuations in spike rate. This tradeoff is demonstrated in figure 4.2. Spikes were generated at random from a rate function that makes a jumps from 50spikessec to 200spikessecHz at 500msec. A large window leads to a good estimate of the firing rate, but smooths out the transition from low to high rate. A short window can resolve the transition, but gives a noisy estimate of the firing rate.Thus far, we have represented spike trains and spike rates in fundamentally different ways. Spike trains are a list of spike times t i , whereas spike rate is a function of time, rate  r. It will be convenient to be able to represent both types of objects using a single mathematical framework. Our approach will be to write the spike train as a function of time, i.e. to come up with a rate function s such that spikes are viewed as a very brief increase in the firing rate, and the period between spikes has zero spike rate. How high should the rate function go at the time of each spike Intuitively, we can make the increase in rate around a spike to be infinitely short, as long as the rate function becomes infinitely large over that period. This process is shown in figure 4.3. Note that when constructing a windowed rate in this manner, the "hump" placed around each spike should have a total area equal to 1. This is because one goes from spike rate to spike number by integrating over time, i.e. the number of spikes in the time interval from a to b should be equal to the integral b a dt r.  For an idealized spike train, we would to place an infinitely thin, infinitely tall window to mark each spike. Such a hump placed at the time t  0 is known mathematically as a Dirac delta function, . It follows that  represents a delta function centered at t t. Note that the delta function is not a true function, since it doesnt take on a specific value when t  0. However, such a generalized function can be defined rigorously as the limit of the process depicted in , for any function f and for a  0  b. With this definition we can represent the spike train t i  as the sum s  i .No that we can view spike trains as a rate function, the windowed rate function can be seen as a "smoothing" of this rate function using the function describing the window. Mathematically, this smoothing process is called a convolution. The convolution f  g of two functions f and g is defined as follows:To see how the convolution works as a smoothing operation, let g be the function to be smoothed and let w be a rectangular window function that is 25msec wide. To operate as a smoothing  window, w must have a total area equal to 1, so it must be 1  100025sec 1  40Hz tall, i.e. w  40Hz for 12.5msec  t  12.5msec, w  0 otherwise. Now we find w  g, the value of the smoothed version of g evaluated at t  50msec:The integral term represents the area under the function g from 5012.5  37.5msec to 5012.5  62.5msec. The term out front multiplies this area by 1 over the length of the interval. Therefore, the value of the convolution w  g is equal to the average value of g in the 25msec surrounding t  50. An example of this smoothing is shown in The same intuition holds if w is no longer a rectangular window. Instead of the exact average over an interval, the convolution represents a weighted average of g where the relative weighting is determined by the shape of w An alternative to using a fixed time window is to fix the number of spikes and then measure the time. In particular, one can focus on the time between two-adjacent spikes, and define the spike rate during that period of time, as 1 divided by the length of the interval. Since the time between spikes is referred to as the interspike interval, I will refer to spike rates calculated in this way as the ISI rate. When using the windowed rate, time scale at which rate changes can be measured are determined by the window. In contrast, when using the ISI rate, the rate is defined on a time scale that is determined by the spike train itself. Connect to the perfect integratorAs shown in figures 4.2B and 4.5, if only a few spikes contribute to the determination of the spike rate at any given time, then the estimate of the rate can be quite variable. One way to make sure that many spikes contribute to the rate calculation is to use a large window, as in figure 4.2A.However, large windows have the problem of being unable to capture rapid changes in firing rate. To reliably capture these rapid transitions, one needs more spikes. One way to get more spikes is to perform repeated trials of the same experiment. For example, figure 4.6 shows the result of repeated trials of ... Just like in the windowed rate, the time axis is divided into bins and a histogram is made of the number of spikes in each bin. Such a histogram is known as a peri-stimulus time histogram or PSTH. By dividing by the width of each bin, the PSTH can be expressed as a rate. I will refer to the rate calculated in this way as the PSTH rate. The PSTH rate is essentially the same as a windowed rate and has issues with edge effects and tradeoffs between variability and temporal precision. However, if a sufficient number of spikes are collected, these problems become minor and one can accurately measurement spike rate at a time scale that is significantly smaller than the typical interspike interval. The PSTH rate is sometimes criticized as being merely a convenient data analysis tool for estimating the underlying spike rate, but telling us nothing about the neural code. The argument goes that an animal must respond to a single presentation of the stimulus it cannot wait for repeated presentations to create a histogram. The obvious response to this criticism is that the brain contains lots of neurons. Therefore, if we assume that there is a whole population of neurons whose response properties are the same as the neuron being recorded from, then the animal could average across neurons to get something like a PSTH in a single trial. While this response addresses the original criticism of the PSTH, it seems to require that neurons must be organized into groups with similar response properties. In chapter 5 below, we will show that the only restriction placed on the organization of neural circuits by this point of view is that a given stimulus must activate many neurons.Connect to The Poisson NeuronProblem 4.1.1 Show with a picture that calculating the rate using a continuously sliding rectangular window is equivalent to placing a window centered on each spike and adding.Problem 4.1.2 Use the definition of a convolution to show that the windowed spike rate for the spike train t i  can be written r  i f, where f is the windowing function. This says that the process of convolving a window function and spike train is equivalent to centering a window function at each spike time and adding.Problem 4.1.3 Suppose that g is a linear function, g  mx  b, and f is symmetric window function, f  f and   dxf  1. Show that f  g  g. Hints: 1. Try to figure out why this would be true if f is a square window. 2. Break the integral f  gis a linear function, g  mx  b, and f is symmetric window function, f  f and   dxf  1. Show that f  g  g. Hints: 1. Try to figure out why this would be true if f is a square window. 2. Break the integral f  gr i represents the firing rate of the ith presynaptic neuron, and u is some internal "activation variable." h is the function that determines the internal state in response to inputs arriving at rates r i and f is an inputoutput function that converts u into an output firing rate r. The dynamics of encoding are determined by the single time constant  . Based on ad hoc arguments,  is generally tied to a single biological parameter. Most commonly,  is assumed to correspond to the membrane time constant . An alternative approach has been to assume that the biophysics of spike generation cam be well-approximated by a simple threshold-crossing criterion. Under the additional assumption that individual synaptic events are small relative to threshold, classic tools from stochastic process theory can be used to determine the distribution of interspike intervals. Again, input rates are assumed to be slowly varying so that the arrival statistics of synaptic input are approximately stationary. Recently, this stochastic framework has been extended to cases including dynamically varying inputs  These notes take a different approach to understanding rate dynamics in simple integrate-andfire neurons. The results stem from the viewpoint that IF dynamics stem from a mixture of two important classes of behavior, roughly corresponding to when spike trains are dominated by very small or very large interspike intervals, i.e. when the neuron is spiking at very high or very low rates. By expressing firing rate in terms of the joint distribution of the membrane voltage and its derivative, it will be shown that for neurons producing exclusively either short or long ISIs, rate responses can be reasonably well-described by closed form expressions similar in form to equations and. This approach is limited by the fact that IF models commonly produce a mixture of long and short ISIs, and hence the expressions derived for either regime give an incomplete picture of IF dynamics. However, the approach yields a clear picture of rate encoding at the extremes of IF behavior, and shows that a number of biological time constants may influence the time scale of neural encoding. The resulting intuitions may help to structure more complete investigations of dynamic rate encoding in both real and model neurons.The spiking model used in these notes is a single compartment IF model in which presynaptic spikes result in an exponentially decaying pulse of injected current. The model is based on the standard passive membrane equation,where R is the membrane resistance,  m is the membrane time constant, and voltages are expressed relative to the resting potential. The synaptic currentdenotes the arrival time of the ith presynaptic input and I i is the peak synaptic current. Throughout I will use the short-hand notation that exp  e t if t is positive and exp  0 if t is negative. When the voltage V reaches threshold , a spike is emitted and the voltage is reset to V  V reset .   20 mV and V reset  10 mV in most simulations. For simplicity, only fast synaptic currents are considered, and both excitatory and inhibitory currents are assumed to have the same time course and magnitude. These simplifications are made for ease of presentation only -the analyses readily generalize to models with heterogeneous synaptic parameters.Because equation is linear in the current I syn, it can be integrated to yield the so-called spike-response formalism :is the time of the k postsynaptic spike, and the after-spike hyperpolarization is given bywith  AHP   m . The function P SP i gives the time course of a unitary post-synaptic potential, and takes the shape of a difference of exponentials:The magnitude of P SP i is proportional to I i R. The synaptic time constant  syn determines the rise time of the PSP, and the membrane time constant  m controls the PSP decay.Problem 5.2.1 Show the equation is linear as a function of current, i.e. a superposition of synaptic currents yields a superposition of membrane voltage.The two basic regimes of IF behavior are illustrated in When the bulk of the distribution of membrane voltages remains below threshold, spikes result from occasional voltage fluctuations above threshold In building a rate model, one first has to choose an appropriate definition of firing rate. These notes focuses on IF neurons receiving stochastic presynaptic input described by a Poisson process. Given the stochastic nature of this input, the membrane voltage will also be stochastic. One can then define spike rate r as the instantaneous probability that the voltage V crosses spike threshold , where the probability is computed over the distribution of presynaptic spike trains. This rate can be termed the PSTH-rate since it can be estimated by calculating the PSTH from many instantiations of the stochastic input stimulus. Experimentally, this histogram can be estimated from many repetitions of a stimulus to a single neuron, or from the activity within a population of neurons having similar response properties. In the rest of these notes we will refer to the PSTH-rate simply as the rate.Assuming that spikes are caused by the membrane voltage crossing a fixed threshold, the PSTHrate can be written as follows:In other words, r is the instantaneous probability that the voltage is below threshold at time t and the derivative of the voltage is large enough to push the voltage over threshold in the infinitessimal interval t. Plotting the voltage on the horizontal axis and the voltage derivative on the vertical, the condition for a spike shows up as the gray region in where P V is the probability density function of V evaluated at , and V  V   is the expected value of the rectified derivative conditioned on V  . denotes an ensemble average over the distribution of stochastic inputs. The argument is quite general and equation can be applied to a wide range of neural models. Intuitively, the equation says that in order to spike the membrane voltage must be near threshold and given that the voltage starts near threshold, spike rate is proportional to the average rate that the voltage crosses threshold. For the specific case of the passive single compartment neuron, if V  , then the derivative of the voltageV    IR. Then, the second term in equation, V  V   , depends only on the synaptic current, whereas the first term, P V, depends only on the membrane voltage. In other words, changes in both the voltage and the current may contribute to the time course of neural processing, and these two factors should interact multiplicatively.In the suprathreshold regime, the membrane potential increases monotonically, punctuated by spikes and rapid hyperpolarizations. The positivity of the derivative is the key feature of suprathreshold behavior. Given a positive derivative, the current term in the rate equation is given byTherefore, the current term depends on the mean input current, and hence reacts to changes in presynaptic spike rate on the time scale of the synaptic time constant  syn . Positivity of the derivative also confines the voltage to lie between the reset voltage V reset and the threshold .To analyze the voltage term, P V, we will take advantage of the oscillatory nature of spiking in the suprathreshold regime. First, consider the case of constant injected current. The neuron oscillates at a firing frequency f. The plot of f as a function of I is the so-called "f-I curve." For an IF neuron,. Suppose we examine the neuron at random phases of the oscillation, and let P V ss denote the "steady-state" probability that the membrane voltage has a particular value V . Intuitively, P V ss is inversely proportional to how fast the voltage is rising past V . For an IF neuron, P ss can be explicitly determined:. Note that since the voltage change slows as the cell decays back to the equilibrium voltage IR, P V ss is skewed toward threshold. The degree of skew depends on how close IR is to threshold, and hence indirectly depends on spike rate. At high rates, the return to threshold is nearly linear, and P V ss is nearly flat. At low rates, there is significant slowing of the voltage derivative as the voltage approaches threshold, and the distribution P V ss is skewed toward.To calculate the true voltage distribution, we multiply P V ss multiplied by the distribution of phases of the ongoing oscillation at time t, , i.e.. Combining equation with the expression for the current term in the suprathreshold regime, we find that leads to a very simple rate model: simply compute the mean synaptic current, and set the firing rate according to that current , the distribution of voltages is given by P V  P V ss. Immediately after the change, the actual voltage distribution  Recall that at very high rates, the voltage distribution is nearly flat. Therefore    and the phase distribution  post remains flat, even after a transient. This effect is demonstrated in Problem 5.5.1 Derive equations,,, and.The subthreshold regime is defined by the condition that spikes are caused by occaisional synaptic fluctuations. Recovery from the previous spike makes a negligible contribution to spiking. Rate encoding in this regime can be analyzed by simply ignoring the AHP term in equation and assuming that the voltage is governed exclusively by the synaptic term, i.e.This leads to a simple "threshold-crossing" model where spikes are registered when the voltage crosses threshold from below, but the voltage is not reset and is allowed to drift above threshold after each spike The advantage of considering the threshold-crossing model is that it can be treated analytically by adopting one additional simplification. Thus far, it has been assumed that the arrival of presynaptic spikes is governed by a Poisson process. Assuming that the unitary PSPs are small, this process is well-approximated by a Gaussian process having the same mean and variance. This is the standard "diffusion approximation" for stochastic differential equations . Under this assumption, the joint distribution of the voltage and the voltage derivative is a two-dimensional Gaussian. For example, the mean of the voltage distribution at time t,  V, is obtained by integrating the contribution from all previous intervals of width dt to the current value of the voltage, i.e.where r c is the rate of the cth class of presynaptic input with unitary PSP shape P SP n . All simulations presented contain just two classes of inputs, one excitatory and one inhibitory, and the PSP shapes differ only in sign. Similarly,where V is the mean of the derivative of the voltage,  V and V are the variances of the voltage and the derivative of the voltage respectively, and  V,V is the covariance between the voltage and the derivative of the voltage. SP is the time derivative of the PSP. It follows that the derivative of the voltage, conditioned on V   is a Gaussian distribution with meanPlugging these parameters into equation yields the following expression for the PSTH-rate in the subthreshold regime:  While it appears to have a relatively minor influence on rate dynamics, varying the size of the after-spike hyperpolarization in the IF model does alter the overall firing rate. Small AHPs lead to higher spike rates, and large AHPs lead to lower spike rates. Since the threshold-crossing model completely ignores the AHP, this dependence is not captured by the rate equation. The dependence of spike rate on the AHP implies that the AHP term does affect spiking to some degree, even at rates of 10 Hz and lower. This is somewhat surprising, given that a 10 Hz spike rate implies that the mean interspike interval is 100 msec, over six times as long as the 15 msec decay time of the AHP. The effect of the AHP can be understood, however, by remembering that in the subthreshold regime model behavior resembles that of a Poisson process. Consider the exponential of interspike interval distribution produced by a Poisson process firing at 10 Hz. Even though the mean interval is 100 msec, the median interval is 69.3 msec, and nearly 14% of intervals fall within the decay constant of 15 msec. Thus, it should expected that the AHP term will have some effect on the number of short interspike intervals, well into what could be considered the subthreshold regime. "Pure" subthreshold behavior is expected only at the very lowest spike rates.While the ability to write down a closed form expression for the spike rate in the subthreshold regime is satisfying, the real value of equations- is the insight gained regarding rate encoding. For example, in the subthreshold regime it is evident that both the mean and the variance of the synaptic input play a role in determining the output rate. This is true both in the current term, where the variance of the voltage derivative enters via the error function, and the voltage term where the probability that V   depends on both the mean and the variance of the voltage distribution. More importantly, consideration of the variance of the input as well as the mean adds a number of additional time scales to the problem.To understand the contribution of these multiple time scales, we first consider a very simple example of a neuron receiving input from a single population of presynaptic neurons. Inputs from this population are assumed to arrive at a mean rate r. Each of these neurons is assumed to give rise to a synaptic current that is described by an instantaneous rise and exponential decay with decay time  syn . According to equation the PSP can be written as a difference of exponentials:But then to calculate the mean potential, equation becomeswhere r  is the rate of presynaptic input smoothed with time constant  , i.e.From equation we see that both the synaptic and membrane time constants enter into the calculation of the mean potential. However, if we consider synaptic currents that are much faster than the membrane time constant, then  m r m   syn r syn and the membrane time constant dominates the calculation of the mean voltage. This case models the most common intuition regarding synaptic integration: the membrane time constant is the limiting factor that determines the time window over which synaptic inputs are integrated. Note that many experiments suggest that this may not be the only, or even dominant mode of synaptic integration. The membrane time constant is reduced when a neuron receives a large barrage of synaptic input, and there is ample evidence that "slow" synaptic currents contribute large synaptic inputs, at least in the cortex.To calculate the mean of the voltage derivative, we must take the time derivative of the PSP and plug it in to equation:In calculating the mean of the voltage derivative, the synaptic and membrane time constants play an equal role. Now lets calculate the variance of the voltage. First we define a "mixed" time constant  mix   m  syn  that will crop up in the calculations below. According to equation we haveFrom this equation we see that there are actually five distinct time constants that affect  V:  m ,  syn ,  m 2,  syn 2,  mix . The latter three arise because of the squaring operation used to calculate the variance. Similar calculations can be performed for V and  V,V.Problem 5.6.1 Calculate V and  V,V for the case of a single population of presynaptic neurons having a synaptic time constant  syn .These notes focus on an analysis of two extremes of IF behavior. Except at the very high or very low rates, neurons are expected to display a mixture of supra-and subthreshold behaviors. For example, short interspike intervals are expected to be governed by suprathreshold behavior since they arise when synaptic input wanders above threshold. Conversely, long intervals are expected to be governed by subthreshold behavior. To truly understand rate encoding in IF neurons, it will be necessary to gain a better understanding of the statistics governing the switching between these two regimes. A number of mechanisms are expected to affect the relative balance between supra-and subthreshold behaviors. For example, experimental evidence suggesting that spike trains produced by neocortical neurons have highly variable ISI statistics Consider the joint probability density function P. Let P V and PV be the probability density functions of V andV , respectively, and let P V V 0 be the probability density of V conditioned onV V 0 . Suppose that the partial deriva-x dV PV  0, and   dVV PV  0 Then, the instantaneous probability of reaching spike threshold,Proof: r can be rewritten asBy setting x t and noting, condition implies that the second term goes to zero as t  . To get a handle on the first term, note that assumption, dP V VdV   K, is equivalent to the condition dPdV   K PV, and P  P VPV , where PV V 0 denotes the probability density ofV conditioned on V  V 0 . Consider voltages where    V  . Then assumption implies thatSince can be chosen to be arbitrarily small, assumption implies thatBy reversing the inequalities and substituting  K for K, a similar argument shows thatChapter The attempt to quantify the accuracy of peoples perceptions can be traced back to Gustav Fechner, the father of the field known as psychophysics. Fechners goal was to uncover the natural laws of perception and the first step was to develop quantitative, objective methodologies for measuring the accuracy of perception. To avoid difficulties related to subjective perception, Fechner focused on the use of simple to choice tasks to determine the magnitude of the weakest stimulus that leads to perception -the absolute threshold -and the smallest change in stimulus parameters that can be perceived -the difference threshold measured in units of just noticeable difference. In attempting to measure these magnitudes, one encounters the fundamental difficulty that presenting identical stimuli doesnt always lead to identical results. All sources of such uncontrolled variability are lumped under the concept of noise.In any perceptual experiment, noise can be introduced by the experimental apparatus, variability introduced by the components of the nervous system or changes in the behavioral or attentional state of the subject. The beginning part of this chapter will present the rudiments of signal detection theory. Not only does this serve as the basis for the filed of psychophysics, this methodology is one of the main ways of quantifying the fidelity of the neural code. Because the same measures can be applied to neural responses as well as perception, this approach has the advantage of allowing direct comparisons between neural responses and behavior. Moreover, because the method generally sets up a choice between two alternatives, important mathematical concepts can be introduced in their simplest form.Consider the following experiment designed to measure your detection threshold for light intensity. A tone goes off and you are asked if this tone was followed by a flash of light. The computer generating the stimuli randomly interleaves trials in which a flash is present or not. This procedure is repeated for varying intensities of the light flash. If the experiment is calibrated properly, some of the light flashes are too dim to see, while others are clearly visible. Plotting the percentage of correct guesses as a function of light intensity is known as a psychometric function for this experiment First we introduce some notation for describing the outcomes of the experiment. On a given trial, there are two possible stimulus configurations: the stimulus can be present or absent. There are also two possible decisions: the subject can respond "yes, the stimulus was present or "no, the stimulus was absent". This leads to four possible combinations for each trial: a correct "yes" responses on trials where the stimulus was present, a correct "no" responses on trials when the stimulus was absent, an incorrect "no" response on trials when the stimulus was present, and an incorrect "yes" response on trials when the stimulus was absent.Stimulus. The distributions of perceived strength for the stimulus trials and no-stimulus trials are often modelled as Gaussian distributions. Note that while the average perceived strength on trials where no stimulus was shown should be zero, neural noise and noise from the background luminance will give rise to a range of perceived strengths. The decision process is modelled as a simple threshold criterion: if the perceived strength is sufficiently large, subjects respond that they detected a stimulus on that trial. In this figure, the threshold has been set at the crossing point of the two distributions. The fraction of incorrect responses are represented by the area of the shaded areas: the black area represents the fraction of trials where the stimulus was present, but perceived strength was below threshold and the light gray area represent trials where the stimulus was absent but perceived strength was above threshold. The picture in figure 6.2 represents multiple trials of a detection experiment for a fixed stimulus. As the stimulus is made more salient, trials in which the stimulus is present will lead to a greater perceived strength on average and the corresponding distribution will move to the right figure A. In this case, only small portions of each distribution lie on the wrong side of threshold, and performance is high. For very dim stimuli, the two distributions will be nearly overlapping and the probability of an error will approach 50%. Under this framework, varying signal strength will change the location of the perceived signal strength distribution, and the fraction of the two distributions lying on the correct side of threshold will map out the psychometric function. Note that the perceived signal strength might not be strictly proportional to the actual signal strength. Changing this relationship will change the shape of the psychometric function.Another way to alter performance in the task is to change the level of noise in the task. For example, one could design an experiment where subjects are asked to detect the presence of a tone with differing levels of background sounds. Increasing noise is modelled as a greater uncertainty in perceived strength, and hence a broadening of the corresponding distributions. Increased noise increases the overlap of the two distributions and hence reduces performance.Now consider the same basic experimental set-up, but this time spike trains are being recorded from a neuron in the visual system that responds to light onset. Assuming that the neuron increases its spike rate as stimulus intensity increases, we can can view the number of spikes as a measure of the neurons "perceived signal strength." That is, we can reproduce figure 6.2 by simply replacing the distributions by histograms of the number of spikes per trial. By setting threshold where the two histograms cross, we could use the response of the neuron to guess whether the stimulus was present or absent on a given trial. In this way we can evaluate the ability of the neuron to detect the stimulus. Plotting the percent of correct classifications vs. stimulus strength is known as a neurometric function. Using this method, we can compare the behavioral performance of an entire animal on a signal detection task to the ability of individual neurons to perform the task.In a series of elegant experiments Ken Britten, Bill Newsome and colleagues performed a series of experiments comparing the psychometric functions from monkeys performing a visual motion task with a neurometric function obtained from neurons in an area of the brain specialized for visual motion processing. See figure -WILL DISCUSS IN CLASS. An important difference between the measurement of a psychometric function and the construction of a neurometric function is that in the behavioral experiment only the yesno responses are available, whereas in the neural experiment the entire histogram of responses is available. This difference is crucial, since behavioral performance is not only dependent upon the overlap of the two distributions, but also upon the setting of the response threshold. For example, two different subjects may perform differently on a detection task, even if their "internal perception" of the stimulus was identical. For example, performance could be suboptimal for a subject who is hesitant to respond yes unless he or she was pretty confident that a stimulus was actually present.One way to partially recover the shape of the underlying distributions of perceived strength is to examine both hit rate and correct rejection rate as the threshold is varied rather than just averaging the two to get an overall percent correct. A subject with a strict threshold will have few false alarms, but will have a reduced number of hits. A subject with more lax threshold will have a greater fraction of hits, but at the expense of a greater number of false alarms. A simple way to mimic a variation in decision threshold is to ask subjects to report their degree of confidence in their behavioral choice, say on a scale of 1 to 5. A strict threshold can be mimicked by reanalyzing the data where you assign a no response to all trials except where the subject said yes with a high confidence level. A lax threshold would assign a yes response to all trials except confident rejections, etc. In this way, at each level of signal strength the tradeoff between hits and false alarms changes with threshold can be mapped out. We will discuss other ways of varying the decision threshold below.The function that maps out this tradeoff is known as an ROC curve. False alarm rate is represented on the horizontal axis, and hit rate is mapped on the vertical axis. Very high thresholds lead to few false alarms but also few hits, corresponding to the lower left corner of the plot. As threshold is decreased, the number of hits goes up without much increase in the false alarm rate. As threshold is decreased further, the hit rate continues to rise but now the false alarm rate begins to increase. The increasing false alarm rate makes the curve begin to bend to the right. As threshold is reduced even further, the hit rate approaches 100%, but the false alarm rate also increases. At very low thresholds, the subject reports a stimulus on every trial. The hit rate and false alarm rate are both 100%, corresponding to the upper right corner of the plot.A single ROC curve maps out the tradeoff between hits and false alarms at a single stimulus intensity. Changing the stimulus intensity leads to a new ROC curve. Note that the best performance is represented by points at the upper left hand portion of the ROC plots: few false alarms and lots of hits. Therefore, if we make the task easier by increasing the intensity of the stimulus, more of the curve moves to the upper left. If the task is very difficult, hits and false alarms co-vary. A high threshold leads to few hits and few false alarms, a low threshold leads to many hits but at the cost of many false alarms. Therefore, for difficult tasks the ROC curve hugs the diagonalWhen recording neural data, we presumably can construct the relevant response histograms an ROC curve gives no additional information. But in psychophysical settings, we measure only yesno responses the shape of the underlying distributions are inferred. Using an ROC curve to describe neural performance allows for a direct comparison of neural and behavioral data.An alternative to measuring the effect of varying the threshold criterion is to design experiments that encourage the optimal placement of threshold. On such design is the two-alternative forced choice task. The main idea here is to try to make the two response alternatives as similar as possible, thereby greatly reducing opportunities for bias. Generally, each trial contains two "presentations" one where the stimulus is present and the other where the stimulus is absent. Continuing with our visual detection example, one could flash the weak stimulus in one of two locations or flash the stimulus in one of two consecutive stimulus periods. The subject would then be asked to make a decision of which of the two possible presentations actually contained the stimulus. D 1 will denote a decision that first presentation contained the stimulus, and D 2 will denote a decision that stimulus fell on the second presentation. Presumably, subjects make judgements by choosing the trial that led to the largest internal response, rather than comparing this response to a subjectively determined threshold. Letting R 1 be the random variable describing perceived signal strength on presentation 1 and R 2 describe perceived signal strength on the presentation 2. That is, the decision is made based on whether R 1  R 2 is positive or negative.We can depict a two-alternative forced choice task by a picture that is very similar to figure 6.2, by calculating the conditional distributions P and P. On trials when the stimulus was present on presentation 1, R 1  R   RS  and R 2  R   RS  . On trials when the stimulus was present on presentation 2, R 1  R   RS  and R 2  R   RS  . Therefore the two conditional distributions P  P and P  P will simply be negative images of each other. As long as the subjects treat presentation 1 and presentation 2 symmetrically, threshold will be set at zero.The ROC curve can be used to quantify performance for varying thresholds. Now we return to the question of how to set threshold to get optimal performance. In section  we implicitly adopted a maximum likelihood strategy, i.e. the appropriate yesno decision on a given trial is given by the stimulus that had the greatest likelihood of generating that internal response, assuming that stimulus was indeed the stimulus presented. In other words, the two curves in the top plot of figure 6.2 represent the two probability distributions P and P. The response was determined by which one of these values was greatest, i.e. threshold was placed at the point where the two distributions crossed.The difficulty with the maximum likelihood strategy is illustrated by the following classic problem.Suppose that you go into the doctors office for a series of tests, and a test for disease X comes up positive. You begin to worry, especially when the doctor says the test is 99% accurate. Whats the chance that you actually have disease X Making the analogy with our presentation of maximum likelihood, there are two stimuli, and two responses. Since your test came out positive, according to the maximum likelihood estimator, you should assume that you have disease X since if having the disease gives a positive result 99% of the time, whereas not having the disease gives a false positive only 1% of the time.Feeling quite concerned at this point you ask the doctor what you should do. She says that you shouldnt worry. Out of the 10 million people that take the test for disease X, only 1 out of 100,000 are likely to have the disease. That means that of the 9,999,900 that dont have the disease, there will be 99,999 false positives, while the 100 people that do have the disease will yield 99 true positives. Therefore, if we consider the total population, the fraction of the time that a positive result actually means that you have the disease is 99 or less than .1%. You leave the doctors office quite relieved, but your confidence in the maximum likelihood strategy has been shattered.Well analyze this example in some detail, because it illustrates a number of important points. We first introduce some notation. We have two possible "stimuli": either the patient has the disease or they dont. We also have two possible responses: the test either comes up positive or the test is negative. In the maximum likelihood strategy, if we are forced to choose which stimulus gave rise to a certain response r, we choose the stimulus that maximizes P, i.e. we choose the stimulus that would have the greatest likelihood of generating that response conditioned on that stimulus was the one actually presented. But what wed really like to choose is the stimulus that maximizes P. The strategy of choosing the stimulus that maximizes P is known as the maximum a posteriori 1 estimate. In our disease example, what we know is P for various combinations of r and s -P  P  .99 and P  P  .01 -while wed like to know P.The rule for combining these conditional probabilities is known as Bayes rule. To illustrate this rule, first consider the two dimensional histogram of the results of applying the test to 10 million people Geometrically, P captures the portion of the total number of people in the S  row of the distribution, while P captures the portion of this row that tests positive. Alternatively, one could find the total portion P of the population that tests positive, i.e. the portion of the distribution in the R  column, and multiply this by the fraction P that actually had the disease. While the problem was defined in terms of the probabilities in equation, it is nevertheless true that this alternative strategy leads to the right answer, i.e.Combining these two equations we find thatNote that evaluating what to do about your positive test result, you dont really care about the overall probability that you tested positive, you already know that. What you are interested in is the relative likelihoods of the various possibilities given that you tested positive. Therefore you dont really care about the denominator in equation Notice that the difference between ML and MAP strategies has to do with your prior knowledge. If you have no prior reason to believe that one stimulus is more likely than another, then the ML and MAP strategies arrive at the same answer. Therefore, for the detection tasks described above, as long as the flash is presented on half the trials, then the stimulus and no stimulus conditions have equal prior probabilities. Hence analysis presented above apply to both ML and MAP frameworks.Consideration of prior probabilities can play an important role in terms of interpreting physiological results. In particular, the nervous system has presumably been shaped by evolution to give optimal performance given the probabilities of stimuli encountered by the animal during natural behavior. Thus if the performance of the nervous system is below optimal for some set of stimuli, it may simply be that the stimuli chosen by the experimenter are not presented with the same relative probabilities encountered in natural settings. For this reason, recently there has been growing interest in trying to obtain quantitative estimates of the statistics of natural scenes.One could argue that to survive it may be more important to be right about some things than others. For example, it might have been useful for our ancestors to detect the rustling of prey in some underbrush, and imperative to recognize whether the rustling was caused by a tiger. Note that it is relatively easy to incorporate such different payoffs within a probabilistic framework to determine optimal behavioral strategies. Within our two choice framework, we can assign payoffs and penalties for each of the possibilities. For example, correctly guessing the presence of a stimulus might lead to 2 units of reward, while a false alarm leads to a penalty of 5 units. A missed stimulus may have a penalty of 1 unit and a correct guess of a non-stimulus trial might lead to a 1 unit reward. This situation can be summarized in the following payoff matrix:It important to note that as long as each stimulus is drawn independently, the best that can be done is to find the optimal course of action for each response level. So for now we will fix a response level R  r and calculate the probability that a stimulus was present or absent give r, i.e. we need to calculate P and P. If we say yes when experiencing a response level r then the average reward for this condition will be 2P  5P while if we say no the average reward will be P  P. Therefore we should guess yes for all responses where 2P  5P  P  PA little algebra reveals that the condition for a yes response isThat is, if a given response level leads us to believe that it is twice as likely for a stimulus to be present than not, we should guess yes. Otherwise we should guess no. This conservative strategy is dictated by the relatively high penalty for false alarms.Chapter In the last chapter we focused on experiments where the number of stimuli is small, as in two alternative forced choice tests. However, it is often the case that stimuli are organized so that there is some notion of distance between stimuli. On the response side, spike rate inherently contains a notion of distance. In fact, in almost all cases tuning curves and response functions are described as continuous functions of continuous variable. In this chapter, well focus on cases where the stimulus is described by a single variable, for example the frequency of a tone presented to the auditory system. Tuning curves or response functions describe changes in the average response for different stimuli. But of course, responses are somewhat variable. This chapter describes various methods for quantifying this variability.Biological Aside. Most experimentally derived tuning curves show error bars indicating the magnitude of the standard error of the mean. While the SEM is related to the variability in the response, it does not directly quantify response variability. In particular, repeating the experiment will reduce the size of the error bars, since one becomes more confident in the measurements of the mean response for each stimulus. But running more experiments doesnt reduce the variability in the responsesThe most direct way of quantifying the level of noise is to measure the variance of the response distribution, averaged over all stimuli. But simply determining that the standard deviation of the response rate is 5 Hz has very little meaning. Should 5 Hz be thought of as a large or a small quantity To get a handle on this question, one might compare 5 Hz to the typical spike rate of a given neuron. 5 Hz might represent a high level of noise for a neuron that typical spikes at 5-10 Hz, but a relatively modest level of noise for a neuron spiking at 100 Hz. But as we learned in section  it the level of change in response that is useful for discriminating between stimuli. If changing the stimulus only changed the mean firing rate by a few Hz, then 5 Hz noise would lead to very poor stimulus discrimination, even if typical spike rates were near 100 Hz. For this reason, one often describes the variability in responses using the signal-to-noise ratio, which is simply the variance of the signal divided by the variance of the noise. If noise levels are vary low, then small changes in the denominator can make a large difference in the SN ratio. For this reason, signal and noise are sometimes quantified by calculating the fraction of the total variance in the response that is accounted for by the response function. Since variances are additive, the total variance is simply the variance of the noise plus the variance of the signal. Therefore, the fraction of the variance accounted for by the signal is given by. Given this distribution, there are two basic strategies for estimating the stimulus. First, one can choose the stimulus that is most likely, i.e. the stimulus s that gives the maximum value for p. This is the MAP strategy outlined in the previous chapter Alternatively, one can produce an estimated stimulus s est that is as close as possible to the true stimulus s. To define "as close as possible" we must provide some notion of distance. One way to do this is to define a loss function, i.e. a function that determines the "cost" of various degrees of error. The most common loss function is 2 . In this case the task is to minimize the squared distance between the estimated and true stimulus. In this case, the optimal strategy is to set the estimated stimulus for a given response r to be the average of the stimulus distribution conditioned on the response r. Given any mapping from response to stimulus, we can talk about the estimated stimulus s est much as we did the "signal." Then the and the error -the difference between s est and the actual stimulus -as we did the noise. MORE.Problem 7.3.1 Show that the mean value is the value that minimizes the least squared error, i.e. given a distribution of values x 1 , x 2 , . . . , x N , the value of y that minimizes i 2 is the mean valuex  1 N i x i .In the rest of the chapter we will deal with a topic known as information theory, first developed by Claude Shannon and colleagues at Bell Labs in the 40s and 50s. While Shannon introduced the subject as a theoretical framework for studying coding along a communication channel, the introduction here will focus on the concept of mutual information as generalizing the signal-tonoise ratio. To make this connection, we first view the variance as a measure of uncertainty. For example, the variance of the noise for a given stimulus represents how uncertain we are about the response to that stimulus. The second connection relates to the formulatotal is the variance of the distribution of responses collected over the entire experiment. It represents the level of uncertainty about the response over the whole range of stimuli, i.e. it represents how uncertain one would be about the response if one knew only which set of stimuli were being presented, but didnt know the specific stimulus presented. In this formulation, the strength of the signal relative to the noise corresponds to how much knowing which stimulus was presented reduces your uncertainty about the response, relative to the initial level of uncertainty. It is in this sense that knowing which stimulus gives you information about the response -it reduces the level of uncertainty. This particular viewpoint means that information is defined as loss in uncertainty. Thus information and uncertainty are flip-sides of the same thing and hence will have the same units.The key to information theory is to make formal mathematical definition that captures the notion of uncertainty. Suppose that we want to define the amount of uncertainty contained in a discrete random variable X. What we mean by X being a discrete random variable is that X has a finite number of states, which we denote by x 1 , x 2 , . . . , x N . X is governed by a discrete probability distribution, that is an assignment of a probability P to each state x i . Remember that the total probability must add to 1: i P  1. We will use H to denote the uncertainty embodied in the distribution X. Shannon outlined three properties that should be satisfied by the function H.1. 0 uncertainty corresponds to the case where one state has probability 1, and all others have probability 0. In math terms, H  0 if and only if P  1 for some x i .2. Maximum uncertainty is attained when all states are equally probable, i.e. H is maximal when P  1N .3. The uncertainty contained in a distribution composed of two independent sources of uncertainty should be the sum of uncertainties for these two sources. To make this formal, suppose we have a second distribution Y composed of states y 1 , y 2 , . . . , y M . Consider the joint distribution with states composed of all pairs. The condition we want isShannon then proved that the only function that satisfies these three properties must be proportional to the function. In particular, while probabilities for independent events multiply, P  PP, the resulting uncertainties must add, H  H  H. The choice to use base 2 for the logarithm is by convention and means that uncertainty is expressed in "bits," with the flip of an unbiased coin having one bit of uncertainty since it represents one random binary choice.There are a number of ways of thinking about the entropy. For example, H can be thought of as the average number of yes no questions it takes to guess which state was chosen at random from the distribution X. Lets consider a particularly simple example. Suppose X has three states with P  12 and P  P  14. Then the optimal guessing strategy will be to first ask the question, "is the state x 1 " If the answer is yes you are done, and if no the question "is the state x 2 " will determine whether the state is x 2 or x 3 . In this example, guessing the state requires one question half of the time and two questions the other half. Thus the average number of guesses, H  1.5. This can be confirmed by plugging the probabilities into the formula:H   log 2  log 2  log 2  .5  .25  2  .25  2  1.5A second way to think of H is to form long "strings" of repeated samples from X, e.g. the 10 state string x 1 x 3 x 3 x 1 x 2 x 1 x 1 x 1 x 3 x 2 . Using the same distribution X as above, wed expect that roughly half of the entries equal to x 1 and roughly one quarter equal to x 2 and another quarter equal to x 3 . If we choose strings with a large number of entries, the chance of getting a string where the state dont show up with these relative probabilities will be very small. Since there are three states, the total number of strings of length N is 3 N . One of the results that Shannon proved is that for long strings the number of typical strings of length N is equal to 2 N H , with the chance of finding any of the other strings being negligibly small. then one will have to come with a lot of abbreviated symbol strings and hence these abbreviations will save less coding space. Thus H is related to the optimal amount of compression that one could achieve. Note that this important result quantifies the maximal amount of compression that could be achieved, but doesnt say anything about the nature of the compression algorithm that would achieve that goal. However, if one constructs a compression algorithm, the entropy can be used to determine how close this strategy is to the optimal strategy possible.So far we have defined entropy for discrete distributions. The most natural way to extend the definition of entropy to distributions over a continuous parameter is to divide the parameter dimensions into discrete bins and calculate the resulting discrete entropy. Then we can define the entropy of the continuous variable as the limit of the entropies as the bin size gets very small. Suppose we parameterize the distribution by the continuous variable x, letting p denote the corresponding probability density function. We then chunk x into bins of width x. Since the bin containing x has probability approximately equal to x P, the entropy at this resolution is given byIn the limit where x  0, the first term becomes the integralThis is often known as the differential entropy. However, the second term  log 2  . This captures the notion that with an infinite number of states, the entropy grows infinitely large. But  log 2 depends only on the resolution, not on the shape of the distribution. Therefore, if we are interested in the difference in entropy between two distributions measured at the same resolution, we can focus our attention on the differential entropy without having to worry about infinite quantities.Problem 7.5.1 Show that if X and Y are independent random variables, then H  H H.Now that we have the proper notion of uncertainty, we return to the problem of quantifying signal and noise. Remember that want to define the information about the response that is gained by specifying the stimulus as the reduction in the uncertainty contained in the entire distribution of responses P to the average uncertainty in the noise. The total uncertainty is given byThe noise uncertainty for a particular stimulus s is the uncertainty in the conditional distribution P:To obtain the average uncertainty across all stimuli, H, we take the weighted average of the individual uncertaintiesWe use I to denote the mutual information between stimulus and response. This is the analogue of the notion of "signal." I is just the difference between the total entropy H and the average noise entropy HMathematical Derivation. Using little algebra, along with Bayes rule, we can write the formula for the mutual information in a nice form. The derivation will rely on two substitutions:The most important aspect of equation In other words, looking at the problem from the experimenters perspective and the organisms perspective gives the same answer.Equation has a number of important consequences. For example, given a fixed amount of noise entropy H, the coding strategy that maximizes the mutual information between stimulus and response will lead to a distribution of responses that maximizes H. What distribution of responses give the maximal entropy This question only makes sense of there is something constraining the possible range of responses -an unlimited range would lead to entropy values that approach infinity. Thus maximizing entropy must be done in the context of some constraint.For example, suppose that we use firing rate as our measure of response and suppose that there is some maximum firing rate. Then the distribution of responses that maximizes entropy and hence mutual information is one where all firing rates between zero and the maximum are equally likely. As an example of this, consider the case of responses of a contrast-sensitive neuron in the fly visual system known as LMC. There are a number of approaches to measuring mutual information. First is the so-called direct method. In this approach, one measures the response entropy and noise entropy by gathering enough data to estimate the required distributions and calculate their entropy. This approach has been pursued in only a few cases since gathering enough data is often not feasible, unless one wants to make strong simplifying assumptions. For example, suppose one doesnt want to commit oneself to a rate coding assumption from the outset. Then each spike train must be treated as a separate response. Even if only brief responses are considered, the number of possible spike trains becomes staggering. Considering a brief response period of 100 msec and characterizing the resulting spike trains with a resolution of 10 msec, leads to at least 2 1 0 possible spike trains, without even considering cases where two spikes fall within a single 10 msec bin. However, certain short cuts and estimates can be taken and this method has been successful in a number of cases.Much more common is to use short-cuts and assumptions to bound the real mutual information within some range. In the most common approach focuses on entropy calculations in terms of the stimulus:Since the stimulus distribution is decided upon by the experimenter, H is usually known. Therefore, if we can get an upper bound on H, we can get a lower bound on the mutual information I. Getting a true measure of H may require a lot of data, particularly if responses are characterized in a complex manner. However, suppose the stimulus is parameterized by a single parameter and we have some method of forming an estimate of the stimulus s est . Since the variance of the distribution is the minimum squared error for any estimator, 2 gives an upper bound on the variance. Since a Gaussian distribution is the distribution that maximizes the entropy for a given variance, we can say thatThe most common way to get an upper bound on the mutual information is to calculate the theoretical maximum for the entropy in a spike train. This will give an upper bound on H. Then, if we simply ignore the noise entropy H, then this upper bound will also give a upper bound on the mutual information sinceIf spikes are measured with infinite resolution, then the maximum entropy would be infinite. But suppose we put spikes into bins of size t and choose small enough bins so that there is at most one spike per bin. If responses have length T then there are T t bins, and if we assume that there is the mean number of spikes equal to R over this period, then the maximal entropy is obtained when each bin has the same probability of seeing a spike, R. The resulting entropy is the maximal entropy of any distribution of spike train responses.How much do correlations between spikes contribute to coding Note that the notion of correlation includes issues of both temporal and population encoding. One way to define temporal coding is a code in which the contribution of one spike in a spike train depends on the existence and location of other spikes in the trains. Otherwise, one could consider this as a rate modulated code. Similarly, one can define a population code as a code where the contribution of one neuron to the decoding of the stimulus depends on the response of other neurons. From this perspective, the key issue is to determine whether dependencies between spikes contribute to coding. Its important to point out that the language commonly used to describe this issue is often borrowed from the two different frameworks we have used for quantifying the relationship between stimuli and responses. Namely, the metric-based framework uses variance to quantify uncertainty, clearly separates the signal from the noise, and uses the covariance to quantify the relationship between variables. The probability-based framework uses entropy to quantify uncertainty, uses conditional distributions to address questions of noise, and uses mutual information to quantify the relationship between variables. For the most part we will use the term dependency and correlation interchangeably, although the term correlation comes from the metric-based framework that is more restrictive than that based on probabilities: two independent random variables are uncorrelated, but uncorrelated variables are not necessarily independent.In determining the importance of correlations for neural coding, the issue becomes complicated because there are multiple types of dependency that one can consider. To clarify the issues we will take the probability-based framework and consider two sets of responses, R 1 and R 2 . We will describe these as the response of two different neurons although the description could just as easily apply to two different temporal components of a single neurons response.The first concept in understanding the importance of correlations for neural coding is to separate out correlations that are due to changes in the signal. As an example, suppose you record from two neurons in the primary visual cortex and find that the spiking of these two neurons is correlated. What can we make of this correlation Does the correlation represent some added dimension to coding Perhaps, but suppose the two neurons have overlapping receptive fields and are both tuned to horizontal orientations. Then whenever a horizontal edge is presented in that portion of the visual field, both neurons will tend to be activated together. Other things being equal, this co-activation will cause spiking in the two neurons to be correlated, but this correlation is simply due to the similarity in their tuning.The standard technique for determining if their is some additional component to the correlation, is the use of the a shuffle correction. For example, suppose that you record responses to 20 presentations of a stimulus set S. To the degree that correlations are simply due to changes in the stimulus, you should get the same result if you correlate the pattern of spiking for neuron 1 on trial 1 with the spiking pattern for neuron 2 chosen from any of the 20 trials. If, however, there was some other factor driving correlations between the two neurons, this correlation should only show up when calculating correlations from spike trains recorded on the same trial. To separate these two components of correlation, one calculates the shuffled correlation by randomly permuting the spike trains of one or both neurons and calculating the correlation. This shuffled correlation can then be subtracted from the true correlation, and any remainder indicates a correlation between the neurons in addition to that predicted from any similarity in the tuning properties of the two neurons.The shuffled correlation is often called the signal correlation and the remainder is called the noise correlation:Although the signal correlation is the correlation among the signal patterns and the noise correlation is the correlation among the noise residuals, the signal and noise terminology can become misleading when the magnitude of the noise correlation is not the same for different stimuli. Then the noise becomes signal-dependent and in some cases the only signal is in the noise. To see a simple example of this suppose there are two stimuli and two neurons, and each neuron has two response levels: high and low. Suppose that high and low responses are equally likely for both neurons and for both responses, so the signal correlation is 0. But for stimulus 1 the neurons tend to respond similarly and for stimulus 2 the neurons tend to be in the opposite response state.We will consider the following three types of dependency:1. Activity dependence. This is dependence between the responses of the two neurons over the entire experiment, and is quantified using the mutual information between the two sets of responses I.2. Conditional dependence. This is dependence between the responses of the two neurons given a the presentation of a given stimulus. It is quantified using the mutual information between the two sets of conditional responses, averaged over the stimuli: s PI.3. Information dependence. This notion is best captured as information independence. Two sets of responses display information independence if they provide independent sets of information about the stimulus ensemble. In this case, I  I  I. We have three information measures at play here: the sum of the separate measures of mutual information, I sep  i I, the mutual information between the stimulus and the shuffled responses, I sh, and the true mutual information, I. What we will show is that we can start with the separate measure of information, and add a number of correction terms to get to the true mutual information, passing through the shuffled information along the way. In doing so, rearranging terms we will construct a decomposition of the true mutual information into parts that represent different contributions of correlated firing. For the first decomposition we writeThis can be directly compared to the decomposition proposed by Panzeri and colleagues, I  I lin  I sigsim  I corrind  I corrdep with I lin  I sep described as the "linear" term, I sigsim  I sh  I sep the "signal similarity" term, and I corrind  I corrdep  I  I sh is the sum of the "correlation independent" and "correlation dependent" noise terms. The correlation dependent noise term I corrdep  I described above. Formulas for these terms are derived below.To start the analysis, lets look at I sep :The first term is the sum of the entropy of the total response distributions R i . The second term is the sum of the entropies of the response distributions conditioned on the stimulus: the "noise entropies." In calculating I sep , we are implicitly assuming independence between the total response distribution across cells, as well as independence of the noise distribution across cells. In contrast, I sh is calculated by constructing a distribution where independence between cells is assumed for the noise, and the total distribution is constructed from there. So we haveTherefore, the noise term will be exactly the same for I sep and I sh , so thatSince the sum i H is equal to the joint entropy when responses are independent, this term captures the effects of dependencies of total response distributions across neurons, ignoring the stimulus. This is why Panzeri et al. named this the "signal similarity" term. Since entropy is maximal when responses are independent, this term is always negative. Note thatSince I sigsim  0, this implies that I synergy  I sh . The easiest example for thinking about this term is the case of recording from neurons with overlapping tuning curves. If one neuron responds to a stimulus, then you can assume that the other neuron is more likely than average to also respond, even without knowing anything about the stimulus. This signal similarity serves to reduce the amount of mutual information between the response pairs and the stimulus. Now we need to understand I sh  I  I sh  I corrind  I corrdep . Before we begin, lets write is the number of guesses that we need to determine that the stimulus was s given that we knew the response was r.Now we can writewhere we have used the fact that shuffling does not affect the stimulus probabilities and so H sh  H. NowLets break down this derivation. The first line says that the difference between I and I sh is due to difference in the "noise entropy" for predicting the stimulus from the response for the shuffled and true distributions. In the next two lines, we have bridged the gap between H and H sh in two separate steps. First we take the difference between the number of guesses according the shuffled and true decoding strategies, averaged over the true distribution of stimulus-response probabilities. This is the term I. Writing this term as in the second to last line, shows that I is equal to the Kullback-Leibler divergence between the true condition distribution P and the shuffled conditional distribution P sh averaged over all stimuli. The Kullback-Leibler divergence can be thought of as a measure of the "distance" from probability distribution to another, and can be shown to be positive. Therefore, I is always positive, confirming our intuition that you cant gain any information by using an alternative decoding strategy that is based on throwing away knowledge about correlations.Second, the term I corrind is seen as measuring difference in number of yesno guesses to determine the stimulus using the shuffled coding strategy if one averages over the true distribution vs. the shuffled distribution. One way of thinking about this is that this term depends on whether the true distribution leads to more or less cases where the stimulus is ambiguous given a response as compared to the shuffled distribution.TO BE EXPLAINED BETTER: For a simple case where responses have a metric, Panzeri et al. show that this term is positive when the stimulus and noise correlations have the same sign and negative when they have a different sign. Also need to explain relation to stimulus-dependent and stimulus-independent noise correlations. I dont really understand this in detail now and am not actually sure that this is exactly right. These notes will introduce the rudiments of linear systems theory as applied to computations within networks of neurons. But the brain is highly nonlinear. Wont focusing on linear systems give a distorted and perhaps overly simplistic view of neural processing While this is a danger, there are many reasons to focus on linear systems. The first is entirely practical -the only systems where general techniques provide solutions to wide variety of problems are linear. In studying the linear approximations to brain function we have a host of mathematical tools at our disposal. A second reason is pedagogical -in learning the basic concepts of linear algebra, students will be able to practice the process of putting biological problems into a more abstract framework as well as the process of contemplating the biological implications of insights gained from a more abstract point of view. Finally, and most importantly, a number of our basic notions about how the brain works can be characterized as nearly linear or as "linear-nonlinear." As a result, linear models can go a long way toward clarifying these basic notions. Moreover, one must first understand the linear explanations of neural phenomena before one can grasp the key issues underlying experimental attempts to quantify just how nonlinear the brain is.What does it mean for a system to be linear The most basic definition of linearity is that the whole is exactly equal to the sum of its parts. More technically, a system is said to be linear if it has the property of superposition. For example, suppose we record from a neuron in the visual cortex when presenting stimuli on a computer screen. The neurons response function is said to be linear if the response to the combination of stimuli s 1 and s 2 is equal to its response to s 1 plus its response to s 2, i.e. r  r  rSuperposition also implies that if we change the strength of a stimulus by multiplying the brightness at each pixel by a scale factor c, we get a corresponding change in the strength of the response:Biological Aside. Note that superposition is a formal version of the common expectation that a mixture of inputs gives a mixture of outputs, and that increasing the magnitude of the cause increases the magnitude of the effect. Any time that these expectations are found -and one runs across them in many neuroscience papers -they imply linear thinking.Pushing our example a bit further reveals why linear systems are so easy to analyze. Suppose every stimulus that we presented could be written as a linear combination of a finite number stimuli, s 1 , s 2 , . . . , s N . That is Therefore, to analyze a linear system, one only has to break a system into its parts, understand each part, and recombine the results. The system is then completely understood. The main goal of chapter  will be to find out how to break a linear system into parts so that the process of recombination is as simple as possible.Warning. Using linear as interchangeable with superposition is the most common definition of the term. However, other things are sometimes meant when using the term linear, and the existence of multiple definitions can sometimes lead to confusion. The most common confusion arises when the term linear is used to describe the fact that the relationship between two variables can be plotted using a straight line. As will be shown in problem 8.3.3, in general such a relationship does not satisfy superposition and hence is nonlinear by our definition above. Another use of the term linear that sometimes leads to confusion is using linear to mean "able to be put in strict order" as in the term "linear thinking."Key concept: A system is linear if it satisfies the property of superposition.The bulk of these notes will focus on models using very simple model neurons. While these models ignore a great deal of complexity, they correspond pretty well to the "rough-and-ready" picture of neurons that many neuroscientists use when thinking about computation in complex neural circuits. Because of this correspondence, they can be used to explore many of the basic concepts in systems neuroscience. In these models, the entire biological neuron, including the highly branched dendritic tree, is drastically simplified into a single "compartment," or "processing unit," represented as a circle in most diagrams. The internal state of such a neuron is represented by a single number s. The neuron receives input from N other neurons. This input in turn drives changes in the internal state s. The model neuron produces action potentials at a rate that is some function of the internal state, i.e. output rate r  g. Depending on the background of the author, the function g has been referred to as an output function, a gain function, or a transfer function.Notational Aside. The derivative of the transfer function g is known as the gain. The gain determines how much extra output you can get per unit of extra input, i.e. gain  outputinput. In the limit of small input, outputinput is equal to the derivative, gain  dgds  g, for small changes in the input. Thus, a "high gain" transfer function is one with a steep slope. The simplest example of such a single compartment neuron is the linear neuron. The model neuron receives input from a number N of presynaptic neurons. To calculate the synaptic input current, s j , from each presynaptic neuron j, the presynaptic firing rate q j is multiplied by a weighting factor w j : s j  w j q j . w j determines the strength or weight of the synaptic connection from neuron i. The total synaptic current s is just the sum of all the individual currents:In the linear neuron the transformation from internal state to output rate is extremely simple: output firing rate is defined to be equal to s multiplied by a scaling factor g: and using it to denote a scale factor. In the first case the gain is equal to g.In the second the gain is equal to g.Biological Aside. The linear neuron has many non-biological simplifications. The most glaring of these is the possibility that firing rates can go negative if the neuron receives enough inhibitory input. However, well see that such a simple model can actually be quite useful, especially if we incorporate some simple nonlinearities explained below. In chapter , well explore more realistic model neurons.Biological Aside. A note on units. While the appropriate units for input rates q j and output rate r are obviously sec 1, the units for w j depend on the biological interpretation of the neuron model. We will generally assume that w j transforms the spike rate of presynaptic neuron j into a synaptic current, and g transforms currents into spike rates. Thus, w j has units of sec nA and g has units of nA 1 sec 1 .Notational Aside. Being careful about keeping all your notation around can get pretty cumbersome, so computational neuroscientists often tend to be a bit sloppy. Since the range of values that a subscript can take is usually pretty clear, we often write j w j q j for N j1 w j q j . In cases with only one subscript, even that is sometimes dropped, e.g. w j q j means N j1 w j q j . I will try to be careful to at least keep the subscript.Since we have called this model the linear neuron, it better satisfy the property of superoposition. This is easy to check. The "stimulus" is just the pattern of input firing rates q 1 , . . . , q N  and the response is the output firing rate r. If we have two input patterns q 1 1 , . . . , q 1 N  and q 2 1 , . . . , q 2 N ,Similarly,Note that the linear neuron is "doubly linear" since the transformation from input pattern q 1 , . . . , q N  into synaptic current s is linear, and the transformation from synaptic current s into output rate is also linear.Key concept: A linear neuron computes a linear transformation from input pattern to internal state, as well as a linear transformation from internal state to output rate.Problem 8.2.1 Show that the transformation from the vector of presynaptic firing rates to the total input s is linear. Then show that the transformation from synaptic current to output is linear.As was mentioned in the introduction, one of the key ideas contributed by computational neuroscience is the concept of state space. The mathematics of linear operations on state spaces is known as linear algebra. Thus, to get a deeper understanding of the computational properties of networks of interconnected neurons, well step back a bit from the biology, and introduce the mathematical definitions and concepts basic to linear algebra.A vector space is simply a collection of objects, known as vectors, along with the operations that are important for defining the property of superposition: a method of adding two vectors and a method of multiplying a vector by a scalar. Subtraction can be then be defined as addition after scalar multiplication by -1. We will focus on three different types of vector spaces.1. Each vector is simply a list of numbers v j :Bold lower case letters are used to denote vectors. The jth number in the list, v j , is called the jth component or element of the vector v. When we need to be explicit about the distinction between vectors and scalars, we will write j . Otherwise the scalar v j is assumed to be the jth element of the vector v. Two vectors are added by component-wise addition: j  u j  v j . Scalar multiplication is defined by multiplying the scalar times each component: j  cv j . N is often used to represent the N -dimensional vector space of real numbersNotational Aside. For reasons that will be made clear below, the list of numbers representing most vectors will be written in column form. We call these column vectors. Since we read from left to right, this is rather inconvenient. A row vector is a vector of numbers listed left to right. Luckily there is a convenient notation for the operation of switching between row and column vectors.The transpose is the operation of switching a row vector to a column vector and vice versa. It is represented by the symbol T :2. Each vector is an arrow in two dimensional space, whose base is placed at a special point called the origin. Vector addition can be defined in two ways. First, u  v can be defined as the arrow starting at the origin and ending at the point obtained by setting the vectors tail-to-tip. Alternatively, u  v can be defined as the diagonal of the parallelogram defined by u and v The vector space that you are most familiar with is the one dimensional vector space of real numbers, i.e. lists of numbers containing only one element. Vector addition is the usual addition, and scalar multiplication is the usual multiplication. Looking at this from the geometric point of view, the vector space becomes the one dimensional number line. Note that one has to be careful in using this example since the distinction between vectors and scalars is blurred.Mathematical Example 8.3.2 It was Descartes who discovered the general equivalence between vector spaces 1 and 2, i.e. each arrow in a plane corresponds to a list of two numbers and vice versa. The operations of vector addition and scalar multiplication correspond as well. The same identification can be made in three dimensional space. This idea seems rather commonplace after being around for over 350 years, but its really quite powerful. By identifying lists of numbers with a geometrical object, one is able to use ones geometrical intuitions to solve algebraic problems, and use algebra to solve geometric problems. Moreover, since many mathematical results are true whether the dimension is 3 or 300, one can use geometric intuitions to get insight into high dimensional problems. In fact, much of what is contained in these notes relates to getting a gut feeling of how to convert from arrows to numbers and back again. Most of the examples concern vector spaces of type 1 and 2, where the list of numbers describes the firing rate of a collection of neurons or neural populations, or the strength of the large number of synapses impinging on a given neuron. Vector spaces of type 3 will crop up now and then, often simply to illustrate a mathematical concept.Now that we have defined vector spaces, we introduce some more terms so that we can zero in on the concept of a linear transformation. A function is simply a rule for taking one object as input and producing another object as output. For the function f , we write f  y, and say that "y is a function of x." Functions are sometimes called mappings, since the function tells one how to "map x on to y." Similarly, functions are sometimes called transformations since they "transform x into y." The vector space to which x belongs is called the domain, and the space to which y belongs is called the range. In the example of linear responses in a visual neuron, the domain was the set of all stimuli, and the range was the set of responsesDefinition 2 A mapping is a linear transformation if it satisfies the property of superposition, i.e. f  ff and f  cf. Both conditions are contained in the expressionMathematical Example 8.3.3 A simple one-dimensional example of a linear transformation is y  f  mx. It is trivial to check that this function satisfies superposition:Plotting x vs. y, we see that the graph of this function is a line. Note that the converse is not true, i.e. a function whose graph is a line is not necessarily linear. One can show that every linear map from a one dimensional range to a one dimensional range has the form y  mx for some constant m.Mathematical Example 8.3.4 Another example of a linear function is the operation of taking derivatives of functions. The derivative is a rule for taking a function f and mapping it onto a new function f  df dx . It follows from the definition of the derivative that it is linear. The linearity of the derivative will be important when we address dynamical systems in chapter .Problem 8.3.1 Use figure 8.4a to practice making Descartes equivalence, i.e. vector operations applied to vectors-as-arrows and vectors-as-number-lists are equivalent. Really think about the translation between numbers and arrows. In section , well generalize this process and it wont be quite so trivial, so think hard about what it means to have a coordinate system. The input to the linear neuron was calculated by multiplying the corresponding elements of the presynaptic firing rate vector q and the synaptic weight vector w, and then adding: s  j w j q j . This gives a linear transformation from the vector of input activities q to the total input s. This linear transformation was followed by another linear transformation, i.e. the transformation from input to output, r  gs. The result of concatenating two linear transformations is always linear. This section will present a geometric picture for understanding the first transformation. In the next section, we will examine the ramifications of considering some simple nonlinearities in the inputoutput function. As usual, we will need some definitions.Definition 3 The dot product of two vectors u and v is defined as follows: The dot product is a special case of something known as an inner product, and is sometimes written u, v or uv .From the definition it is easy to show that the dot product is linear in each of its arguments, i.e.. But the easiest way to really understand the dot product is to connect its algebraic definition to the geometry of vector spaces.The length, absolute value, or norm of a vector v is given by v  j v 2 j   v  v. This is just the standard Euclidean length of a vector viewed as an arrow in space, i.e. v equals the distance from the origin to the tip.The distance between two vectors u and v is equal to the length u  v. This is just the Euclidean distance between the tips of the two vectors.Definition 6 A unit vector is a vector whose length is 1. By the above definition, the unit vector in the direction of v is vv. We will use the notation v  vv.Given the relationship between the dot product and vector length, a trigonometry fact can be used to show that u  v  u v cos where  is the angle between the two vectors. Similar reasoning shows that u is the projection of v onto u. u  v is equal to the length of the projection of v onto u times the length of u. Similarly, u  v is equal to the length of the projection of u onto v times the length of v. The dot product also allows us to determine when two vectors are perpendicular: With our new notation we can write the output of the linear neuron in vector notation,where w is the vector of synaptic weightings and q is the vector of presynaptic firing rates. Given our geometrical interpretation of the dot product, the total input to a linear-nonlinear neuron is proportional to the length of the projection of q onto the weight vector w. Thus, in a rough sense, these neurons respond to the "similarity" or "match" between the pattern of presynaptic activity q and the pattern of synaptic strengths w. In fact, if we "normalize" all input vectors so that they have the same length, w  q is proportional to the cosine of the angle between w and q. Under these circumstances, the statement that the distance between w and q is less than a given radius is equivalent to the statement that the dot product is greater than some threshold value . w q1 Notational Aside. For the remainder of this chapter and beyond we will set the gain g  1, so that r is simply equal to the summed input. This can be viewed as a change of units that will simplify the formulas without changing any of the results. Alternatively, we can assume that g has been absorbed into the weight matrix so that w j describes how the presynaptic firing rate gets transformed directly into postsynaptic spike rate.One must be very careful when thinking about the selectivity resulting from taking dot products:Warning. A pattern of strong activity in a direction not closely aligned to a neurons weight vector can give rise to the same amount of input as a pattern of weak activity well matched to the weight vector.. So lets add a constraint on the total amount of presynaptic activity. Suppose we say that the sum of the components j q j  1. Its easy to see that thats not good enough. Focusing on the two dimensional example, all input patterns that have total activity equal to 1 can be written q  p, 1  p T for some p. But then q  w  p2   1  p2. But then as p gets to be a bigger and bigger negative number, q  w increases without bound. To get rid of such solutions with negative components, suppose we also constrain all the q j s to be positive. Under these retrictions, we can give a concrete answer to the question of what input vector gives the best match to the weight vector w  0.5, 1 T : q  0, 1 T . To geometrical way to see why this is the best match is shown in This example points out one of the key problems with simple model neurons. If the total input is calculated using the dot product and activity patterns are restricted to have a fixed sum of activity, then the optimal activity vector is not in the direction of the weight vector. To have the weight and optimal activity vector matched, we need to do something like constrain the size of the activity vector, i.e. constrain the sum of the squares of the presynaptic activities. The problem here is that while extracting the sum of activities is easy to do biologically, extracting the sum of the squares of activities may not be. Well talk more about issues of normalization in chapter .Problem 8.4.1 Show that the dot product is linear in each of its arguments, i.e.  Problem 8.4.4 Construct a linear neuron whose output is equal to the sum of the activities in its input neurons. is linear. Problem 8.4.6 Show that u  v  u v cos where  is the angle between the two vectors. Hint: cos  cos cos  sin sin Problem 8.4.7 Given a that the input vector is positive and that the sum of presynaptic activity is constrained, show that the input vector that has the largest input to a linear neuron is one where activity is concentrated in the presynaptic neuron that has a strongest synapse. Hint: assume a pattern of activity that satisfies the constraint, but where more than one input unit is active. Then show that a slight change in the activity pattern can give even more input.We now consider the geometric interpretation of the computations performed by neurons with inputoutput functions g  g that are nonlinear. The process of converting presynaptic rates q to total input s is still linear, so Ill call them linear-nonlinear neurons.A linear-nonlinear neuron that has a storied history in the field of computational neuroscience is the binary neuron, also known as the McCulloch-Pitts neuron since it was introduced by these two authors in an important paper published in 1943. Inspired by the all-or-none nature of action potential generation, they proposed that during each 1-2 msec time bin, a neuron would have output value 1 if the summed input was greater than some threshold value, and be silent otherwise. In our notation, the inputoutput function g is a step function. McCulloch and Pitts asked the question whether networks of these simple neurons could be constructed to compute any arbitrary logical operation on a set of inputs. From this point of view, the output value 1 corresponds to true, and 0 corresponds to false. So if one wanted to make a network that would compute the truth value of the statement "A and B are true," one could let one input neuron represent the truth of A and another the truth of B. These could be connected to a single output unit with strength 1. If the output unit had a threshold   1.5, the output unit would signal "true" only if both A and B were true. McCulloch and Pitts showed that arbitrary logical operations could be performed by networks of these neurons, as long as the weights were set properly.Biological Aside. Problems with interpretation. MORE. Another common use for networks of McCulloch-Pitts neurons is pattern classification. Suppose that a neuron experiences a range of differnt types of activity patterns across its inputs, and some of these belong to category A, while others do not. This raises the following question: can connection strengths w i and threshold  be found such that the neurons output is equal to 1 whenever it is shown a pattern that belongs to A and the output is 0 whenever the input is not in A In the early 60s,  published an algorithm, the perceptron learning rule, for setting the weights and threshold so that the problem was solved for all categories that were linearly seperable, i.e. in the space of inputs a line could be drawn so that all the patterns belonging to category A fell on one side of the line, while all the patterns not in category A fell on the other. Note that a single McCulloch-Pitts neuron can not be arranged to separate the Bs and Cs, i.e. these clusters are not linearly separable. One of the most widely used linear-nonlinear neurons is the sigmoid neuron. The term sigmoid comes from the fact that the curve looks somewhat s-shaped and sigma is the Greek letter for S. Since it takes a linear input and squeezes it down to fit between zero and one, this kind of inputoutput function is sometimes called a squashing function. The sigmoid neuron can be seen as a compromise neuron that retains the ability to output a continuous range of output rates, but output rates are always positive and there is a upper limit to the allowed spike rates. While most of the time it really doesnt matter what the exact shape of the squashing function is, there are two special functions that I will point out. The first is the piecewise linear inputoutput function. As long as the neuron doesnt cross the "kink" in the transfer function, it is a linear neuron. As such, linear analysis techniques can be applied in piecemeal fashion to networks made up of such neurons.. Using this function, analogies can be drawn between neural networks and statistical mechanics. The input s is viewed as the energy difference between the active and inactive state, and the rate r is the probability of the neuron being in the active state. As s increases, the probability that the neuron is in the active state approaches 1. The parameter T is analogous to temparature. When the temperature is high, random perturbations can knock the neuron back and forth between the active and inactive states, even if there is a signficant energy difference between the states. Thus, squashing functions show a gradual increase in activity when the paramter T is large. At small temparatures, even if s is only slightly different from 0, e will be near 0 or near infinitity. So as T gets smaller and smaller, the sigmoid neuron gets more and more similar to a binary neuron.  The most important linear-nonlinear neuron is the linear-rectified neuron. In engineering, rectification is the process of making an alternating current flow in only one direction. Mathematically, we define rectification as an operation that allows numbers to "flow" in the positive direction, but stops numbers from going negative. We can do this by comparing a number to 0 and taking the maximum: x   max.In real neurons, a certain amount of current is required before the membrane voltage reaches spike threshold and the neuron begins to fire action potentials. To incorporate this biological fact, the linear rectified neuron assumes that the output rate r  0, until the input s reaches a threshold . The output function is then "linear" after that. We write r  g s    . Note that rectification is a nonlinear operation. In fact, it is often underappreciated just how nonlinear rectification is. I would even go so far as to say that rectification due to spike threshold is the most fundamental nonlinearity in neuroscience.So far weve introduced some simple model neurons and an abstract geometric way of evaluating their responses. This section well see how this linear picture relates to a more common way of presenting the responses. Most neurophysiology experiments consist of systematically manipulating some experimental variable, and recording the resulting changes in neural response. Often these results are presented in the form of a tuning curve. A tuning curve plots how a single response variable changes as a function of a single experimental variable. For example, one could record the number of spikes elicited in the motor cortex of a monkey, when the monkey was intructed to touch one of a number of lighted buttons. If the buttons are arranged systematically in a circle, one could make a "motor response tuning curve" by plotting spike number on the vertical axis vs. direction on the horizontal axis. An example of such a tuning curve computed by Georgeopolis and colleagues is shown in The geometric picture gives a qualitative picture of our tuning curves. But using our simplifying assumptions, we can actually write down some formulas. Since the magnitude of the rightward motion depends on the cosine of the angle , the rightward neurons response is given by r right  cos. Similarly, r up  sin. Then, assuming a linear rectified model, r  2 cos  1 sin      5 cos    . Thus, inputs that are a linear function of position naturally give cosine shaped tuning curves in response to stimuli described by a circular variable. This picture is all quite satisfying. In fact, the critical reader should be wondering at this point if things are too satisfying -our model of the inputs is way too simple. The most obvious thing is that we only have two input neurons, conveniently detecting rightward and upward. Later well see that this really isnt that much of a restriction. However, weve also modeled the these detectors as linear. That means that weve represented leftward and downward as negative activities in the rightward and upward detectors. If were thinking of these detectors as neurons, weve got problems since rates cant go below zero.The most common solution to this problem is to assume that neurons come in opposing pairs, and the connections from these pairs are arranged in a "push-pull" arrangement, i.e. if a neuron receives an excitatory connection from one neuron, it receives an inhibitory connection from the other. For example, we can recover the linear picture of figure 8.11b if we assume that our neuron receives an inhibitory input from a leftward neuron that is the same strength as the excitatory connection from the rightward neuron, and we assume a similar push-pull arrangement for an upward and downward neuron.There is certainly plenty of circumstantial evidence for push-pull arrangements. In the visual system. Retinal ganglion cells come in both "ON" and "OFF" subtypes. Even color seems to be represented in paired dichotomies. This is true for red-green. Im not so sure for blue... More directly, for certain types of visual cortical neurons it has been shown that in locations where bright spots elicit excitation, dark spots elicit inhibition and vice versa. In the leech system, the arrangement is a "distributed push-pull," with the four sensory receptor neurons projecting onto 25-30 interneurons which then project onto 10 premotor neurons that contract or expand muscle groups arranged around the animals body. I have to do some reading to gather evidence for push-pull mechanisms across more systems. In fact, this might be the beginnings of a class project for someone...However, there are plenty of problems with this picture. First of all, to give truly linear responses, the pairing would have to be well balanced and all the neurons in question would have to have effective thresholds very near zero, otherwise the response wont look truly linear. A more biological criticism of push-pull as a fundamental property of neural circuits is that patterns of excitation and inhibition in the brain look quite different, not the neat mirror imaging of excitation and inhibition predicted by push-pull. For example, projections from one brain region to another are generally either all excitatory or all inhibitory. Push-pull could still hold, but it would require some detailed circuitry involving local interneurons. A bigger problem for push-pull is that in many brain regions, the number of excitatory and inhibitory neurons are imbalanced. For example, in the cortex excitatory neurons outnumber inhibitory neurons by about 4 to 1. Ultimately, the importance of push-pull in neural processing is an experimental question.To end this section we return the fundamental warning about neurons that compute with dot products: their output reflects an ambiguity between input pattern and input magnitude. The manifestation of the ambiguity in our picture of tuning curves is shown in MORE.Problem 8.6.1 What would the tuning curve of the monkey neuron look like if the rightward and upward neurons were linear-rectified, just like the output neuron Explain your answer using both the dot product and tuning curve pictures in figure 8.11b.Problem 8.6.2 What does the input look like from a push-pull pair in which the effective threshold was not equal to zero For example, draw the input as a function of left-right position for a leftwardrightward pair in which the effective threshold is above or below zero.Iceberg EffectContrast Invariance MORE. Chapter In the last chapter we have examined a single postsynaptic neuron receiving input from an array of N presynaptic neurons. Now we extend this picture to consider an array of P output neurons as well. Note that the two "layers" of processing units may have different numbers of neurons. We write all the strength of all possible connections between input neurons j and output neurons i in a compact, two dimensional array: W is known as a weight matrix. A matrix with N rows and P columns is said to be an N  P matrix. Displaying the network as in the righthand side of figure 9.1a makes the correspondence between connection strength and an array of numbers easy to see.Network Aside. For obvious reasons, these networks are commonly referred to as "two-layer" networks. However, some researchers who focus more on the patterns of weights than on patterns of activity, would refer to these networks as "single layer" networks, since they have only one layer of connection weights. This confusion is extended to multi-layer networks, with the same network called a three-layer network or a two-layer network depending on whose convention is being used. We will classify networks by the number of layers of processing units. This terminology is most common.Notational Aside. Mathematicians often use the notation f : N  P to abbreviate the statement "the function f maps vectors in the domain N into the range P ," or more tersely "f maps N into P ." For example, single neuron models perform a mapping f : N  1. We want to define matrix multiplication in such a way that the product Wq  r denotes a linear conversion of a pattern of inputs into a pattern of outputs. If look at the problem from the point of view of the output layer, each output neuron is acting independently. From this perspective, we can construct the output pattern element by element, calculating the activity of each output element in turn. Given the way we have written the matrix W, the vector of synaptic weights impinging on output neuron i is the ith row of W. We will denote this row vector by W i: . Therefore, the activity in the ith neuron, r i , is determined from taking the dot product of the ith row of W and the input vector q:Note that this definition can be applied to our previous case where the output layer had only one neuron. In this case, W is a 1  P matrix. If we let w be the vector of weights for this neuron, we have W  w T . Since r  w  q using our vector notation, and r  Wq using matrix notation, w  q  w T q. In other words, given our definition of matrix multiplication, the dot product of two vectors u and v can be written as the matrix product u T v. We will use both the "transpose" and "dot" notations to denote this operation.Since each output element is treated separately, from the postsynaptic perspective it can be difficult to understand how presynaptic activity gives rise to a pattern of outputs. So now we focus on what each presynaptic neuron contributes to the final answer. Examining figure 9.1a reveals that the vector of synaptic weights emanating from the jth input neuron can be found in the jth column of W. We denote this jth column vector W :j . By writing out the sumwe see thatIn other words, the final output pattern r is the sum of the column vectors W :j , weighted by the presynaptic activity levels q j . That is, each presynaptic neuron drives the output toward its vector of outgoing weights W :j , with a strength that is proportional to its activity level q j .Example 9.2.1 Consider the simple example whereThe fact that W has three rows and two columns indicates that the input layer has two input neurons and the output layer has three neurons. Suppose that this network has an input activity vector q  1. Mathematical Aside. At this point it may be useful to pause and take a broad view of how far weve come toward understanding linear transformations. We started by looking at a single neuron adding up individual synaptic currents from a number of presynaptic neurons. We then made the conceptual leap to where the entire pattern of presynaptic activity was considered as a single object, a vector. In example 9.2.1 we get a glimpse of how an entire collection of vectors can be viewed as a single object, a subspace. Subspaces will play a key role in understanding the nature of the Hebbian learning rules that we will introduce in the next chapter.Problem 9.2.1 Show that the transformation from inputs to outputs in the two-layer network is linear.  Problem 9.2.3 All planes that can be drawn in 3 do not constitute a linear subspace from the vector space point of view. What is the key condition that distinguishes planes in 3 that are linear subspaces from those that arent.So far we have described how to add two vectors, multiply a vector by a scalar, "multiply" two vectors using the dot product, and multiply a vector by a matrix. Like vectors, the addition of matrices is done by adding corresponding elements, i.e. ij  W 1 ij  W 2 ij . Note that matrix addition is only defined if both W 1 and W 2 have the same size. Multiplication of a matrix by a scalar is also defined element-wise ij  cW ij . Now we go on to describe how to multiply two matrices.Network Example 9.3.1 The three-layer linear network. Suppose we add another layer of neurons to our two-layer network. Since this third layer is the final processing stage of our network we will call it the output layer. We rename the second layer the hidden layer since activity in this layer represents the internal workings of the network and is therefore "hidden" from an outside viewer that can only look at inputs and outputs. This network has two layers of synaptic weights, the matrix W of weights connecting the input and hidden layers, and the matrix T of weights connecting the hidden and output layers. The first thing to notice is that the transformation from input pattern to output pattern is linear. This can be shown as follows:We define the matrix product TW as the matrix that implements the transformation v  T. Similar to our two-layer network, we want the kjth entry of TW to represent how strongly input neuron j effects output neuron k. In the two-layer case, this influence can be interpreted as a synaptic strength. But now there are multiple pathways connecting these two neurons: input neuron j affects hidden layer neurons and these in turn influence output unit k. The matrix product of an N  Q dimensional matrix T and a Q  P dimensional matrix W is the N  P matrix TW whose kjth entry is given by kj  i T ki W ij .Note that kj  T k:  W :j . This equality gives an intuitive interpretation of matrix multiplication, the kjth entry of the product matrix TW represents how well the vector of weights from input neuron j matches the vector of weights onto output neuron k, where the match is measured using the dot product. We also point out that although we have added another layer of neurons, we havent gained any information processing power: the transformation from inputs to outputs is still linear, and hence could have been implemented by a matrix of direct connections from input to output neurons. However, if the hidden units are not linear, for example if they have a threshold, then it can be shown that any arbitrary mapping from inputs to outputs can be implemented, as long as there are a sufficient number of neurons in the hidden layer. The above perspective provides an element-by-element interpretation of the product matrix TW, namely kj represents the net connection strength from input unit j to output unit k. There is another way to interpret the matrix TW that focuses on the hidden layer. The matrix TW represents the net effect of how a pattern at the input layer affects a pattern at the output layer. Suppose we want to examine the component of this net effect that passes through a given hidden unit i. To do so we simply ignore the other hidden units and examine the three layer network with just the one hidden unit i NEEDS BETTER Mathematical Aside. The operation of matrix multiplication is so fundamental, that it is important to learn the mechanics of performing this operation, as well as getting an intuitive understanding of what it means. If we write out T and W as arrays of numbers, we see that kj is simply the dot product of the kth row of T with the jth column of W. Of course this means that the "width" of T must equal the "height" of W. Also notice that multiplying a matrix times a vector is just a special case of matrix multiplication where the vector is viewed as an N  1 dimensional matrix. Even the dot product u  v is just the matrix product of the 1  N dimensional matrix u T and the N  1 dimensional matrix v.Problem 9.4.1 Practice doing matrix multiplication. Make up two example matrices, each with 2-5 rows and columns of entries that are single digit integers, and multiply them. Do this enough times that you really get the hang of the mechanics.Problem 9.4.2 After waiting at least one day, repeat problem 9.4.1, making up some new examples. The mechanics of matrix multiplication is one of the very few mathematical operations in this class that you should learn how to actually do it, rather than just learn what its about.Problem 9.4.3 Use vector and matrix notation to reformulate equations 9.6 and 9.7 into a simple vector equation. Discuss the interpretation of the matrices and vectors used in this equation. Hint: the equation should look like the equation for a three-layer network.We will write a vector as a bold-faced small letter, e.g. v this denotes a column vector. Its elements v j are numbers and hence are written without bold-face:Here N , the number of elements, is the dimension of v. The transpose of v, v T , is a row vector:The transpose of a row vector, is a column vector in particular, T  v. To keep things easier to write, we often writeWe will write a matrix as a bold-faced capital letter, e.g. A its elements A ij , where i indicates the row and j indicates the column, are written without boldface:This is a N P matrix, i.e. it has N rows and P columns. An N-dimensional vector can be regarded as an N  1 matrix, while its transpose can be regarded as a 1  N matrix. A square matrix is a matrix with the name number of rows as columns. The transpose of A, A T , is the matrix with elements A T ij  A ji :Note that the transpose of a N  P matrix is an P  N matrix. A square matrix A is called symmetric if A  A T  that is, if A ij  A ji for all i and j.The N dimensional identity matrix I is the matrix such that Iv  v for all N -vectors v. I is an N dimensional square matrix with 1s along the diagonal and zeros elsewhere. We will generally use 0 to mean any object all of whose entries are 0. It should be clear from context whether the thing that is set equal to zero is just a number, or a vector all of whose elements are 0, or a matrix all of whose elements are 0. So we abuse notation by using the same symbol 0 for all of these cases. Occaisionally we will use the symbol 1 to denote the matrix or vector all of whose entries are 1.The definitions of matrix and vector addition are simple: you can only add objects of the same type and size, and things add element-wise.u  v is the vector with elements j  u j  v j . Addition of two matrices: A  B is the matrix with elements ij  A ij  B ij . Subtraction works the same way:Scalar multiplication is also applied elementwise: j  cv j , ij  cA ij .The dot product of two vectors u and v is defined as u  v  j u j v j . Note that the dot product is only defined for vectors of the same length.The multiplication of two objects A and B to form AB is only defined if the number of columns of A equals the number of rows of B. Note that this means that order matters To form AB, take row i of A, rotate it clockwise to form a column, and take its dot product with column j of B. That gives a single number, entry of the resulting output structure AB. In other words, AB is the matrix with elements ij  k A ik B kj  A i::jMatrix multiplication is associative, i.e.C  A, but it is not commutative, i.e. AB  BA even in cases where both AB and BA are defined. Both scalar and matrix multiplication are distributive over addition, i.e.Chapter 10Recurrent NetworksSo far, we have only considered "feedforward" networks, where the activity in the input layer determined the activity in the next layer and so on. We now introduce "feedback" or "recurrent" networks. As opposed to feedforward networks, which only had connections between layers, recurrent networks have synaptic connections between neurons in the same layer. The standard example that we will consider has two layers of neurons with recurrent connections in the output layer Note that given an input pattern, we cant directly compute the outputs because the output variables r i fall on both sides of the equation. This is just a mathematical restatement of the fact that the neurons are reciprocally connected.The easiest way to proceed is to rewrite the equation using vector notation:To get further well need to introduce a useful and important matrix:Definition 9 The identity matrix I is the matrix such that for all vectors v, Iv  v.I acts like the number 1 in ordinary multiplication, and is a square matrix with 1s along its diagonal and zeros elsewhere. Now we can define the inverse of a matrix W to be the matrix W 1 such that W 1 W  I. Returning to the recurrent network equations we can write,Therefore, the net effect of the recurrent connections is to perform a linear mapping, T  1 , on the pattern Wq of total synaptic input arriving at the output layer. In this chapter we will focus on how the matrix T transforms the total feedforward input Wq. For simplicity we will replace Wq with q, and interpret it as the pattern of the total input arriving from the input layer.Biological Aside. In trying to understand information processing within hierarchical networks, it is often useful to conceptually divide inputs into feedforward or bottom-up inputs coming from the previous processing layer, lateral or recurrent inputs from other neurons at the same layer, and top-down inputs coming from the next layer up the processing hierarchy. Note that the term feedback connection can be used for either lateral or top-down connections. Inputs that cannot be easily put within this hierarchical framework are often thought of as "modulatory" inputs. Parsing the role of the different inputs in the visual cortex has been the subject of a number of experiments and models.For "well-behaved" recurrent matrices T,We define T 2 to be the matrix TT, T 3  TTT, etc. Equation has a natural biological interpretation. The activity in the output layer results from the direct input, q  Iq, plus the input resulting from q being passed through the recurrent weights, Tq, plus the input resulting from this activity being passed through the recurrent weights again, TTq, etc. In other words, T ik describes the degree to which the feedforward input arriving at the kth neuron drives the output of the ith neuron, by summing up over all possible pathways of information flow.Problem 10.1.1 Show that the identity matrix for N dimensional vectors is an N  N matrix, with 1s along its diagonal and zeros elsewhere. Hint: show that for every way in which the assumptions would not hold, one could produce a vector v with Iv  v.Problem 10.1.2 Justify the expansion in equation under the assumption that lim n T n  0.We will present the main techniques related to linear recurrent networks in the context of a simple example containing only two output units Definition 10 A matrix M is a symmetric matrix if M ij  M ji for all i and j.Symmetric matrices have a number of very nice mathematical properties and can be found in many network models. We will discuss these properties below. While the assumption of symmetric connections is not supported at the level of single neurons, it is often a reasonable assumption at the level of neural populations. Note that if the matrix M is symmetric, the M 1 is symmetric as well.Using this formula we find thatProblem 10.2.1 Confirm the validity of equation.The network becomes significantly simpler if we remove the connections between the two outputs, i.e. T 12  T 21  0. Using the equations above,Since the only non-zero terms in the matrix lie along the diagonal, we say that T is a diagonal matrix. The solution to the equation r  Tq is given byThe network simply multiplies input i by a gain factor of 1. If T ii  0, then the gain is 1 and the network simply replicates the input. As T ii grows larger and approaches 1, then the gain 1 approaches infinity. At this point the positive feedback from self-excitation makes the system unstable. We will explore the issue of stability in chapter . For negative values of T ii , the recurrent connections implement negative feedback and the gain is less than 1.In many two dimensional problems, it can be useful to change coordinates to "sum and difference coordinates."Example 10.3.1 Suppose we perform the following hypothetical experiment, aimed at determining the "binocularity" of visual cortical neurons. We first present a stimulus to the right eye, counting the number of spikes produced by that neuron in response. We then present the same stimulus to the left eye and note the response. For each neuron, the outcome of the experiment can be described by a two dimensional vector r  r right , r lef t  T . But the same information could be represented using a different set of coordinates: r  r sum , r dif f  T , where r sum  r right  r lef t and r dif f  r right  r lef t . The difference coordinate is one measure of binocularity of the cell, i.e. how much the cell responds more to one eye than the other. The sum coordinate captures the total responsiveness of the neuron to the stimuli presented.Lets follow this sum and difference approach and consider an input vector q  2, 6 T . We can use vector notation to write the sum q 1  q 2  8 as . Consider the input patterns e i where input i has magnitude 1 and the other input is 0. Then the input vector q  2e 1  6e 2 . Now let d 1 be the input pattern where both inputs are at 12, and let d 2 be the input pattern where input 1 is equal to 12 and input 2 is equal to -12. Then q  8d 1  4d 2 .Definition 11 A basis B for a vector space V is a set of vectors B  v 1 , v 2 , . . . , v n  in V such that any vector u in V can be written as a linear combination of vectors in B. The vectors in B are said to span V . If we write u  i c i v i , the list of numbers c 1 , c 2 , . . . , c n  T B are the coordinates of u in the basis B. If the vectors in B are mutually orthogonal and normalized, then the basis B is said to be orthonormal. e 1 , e 2  is the standard basis in two dimensions, and d 1 , d 2  is the appropriate basis for the sum and difference coordinates. Note that we can writeGenerally, when we write a vector as a list of numbers, there usually isnt any special notation. Most commonly, it is implicitly assumed that coordinates relate to the standard basis. In other cases, the basis is generally clear.Orthonormal bases have a number of nice properties. In particular, if B  v 1 , v 2 , . . . , v n  is an orthonormal basis then u  iv i for any vector u. Since the basis vectors are normalized, coordinate i is simply the length of the projection of u onto the basis vector v i . Geometrically, the vector e 1 lies along the x-axis and e 2 lies along the y-axis. Since the standard basis is orthonormal, associating coordinates and vectors follows the cartesian procedure shown in The details of changing coordinates can get a little confusing. For that reason, Ive separated it into sections 10.7.1 and 10.7.2 below. You can get by with the material I presented here.Problem 10.3.1 Write the standard basis vectors in sum and difference coordinatesProblem 10.3.3 Show that for any basisNow that weve clarified the relationship between vectors in general and lists of numbers, we need to clarify the relationship between matrices and the linear transformations that they implement. To keep things simple lets consider a linear transformation M that takes vectors in two-dimensional space and transforms them into other vectors in twodimensional space. Suppose we start with the standard basis e 1 , e 2 , and use our procedure for matrix multiplication.So, given a linear transformation M and a basis B  v 1 , v 2 , . . . , v n , we can represent M as an array of numbers such that M ij is the ith coordinate of the vector v j in the basis B. That is,For an example of expressing a matrix in new coordinates, lets return to our simple recurrent network example, and suppose that each output unit has the same pattern of connectivity. Therefore, the matrices T and T are symmetric. Moreover, T 11  T 22 . To be concrete lets consider the matrixwhere we have used the subscript S to specify that the transformation T is written using the standard basis. Now lets see what happens if we apply the matrix T to the sum and difference basis vectors We say that the basis D diagonalizes the transformation T. In diagonalizing T, sum and difference coordinates transform the problem back into a noninteracting case. However, the lack of interaction is not between individual output units but between separate patterns of activity. Thus, T has the effect of compressing the sum of the inputs by a factor of two, no matter what the difference between input 1 and 2, while scaling the difference between the inputs by a factor of 2.5, no matter what the sum of the inputs might be. Geometrically, the vectors d 1 and d 2 have the very special property that multiplying by the matrix T simply scales these vectors without changing their direction. Such vectors are called eigenvectors and the values .5 and 2.5 that represent how much these vectors are scaled are the corresponding eigenvalues. It is important to point out, that the definition of an eigenvector and eigenvalue is "coordinate free", i.e. the eigenvectors of a transformation will be the same vector no matter what coordinates they are expressed in and the corresponding eigenvalue will always be the same. If one can find a basis consisting of eigenvectors, this basis is called an eigenbasis. Thus, the basis d 1 , d 2  is an eigenbasis for the transformation T.This example illustrates a fairly general strategy for solving a number of problems in linear algebra. If one can find an eigenbasis for a linear transformation then one can view the transformation as a series of independent scalings in a number of directions. Luckily, any symmetric matrix M has an eigenbasis. Moreover, one can find an orthonormal eigenbasis for M. This property makes symmetric matrices particulary amenable to analysis, and that makes them particularly common in the field of computational neuroscience. However, it is important to remember that not all matrices are symmetric, nor do all of them have an eigenbasis.In our example, we didnt use any procedure to find the eigenvectors, we just applied the matrix to the sum and difference basis vectors and confirmed that indeed these were eigenvectors. While this seems like cheating, guessing the answer and then proving it works is a tried and true methodology in applied mathematics. On further reflection, the sum and difference coordinates werent such a far out guess. This is because they are aligned with the symmetries of the problem. In particular, the connectivity of each output unit is identical. Therefore, the solution to the problem shouldnt be different if we switch the indices 1 and 2. Geometrically this means that the eigenvectors should be the same if we exchange the x and y axes. The sum and difference eigenvectors lie in the two directions that arent changed by such a permutation. Although this method of exploiting the symmetries of the problem is successful in many cases, it isnt always so.There is a more general method for finding eigenvectors and eigenvalues that is presented below in section 10.7.3 for the interested reader. More common nowadays is to rely on software packages to solve for the eigenvectors and eigenvalues.Problem 10.4.1 Find the matrix T that gives the specific T in equation.Problem 10.4.2 Write out the first few terms in the expansion T  1  I  T  T 2  T 3  . . .. Use the specific values for T found in problem 10.4.1.Problem 10.4.3 Find the conditions on a 2  2 symmetric matrixthat ensures that the sum and difference basis is an eigenbasis for M.Problem 10.4.4 Another set of coordinates that is often convenient in two dimensional problems are the average and deviation coordinates:   q 1  q 2 2 , q 1  q 2 2 Find the basis vectors for this coordinate system. Problem 10.4.5 Show that any matrix is diagonal when expressed in its eigenbasis, and the eigenvalues are the entries along the diagonal.Problem 10.4.6 Find an orthonormal eigenbasis for the matrix T in equation.In our example, the effect of the recurrent network was to expand difference between the inputs by a factor of 2.5. Such an expansion of the difference is expected since the connections between the output units are negative. Thus, each unit tends to inhibit the other one, increasing the difference in activity levels. However, if we takeOnce again the sum and difference coordinates form an eigenbasis, with the sum direction eigenvalue equal to 4, and the difference eigenvalue equal to 2. So the network still expands the difference between the values even though there is a positive connection between the two output units. Why is this so The easiest way to clarify the issue is to go back to a non-interacting network and take T 11  T 22  12. Then T  2 0 0 2  2I Therefore, T simply scales the input vector by a factor of two without changing the relative magnitude of the two components. But of course this uniform expansion will increase the difference between the components as well as increasing their sum. What is needed for the network as implementing a true competition between the inputs is for the difference to increase more than the sum. One can show that this happens exactly when the connections between the units are negative.Having an eigenbasis for a linear transformation makes it quite easy to get a geometric picture of the transformation. In particular, the eigenvectors determine the directions in space along which vectors are "stretched" will the eigenvalues giving the magnitude of the stretch. Negative eigenvalues lead to a "flip" along with a stretch. FIGURE It is hard to overemphasize the importance of eigenvectors for computational neuroscience. For any system that is linear or nearly linear, the eigenvectors are a guide to breaking a problem into its component parts. While it is not absolutely necessary to understand the mechanics of finding the eigenvectors, understanding the conceptual importance of eigenvectors is key to understanding a number of papers in the literature.Problem 10.5.1 Show that the whether the difference component increases more than the sum component depends on the sign of the connection between the two output units.Although we relegate the details of some of the linear algebra to the section below, there are a two important quantities that one can extract from square matrices that are invariant under a change of coordinates. Weve already seen such quantities, namely the eigenvectors and eigenvalues of linear transformation are the same no matter what coordinates are used to express them. Since the eigenvalues, lets call them  1 ,  2 ,  3 , . . ., dont change under a change of coordinates, it is not surprising that their sum and product i  i and i  i are also coordinate free. What is surprising is that these numbers can be extracted in a relatively straightforward way from the entries in the matrix describing a linear transformation, and these do change as one changes coordinates.For example, one can define the trace of a matrix M as the sum of the diagonal elements, Trace  i M ii . If one expresses a linear transformation using the eigenbasis, then the corresponding matrix is diagonal, and the trace is equal to the sum of the eigenvalues. But the trace is coordinate free. That means the sum of the diagonal elements of a matrix does not change as one changes coordinates, even though the individual entries may indeed change. In our original example above, Trace can be computed from matrix in the original coordinates or after it had been diagonalized. Thus one can get some information about the eigenvalues of a matrix, even before finding the eigenvectors.A second quantity that can be extracted is the determinant. The formula for the determinant for a general square matrix is a bit complicated. For a 2  2 matrix it is equal to M 11 M 22  M 12 M 21 . For matrices with an eigenbasis, the determinant is equal to the product of the eigenvalues. Therefore, the determinant is coordinate free and applying the formula to the entries of two different matrices will come up with the same answer if the two matrices express the same underlying linear transformation in different coordinates. A geometric way to interpret the determinant is as the expansioncompression ratio for volumes under the linear transformation, i.e. it is the volume of the image of a cube with area 1. If the determinant is negative, then it tells you that the transformation has a net flip.In this section we will expand on the above ideas with a bit more rigor.SECTION NEEDS REWORKINGCOMPLETION.The goal of this section is to determine how to take a vector expressed as a list of coordinates in one basis, the "old basis," Q  q 1 , q 2 , . . . , q N , and express it as a list of coordinates in another basis, the "new basis," R  r 1 , r 2 , . . . , r N . In other words, suppose we are given a vectorT describes the same vector v relative to the new basis R. To do this we must assume that we know how to express the new basis vectors r 1 , r 2 , . . . , r N  as lists of coordinates in the old basis Q. The task will be to use that information to be able to change coordinates, i.e. to take any vector that is given as a list of old coordinates, and transform that list so that the vector is expressed as a list of new coordinates. Note that expressing the new basis vectors as coordinates in the new basis is trivial:The transformation that takes in lists of numbers representing vectors in the old coordinates and transforms them into lists of numbers in the new coordinates is a linear transformation. Therefore, changing coordinates can be accomplished by matrix multiplication. We will denote the matrix that transforms vectors in the basis Q to the basis R asNote that solving the reverse problem is easy: if we are given a vectorT expressed in the basis R, it is easy to express v in the basis Q. By definition, we have the following vector equation:To get the coordinates of v in the basis Q we simply add component by component:These equations can also be expressed in matrix notation, asi.e. the columns of Q C R are the coordinates of the basis vectors r 1 , r 2 , . . . , r N  expressed in the basis Q. We now return to the problem of finding the matrix R C Q . If we start with a vector v in the old coordinates, transform it to the new coordinates by multiplying by R C Q , and transform it back to the old coordinates by multiplying by Q C R , we should get back to same vector. Mathematically,T . In other words, R C Q  Q C 1 R . As before when we discussed the optimal population decoding, we will not describe how to actually compute the inverse. We do note, however, that Given the definition of matrix multiplication, equation says that the kth coordinate of the vector v in the basis R is equal to r k  v. Equation can also be interpreted geometrically: the kth coordinate of a vector v in the orthonormal basis R  r 1 , r 2 , . . . , r N  can be found by projecting v onto r k . When the vectors r k are not orthonormal, the change of coordinates is accomplished by projection onto the basis vectors r k , followed by a compensation for the correlations among the basis vectors via multiplication byNow we have clarified the relationship between a vector and a list of numbers that describes that vector, i.e. we know how to express vectors in any given coordinate system or basis. We also know how to change coordinates so as to express this same vector using a different set of numbers. We have also introduced matrix multiplication as a way of performing linear transformations from one vector space to another. Certainly, when we choose to express vectors in different coordinates, the elements of the matrix defining a given linear transformation must also change. Again, we have disconnected the notion of an abstract object -in this case a linear transformation -and the numbers used to express that object. Suppose we have a linear transformation W : N  P , and we have a basis Q  q 1 , . . . , q N  for N and a basis R  r 1 , . . . , r P  for P . Let R W Q be the matrix of numbers that represents W using coordinates obtained from expressing vectors in these bases. Now suppose we express vectors in N in a new basisQ and vectors in P in a new basisR. How do we findRWQSuppose we are given vQ, a vector in N expressed in the new basisQ. To calculateRWQvQ we will simply take a round-about path. First we translate vQ back into the original coordinates by applying the matrix Q CQ as calculated in the previous section, i.e. v Q  Q CQvQ. Now we use the matrix of numbers R W Q representing the transformation W in the original bases to transform v Q into a vector R W Q v Q  R W Q Q CQvQ. This vector will be a vector in the image space P expressed in the old basis R. Now we translate this vector to the coordinates for the new basis R, i.e. we applyRC R . We finally arrive at the vectorRC R R W Q Q CQvQ. Since this procedure works for any vector vQ, we have shown thatRWQ R C R R W Q Q CQ.The notation here can get rather hairy, so lets strip some of it away and make the problem a bit simpler. Suppose we have a linear transformation that maps N  N , and we let W be the matrix that represents that transformation when vectors are expressed using the standard basis. Now we want to find the matrix that represents the transformation when we transform to a new basis for N . If we let C be the matrix that changes coordinates from the standard basis to the new basis, then C 1 changes coordinates to the new basis back to the standard basis. But then CWC 1 represents the transformation in the new basis since it transforms coordinates back to the old basis by applying C 1 , performs the linear transformation in the old coordinates, and changes the answer into the coordinate system.FOR NOW SEE HIRSCH AND SMALE OR ANOTHER LINEAR ALGEBRA TEXT.In chapter 9 we saw how networks can implement a linear transformation of a pattern activity across a number of input neurons into a pattern of activity across a number of output neurons. The values for the synaptic weight matrices were assumed to be given. In this chapter, we examine the synaptic connection matrices arising from associative learning rules. This and then go on to discuss the decoding problem where the task is to determine the input pattern that gave rise to a given pattern of outputs. focus on the structure of linear transformations.Associationism has a long history in the study of the mind. Some credit Aristotle with making the earliest arguments that making associations between events in the world is the key to knowledge. Others trace the roots of this tradition to the philosopher David Hume of "tabula rasa" fame. There was a great rise of associationism in the nineteenth century, and it lies at the heart of William James "When the axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such the As efficiency, as one of the cells firing B, is increased." This proposition has led to a number of mathematical learning rules, the simplest of which iswhere W ij is the change in the weight connecting neuron j to neuron i and r i and q j are the firing rates of neurons i and j.  determines how much the weight changes during each association and hence the speed at which change takes place. If is often called the learning rate. One way to interpret this is that each time neuron j fires an action potential, the weight is increased in proportion to the activity of neuron i.Historical Aside. The Hebb rule could easily have been known as the the James rule if computational neuroscience had been developing as rapidly at the turn of the century as it was in the 1950s. For example, James wrote "When the two elementary brain-processes have been active together or in immediate succession, one of them, on re-occurring, tends to propagate its excitement to the other."In 1973, Bliss and Lomo first published evidence for a biological mechanism leading to associative change in synaptic strength between neurons. Given the centuries-long suggestion that such a mechanism could underlying learning and memory, this discovery lead to much excitement and an ongoing experimental effort to understand the cellular and molecular underpinnings of the long term increases or potentiation in synaptic strength that they called LTP. The associative nature of LTP is due to two important properties of a synaptic receptor called the NMDA receptor. When the neurotransmitter glutamate is released from the presynaptic terminal of many synapses in the brain, it binds to two kinds of postsynpatic receptors. Binding to the first kind of receptor, the AMPA receptor, leads to a rapid increase in current in the postsynaptic cell, possibly contributing to spiking in this cell. The action of the NMDA receptor is more complicated. At potentials below threshold, the NMDA channel is blocked by Magnesium ions. However, this block is voltage dependent, being relieved if the postsynaptic cell is depolarized to near or above threshold. Therefore, to pass current NMDA channels need to bind presynaptically released glutamate and to be unblocked by postsynaptic depolarization, for example by the action potential generated by the postsynaptic cell. The second important property of NMDA channels, is that part of the current that they pass is carried by calcium ions. A host of experimental results indicate that calcium then lead to a cascade of cellular events and that this eventually leads to LTP. The action of the NMDA channel is the basis for the multiplication of equation -potentiation of synaptic strength only occurs if the presynaptic activity q j  0 and postsynaptic activity r i  0. However, if we focus on the mathematical implications of equation, we notice one obvious drawback: given that firing rates are positive quantities, equation implies that synaptic strengths can only increase. On the face of things, biology seems to come to the rescue: in 19XX,  discovered the phenomenon of long term depression. By stimulating the presynaptic neurons at a lower intensity, they were able to show that synapses can be made weaker. Subsequent experiments have led to the general hypothesis that low levels of calcium lead to LTD whereas high levels lead to LTP. This is often written as W ij  q j Equation can be interpreted as follows. Each presynaptic spike leads to the binding of glutamate at the synapse. The amount of calcium let into the cell is proportional to the postsynaptic activity, and the postsynaptic change is proportional to calcium influx, minus a threshold. Since the number of such events is proportional to q j , we arrive at equation. Again, the constant  determines the rate of synaptic change. The threshold in equation This makes the presynaptic term negative for synapses from inactive neurons, leading to LTD. One can write down a Hebbian learning equation that includes both forms of LTD:There are a couple of things to note about equation. First, if one sets the threshold to be the mean value of pre and postsynaptic activity, then the change in synaptic strength is related to the covariance of pre and post activity rather than raw pre-post correlation. Second, while adding the thresholds makes sense in the context of certain specific experiments, equation leads to synaptic changes that are non-biological. For example, if there is no activity in the presynaptic or the postsynaptic neurons, equation says that synaptic strength should increase. However, much like the linear neuron, as long as one is careful equation can be used to illustrate basic features of Hebbian learning. While the existence of LTD allows for the possibility for weights to go both up and down, well see below that it doesnt really do much to solve the problem of weights continuing to increase without bound. For now well just wave our hands and assume that some biological mechanism keeps the weights in a reasonable range.The connection between associative learning, equation, and LTP was a major factor leading to the vision, popular in the 1980s, that psychological, computational, and biological approaches to understanding the brain were rapidly converging. But weve actually got a long way to go. Slight complexifications of equation are still the dominant formulation of biological-based learning rules, but these rules have found limited application and the computational community had largely passed them by. Furthermore, even though there has been vast increase in our knowledge of the cellular mechanisms underlying LTP, clear insight into how to steer equation and toward biology has not been forthcoming. Finally, while associationism is still a hotly debated issue in the psychology community, the connections to biology and computation havent been significantly strengthened in the last 20 years.Returning to equation, lets see how it gives a simple account of certain phenomenon that can be characatured using our two-layer linear network. In our first example, well return to the turn of the 20th century and provide an formal associationist account of Note that using our notation from last chapter we can writeIn chapter 8 we defined the inner product of two vectors as u T v. The inner product takes two N  1 dimensional vectors and yields a scalar. Now we defineThe outer product of two vectors u and v is the matrix uv T . If u is an N  1 dimensional vector and v is P  1 dimensional, then uv T is an N  P dimensional matrix.Auditory Inputs Now what happens if we present the bell alone In our linear network,If r bell is appropriately normalized so that r bell  2  1, ringing the bell leads to salivation.For now we will leave issues of normalization aside, both in the weights and in the activitieswell come back to them in chapter xx. Then the outer product rule can be used to give an abstract sketch of how an associational mechanism like LTP might play a role during various learning tasks.To view the outer product rule as contributing to operant conditioning, we again view the input layer as representing some sort of sensory input and the output layer as representing motor commands that lead to behavior. But now learning is "gated" by a reinforcement signal so that without this signal no learning takes place. But then when the animal happens to stumble across the proper behavior, a reinforcement signal comes in and says "now print." In this way, specific stimulus-response parings can be learned, just as in classical conditioning. The main difference is that learning isnt restricted to learning behavioral responses in the animals innate repertoire. Animal trainers have been used this type of learning for eons.Strict behaviorists viewed the main function of the brain as taking in a sensory stimulus and responding with the motor behavior that led to the greatest reward. This overall viewpoint is still implicit in many studies of the nervous system. But it is obvious that information can flow in the opposite way. For example, when planning or performing a motor task, you dont need to wait and see what happens, you are able to generate a sensory expectation. Such expectations are crucial for obtaining the fast and fluid behavior that is necessary to survive in the world. How can these be learned Let r motor be a pattern of premotor activity leading to some behavior, say extending your arm. That pattern of activity will regularly be followed by a pattern of sensory input r sensory corresponding to seeing and feeling your arm being extended. But then if connections W from the motor area to the sensory area are strengthened according to the outer product rule, W  r sensory T , subsequent performance of r motor will lead to an internally generated signal that carries the expectation of what the sensory input will be. This strategy can be used to learn a rich understanding of the capabilities of ones own body, just by randomly flailing around. Babies do quite a bit of this "motor babbling," and, as the name suggests, this is an important part of the learning complex motor behaviors such as speech. Another important functional role for motor-sensory matching, is the ability to tell the difference between sensory input that are generated by ones own actions, and those generated by external events out in the world. For example, the sensory experience one has when the entire visual world moves as a result of turning your head, is quite different than when the world moves on its own, as anybody who has experienced seasickness will tell you. In this context, the sensory signal generated from a motor command is sometimes called an efference copy.The same idea used for sensory-motor matching can be used as a basic explanation of learning associations between different sensory modalities. For example, the sound of a dog barking is most often accompanied by the visual image of a dog. Hebbian learning between the auditory representation for "bark" and the visual representation for "dog" can be used to strengthen the connections between the neurons involved in these representations.Problem 11.2.1 Confirm that if u is an N  1 dimensional vector and v is P  1 dimensional, then uv T is an N  P dimensional matrix.Problem 11.2.2 Confirm the equivalence of equations and.So far we have considered associations between a single input vector and a single output vector. What happens if we have more than one input-output pairing To take the simplest case, suppose we have two such parings and suppose we let the connection matrix W just be the sum of their outer products:Now suppose that entries in the two input vectors q 1 and q 2 are uncorrelated. As we saw in chapter , this is equivalent to having zero inner product, T q 2  0. But then if we present stimulus 1, the output will beSimilarly, Wq 2  r 1 T q 2  r 2 T q 2  r 2 q 2  2Its easy to see that this argument generalizes to many memories. In the general case we writeThe bottom line is that as long as the input vectors are decorrelated, the outer product rule gives perfect retrieval. What happens if the input vectors arent orthogonal Well then we got problems. In fact, dealing with correlated patterns of activity in Hebbian networks is a poorly understood and largely unsolved theoretical problem. So even though Hebbian learning is the dominant paradigm for thinking about learning in the brain, it has found limited practical application in the computational learning community.Mathematical Aside. Note that we can rewrite equation using matrix multiplication notation.In the examples presented so far, we have used the Hebb rule that only has LTP. Suppose we use the rule with pre-andor post-synaptic thresholds for plasticity. These rules can also be expressed as an outer product by using 1to denote the vector where each element is equal to 1. Then equation can be rewritten in vector form as W   T. Adding LTD has the effect of learning associations, not between the patterns of activities themselves, but between the patterns of activities viewed relative to a threshold. Adding plasticity thresholds by including LTD-like effects has important ramifications for the issue of orthogonality. Two vectors that have only non-negative entries are only orthogonal if and only if the sets of neurons that are above threshold in each pattern are completely non-overlapping, i.e. q 1 and q 2 are orthogonal if and only if q j 1  0 implies that q j 2  0 and vice versa. Therefore, requiring orthogonality is equivalent to making the requirement of completely non-overlapping representations.This restriction doesnt hold of we consider LTD thresholds. This is shown geometrically in While adding LTD threshold can remedy the most obvious restriction of requiring orthogonal memories for learning, the readout of the learning is done using the original coordinate system. That is the weight matrixFigure 11.2: LTD thresholds can make positive vectors effectively orthogonal.but we want to calculate the readout for a given memory, say q 1 , not q 1   pre 1. One way to see the effects of this mismatch in coordinate systems is to rewriteThen, assuming that the memories were orthogonal in the recentered coordinates, Problem 11.3.1 Show that if all the entries in q 1 and q 2 are non-negative, then q 1 and q 2 are orthogonal if and only if q j 1  0 implies that q j 2  0 and vice versa.In the rest of this chapter, well use Hebbian learning to get a better understanding of some basic mathematical concepts. Well revisit some of the computational problems with associational learning in chapter .To get a better picture of what the outer product rule actually does, lets look at the problem from a geometric perspective. It will take a little while before the general idea of whats going on becomes clear. So well take a simple example, mull it over for a while, and then draw general conclusions at the end. Be patient Suppose we have an outer product matrix W built from two association pairs, q 1 , r 1  and q 2 , r 2 . To start with the simplest case, well assume that all vectors are unit vectors, and that the memory vectors in both the input and output spaces are orthogonal. To visualize things well assume that both the input layer and output layer have 3 neurons. How does the weight matrix W transform a general input q Algebraically, Wq  r 1 T  r 2 T q  r 1 T q  r 2 T qFor unit vectors q i , T q is just the projection of q onto q i . In other words, W acts to project each input vector onto each of the input memory vectors, and then produces an output vector that is a linear sum of output memories, weighted by the length of the projections since we have assumed T q  and T q  are equal to 0. So we have Wq  Wq. Let q  be the vector that starts at q and ends in the plane defined by q 1 and q 2 We have divided the action of W into two different steps: first, project the input vector onto the input memory subspace, and then project onto the individual input memories to determine the relative weightings in the output memory subspace. What have we gained by this division It seems at first that weve just added an extra projection step -after projecting onto the input memory subspace we still need to project onto the individual memories in the input space to obtain the correct output vector. To understand why we make this distinction, we redo our example with two new parings q 1 , r 1  and q 2 , r 2 , where the new vectors are made up of combinations of the old vectors:We leave it as an exercise to show that q 1 and q 2 are still unit vectors that are perpendicular to each other. Now if we apply the outer product ruleThe new vector pairs give the same weight matrix as the old By looking at the entries in the matrix W, we cant tell what the individual memories were, we can only determine information about the memory subspaces, both input and output. This is why it is conceptually useful to separate the projection onto the input memory subspace into a separate step. This first step is identical for both sets of memories. But the two sets of memories lead to different projections within the input memory subspace and different recombinations in the output memory subspace, but these calculations arrive at the same final answer. The underlying reason for this important fact is rather simple. The matrix W is determined by the statistical structure of the memories, rather than the memories themselves. Therefore, any set of memories that have the same statistical structure will lead to the same weight matrix.Problem 11.4.1 Show that q 1 and q 2 are unit vectors and that they are perpendicular to each other.Problem 11.4.2 If W is obtained as a sum of outer products via Hebbian learning, show that is equal to the set of all vectors that are perpendicular to the input memory subspace.Chapter Much of computational neuroscience concerns systems that are dynamic, i.e. they change in time.For example, the presentation of a single, static stimulus might trigger an entire chain of neural events -a transient "onset" burst of activity followed by a sustained response that slowly decays due to neuronal adaptation. On a slower time scale, the pattern of and strength of synaptic connections within a given brain region may change, perhaps triggered by developmental processes or as part of a learned response to a set of external stimuli. In fact, the brain is an incredibly complex web of interacting dynamic systems operating on time scales from less than a millisecond to years.To cope with this dizzying complexity, computational neuroscientists usually focus on a small number of mechanisms operating within a narrow range of time scales, and assume that these mechanisms can be separated out from dynamic processes operating at other time scales. Slower processes are assumed to change so slowly that they can reasonably be viewed as being fixed in time. Faster processes are often assumed to happen frequently enough so that one need only consider their average effect.Most studies in computational neuroscience focus on one of two basic dynamical problems. The first problem is to understand how patterns of activity are generated by a given neural circuit. The parameters determining the structure of the circuit are viewed as fixed over the time scale of activity. It is also common to average over individual spikes and only consider rates, although so-called spiking networks are also studied. The second basic problem concerns neural plasticity, i.e. the changes that occur within neural circuits during learning and development. Most commonly, plasticity is assumed to be a slow process, with changes in synaptic strength dependent upon the average coincidence of pre and postsynaptic activity over the course of many stimulus presentations.Before getting into the biological examples, we note that all dynamical systems can be divided into two basic types, depending on whether time is a continuous or discrete variable. In the discrete case, time ticks off in a series of regular intervals or steps. Usually the state of the system at the next time step is some function of the current state of the system. If x n1 is the state of the system at time step n  1, we writewhere f n denotes n repeated applications of the function f . A trajectory of such a system, i.e. the points x 0 , x 2 , x 3 , . . . looks like an infinite series of points starting at x 0. In continuous time dynamical systems, the state of the system evolves smoothly as time flows onward. Usually, the derivative of the state depends on the current state of the system, i.e.All the possible values of x determines the state space of our system, and equation In these notes, we will only consider examples where time is continuous. Similar mathematical tools can be used to solve the analogous discrete versions of these examples The goal of this chapter is to understand the solutions to these two basic dynamical problems. We begin the chapter by introducing the problems, and derive the linear equations that describe the simplest versions of these problems. We will then describe the basic mathematical techniques that can be used to solve linear dynamical systems. Well then go back and apply these techniques to our examples, and introduce additional mathematical techniques that can be applied to specific versions of these problems.Our first example concerns the dynamics in a network of mutually connected neurons or neural populations. The neurons presented here are very similar to the ones in chapter 8, except that the internal state is not instantaneously determined by the summed input, but rather tracks changes in the total input. In particular we assume that the rate of change in the internal state is proportional to difference between the internal state and the current value of the input. So if the current input is large, the internal will increase rapidly, slowing down as it approaches the value of the input until eventually the internal state is equal to the summed synaptic input. Since these dynamics require a distinction between the total input and internal state, we have to introduce a new variable. We will retain the notation that s i is equal to the total synaptic input to neuron i. We will let u i denote the internal state of neuron i. Mathematically we write and the decay is slow.  has units of time, and is usually given in milliseconds. A common interpretation of these equations is that u i represents an average of the membrane voltage in neuron i,  is the membrane time constant, and s i is equal to the total synaptic current times the membrane resistance. These issues will be discussed more thoroughly in chapter 5. Well start by confining ourselves to linear neuron models, we let the inputoutput function be linear, g  g u i . As before, we will assume that g  1.The main architecture that we will consider is the two layer recurrent network introduced at the end of chapter 9. We will assume that neurons in the input layer suddenly change their firing rate to a new pattern, and then remain constant. The focus of our investigation will be to understand how the activity in the output layer reacts to this sudden change in input. The fixed parameters in the network are then the pattern of feedforward weights W, the recurrent weights T and the pattern of input q. Therefore, the total synaptic input coming in to any output neuron i is given byThe dynamical equations are thenUsing vector and matrix notationIf we consider the case with no external input, i.e. q  0, then we have a linear differential equation, since the derivative of the state vector u is a linear function of the current state u.The second example concerns development in a set of synapses according to a Hebbian learning rule. In this example we will use the standard trick of averaging over activity patterns to determine the changing pattern of synaptic weights. Note that in this example the dynamic variables are the weights and the parameters are result from the patterns of activity. In the previous example, the activity levels were the variables, and weights were parameters. We focus on the synapses onto a single neuron with weight vector w. For each fixed pattern of inputs, the Hebbian learning equation can be rewritten in a dynamical form, i.e.where r is the output of the neuron in question and q j is the activity in the jth input neuron. Using vector notation,  qrBecause we are constructing a model of the gradual change in synaptic strength occurring during the course of neural development, we will assume that the average change in the pattern of weights will be guided by the average correlation of input and output, i.e.where x denotes the average value of the quantity x, averaged over a representative sample of input patterns rin. Again, we will start by examining a linear neuron model. For a linear model, the output r  w j q j  T w. Substituting into equation, we havLetting C be the matrix q T , we find that we need to solve the following linear differential equation:  qr   q T w  CwThus the equation governing the dynamics of activity within a recurrent network linear neurons are of the same form as the equation governing Hebbian learning of the weights onto a single linear neuron. The mathematical tools needed to solve linear differential equations should be applicable to both problems. Before going on to develop these tools, lets look at the matrix C in a little more detail. The ijth entry of C is given by C ij  q i q j , i.e. C is simply the matrix of correlations in the activities of the presynaptic neurons. As we might have expected, the dynamics of a set of synapses developing according the a Hebbian rule is governed by the correlations to be found amongst its inputs.Now we go on to mathematics of linear dynamical systems. For the next few sections the presentation will be strictly mathematical -well need some rather sophisticated tools before returning to solve our example problems. We start by solving a very simple, a one-dimensional dynamical system:  axThis equation is easily solved: Now we consider a slightly more difficult equation: As we saw with the passive RC circuit, this equation represents an exponential decay to the value b a with a time constant   1 a :If a  0 the "time constant"   0, and the system represents exponential growth rather than exponential decay. 12.5 Stability 12.6 Phase Plane AnalysisNow were going to generalize equation and solve the higher dimensional linear dynamical system  Ax But then the ith component of the vector Ax is just A ii x i . Then the matrix equation is equivalent to a collection of independent equationThese can be solved as above to yield To solve dynamical systems when A is not diagonal, we need to generalize the procedure outlined at the end of the previous section. The key is finding the initial conditions that lead to straight trajectories. For the trajectory to be straight, the vector field at any point along the trajectory must lie parallel to the vector describing that point. We can write this condition awhere  is a constant that gives the size of the derivative vector. Negative values of  yield derivative vectors that are in the opposite direction to the vector x. For linear dynamical systems, this is equivalent to the conditionThis condition captures one of the most important concepts from linear algebra:Definition 14 Given a square matrix A, a vector v is called an eigenvector for A if it satisfies Av  v, for some value of . The value of  that makes this condition true is called the eigenvalue of A associated with the vector v. Note that any multiple of an eigenvector is also an eigenvector. Therefore, it may be more appropriate to speak of eigendirections. Well use both terminologies.If the initial condition x is an eigenvector of the matrix A then the derivative vector will be parallel to x, and hence the entire trajectory will lie in the eigendirection defined by x. For such an initial condition, the system behaves like a one-dimensional system and the trajectory But since we know the trajectories for these eigenvectors and we are dealing with a linear system, we can simply write down the solution: Assuming that any initial condition x can be written as a linear combination of eigenvectors v 1 , v 2 , . . . , v N , is equivalent to assuming that the eigenvectors form a basis V for the state space. We call such a basis of eigenvectors, V, an eigenbasis. Now suppose we change coordinates to this eigenbasis. What does our matrix A look like Lets start by expressing the eigenvector v 1 in the basis V: v 1  1, 0, . . . , 0 T . Since v 1 is an eigenvector, we haveTherefore, A 11   1 and A i1  0 for i  1. Making this calculation for each eigenvector we see that A is a diagonal matrix when expressed in the eigenbasis V. In other words, assuming we can find a set of eigenvectors that span the entire state space, we can simply change coordinates to this basis and solve the simple case of a linear dynamical system defined by a diagonal matrix. Note that not all matrices have such a set of eigenvectors. Luckily, one can prove that if A is a symmetric matrix then A does have a complete basis of eigenvectors. But things are even nicer than this. Not only can we find a complete basis, we can find an orthonormal basis of eigenvectors. Thus, changing coordinates to such a basis is easy -we just need to project on the different eigenvectors. We will use these facts in the next two chapters.The Dynamics of Hebbian DevelopmentNow we have the tools to return to our biological examples. Lets reconsider equation:To solve this, wed like to find the eigenvectors for the correlation matrix C. Luckily, C is a symmetric matrix, so it does have an orthonormal basis of eigenvectors. Actually, because C is a correlation matrix, i.e. C  q T , it can be proved that all the eigenvalues of C are non-negative. i.e. the dominance of the horizontal component grows exponentially. How can we use figure 13.1 to inform the biology First we must address the obvious, nonbiological aspects of our model. Most obviously, the weights are becoming infinitely large. This is a consequence of the fact that Hebbian learning is a positive feedback system: making a connection stronger, makes coincident activity in the pre and post-synaptic neurons more likely, which further increases the strength of the connection. Instead of addressing this issue directly, we will simply assume that biology has come up with some mechanisms to keep the weights from growing out of control. Thus, at some point, the trajectory will cease growing. We further assume that this process doesnt otherwise alter the pattern of weights in any significant manner. The qualitative picture to take home from figure 13.1 is that if the system starts with initially small connections, then correlation-based learning rules will settle on weight vectors that lie close to the direction of the eigenvector of the correlation matrix C with the largest eigenvalue.Given a distribution of vectors and the corresponding correlation matrix, the eigenvector that has the largest eigenvalue is know as the principal component of the distribution. WILL GO OVER PCA IN CLASS. Now that we have some of the mathematical tools to solve linear differential equations, lets apply them to a simple example of synaptic plasticity -the development of ocular dominance. Ocular dominance refers to the fact that while many cells in the visual cortex receive inputs from both eyes, the degree that cells receive more input from one eye varies across cells. In fact, in many species ocular dominance is mapped on the cortex, i.e. cells with similar ocular dominance are found close to each other. Well address the issue of mapping later. First well consider a very simple example.Suppose we examine a single cell in the primary visual cortex, and look at the average input from two populations of input neurons in the LGN, one population represents LGN neurons getting input from the left eye and the other represents the population of LGN neurons getting input from the right be the matrix of correlations between LGN cells receiving input from the two eyes. The ones along the diagonal mean that we are assuming that the mean squared activity in each LGN population is equal to one, in whatever abstract units we are representing these correlations. The in the off diagonal position means that the correlation between activity in the two populations is times as strong as the mean squared activity in each population. If is positive, activity in the two eyes are positively correlated if is negative, activity in the two eyes are negatively correlated. Note that since the correlations between eyes can be no stronger than the correlations of a single population with itself, we have that 1   1.LGN w R w L Left eye Right eye Lets find the eigenvalues and eigenvectors of C. While there is a system for finding eigenvalues and eigenvectors, one time honored strategy for solving problems in mathematics is to guess the solution and then prove that you are right. This is an especially useful strategy when there are important symmetries in the problem. In the problem at hand, we have made no distinction between the right-eye and left-eye LGN populations, so wed expect that the eigenvectors shouldnt be changed if we switched w L and w R . This actually confines our guesses quite a bit in our two dimensional example, since there are only two directions in the plane that are unchanged if we switch axes. The two directions are the 45 o line itself, and the line perpendicular to it. Thus, good guesses for eigenvectors would be the vector 1, 1 T and 1, 1 T . Multiplying by the matrix C we find thatSo these vectors are indeed eigenvectors. The corresponding eigenvalues of 1 and 1 . Note that if we wanted to consider orthonormal eigenvectors, then we would use the unit vectors 1, 1 T   2 and 1, 1 T   2. The biological interpretation of this eigenbasis is rather easy. First, the eigendirection 1, 1 T represents the sum of w L and w R , i.e. this direction represents the total synaptic weight onto our V1 neuron. The eigendirection 1, 1 T represents the difference between w L and w R , with positive values meaning that the contribution from the left eye is dominant and negative value meaning that the right eye is dominant. A reasonable definition of ocular dominance is the difference between the two weights normalized by the sum, i.e. OD  w L  w R . The fact that the two eigenvalues are perpendicular means that these two components -the sum of the weights and their difference -develop independently. For example, the growth in the difference between w L and w R depends only on the current difference -the difference grows just the same if w L  1 and w R  2 or if w L  101 and w R  102. Likewise, the total weight grows the same whether w L  10 and w R  10 or w L  1 and w R  19.In looking at the qualitative behavior of this system, the first thing to note is that since    1 the eigenvalues 1   0. Therefore, both the total input and the ocular dominance are increasing exponentially. There are two main cases to consider, depending on whether is bigger or smaller than 0. If  0, the eigenvalue 1  is the principal eigenvalue, and the growth in the system is dominated by the growth in the total weight. If  0, the eigenvalue 1  is the principal eigenvalue, and the growth in the system is dominated by the growth in the difference between the weights. Since we have defined ocular dominance to be the difference component divided by the sum component, we reach the following conclusion from this simple model: ocular dominance develops only in the case where activity in LGN neurons receiving input from the two eyes is anti-correlated correlated activity in the two eyes leads to synaptic connections that have similar strengths, i.e. the ocular dominance is small.It is very important to remember that the applicability of this conclusion to biology rests on the assumptions put into the model. The linear model is very simple, and one must always worry about negative activities and weights. Also, the system is unstable in the sense that the total weight will grow infinitely large, unless some other mechanism is incorporated into the model. However, the simplicity of the model can also be an advantage. Since there are very few elements in the model, the relationship between the correlation structure of the input and the qualitative behavior of the model is quite clear. This focuses attention on any new mechanisms added to more complicated models that show qualitatively different behavior. In particular, since animals do develop ocular dominance, our simple model indicates that simple associational rules by themselves are not enough. We should focus attention on biological mechanisms that lead to competition between the inputs from LGN cells corresponding to the left and right eyes.Mathematical Aside. The use of the term correlation is often quite sloppy, and it is sometimes unclear exactly what mathematical calculation the author is referring to. One common confusion when speaking of the correlation between two patterns is whether the mean value is assumed to have been subtracted or not, i.e. whether the correlation between vectors p 2 and p 2 is calculated as i p 1where we have use p k to denote the average value of the entries in the vector p k . The correlation with the mean subtracted is properly termed the covariance of the two patterns. Sometimes it is implicitly assumed that the means have already been subtracted. In this case the correlation is the same as the covariance.Biological Aside. The reader should note how the circles in So far our simple model has two problems: the weights can grow infinitely large, and ocular dominance does not develop unless the activity in the two eyes is anti-correlated. We can actually make substantial progress on both of these issues at once if we posit a biological mechanism that constrains the total synaptic strength onto a neuron to remain within a reasonable range.The simplest way to do this is to simply assume that the sum of the synaptic strengths onto a neuron is kept fixed by some monitoring mechanism within the cell. Geometrically this means that we are confining our dynamics to remain in the subspace where j w j is equal to some constant. In our simple two dimensional example, this so-called constraint surface is just a line. Enforcing this constraint is quite easy to do within our simple model: we simply set the derivative to 0 in the eigendirection corresponding to the sum of w L and w R . This corresponds to projecting the vector field onto the constraint surface, and examining the dynamics within that surface. Because this surface corresponds exactly to an eigendirection, this is easy. If we let w D  w L  w R denote the difference in the weights, we know that w D  w Det . Since  1 unless activity in the eyes is identical, constraining the sum of the activity has the effect of leading to ocular dominance segregation in all cases.w R w L How do we understand this result SUBTRACTIVE CONSTRAINT. MORE.Now we consider the development of ocular dominance in a whole sheet of interacting cortical cells. Primary visual cortex is a largely two dimensional piece of tissue, having an area of several square centimeters, but being only 2 mm in depth. Furthermore, electrophysiological recordings reveal that the response properties of cells throughout the thickness of cortex but at the same location. Thus, each "cell" in our developmental models may just as well represent one of these cortical columns, and many models consider the cortex to be two dimensional. To simplify the problem further, we will consider a reduced, one dimensional cortex. This will allow us to explore the importance of spatial structure in the intracortical connectivity, yet still view cortical activity patterns as vectors in a natural way.So we consider a two layer recurrent network, where the input layer still has just two populations but where the cortex has N neurons or columns. The recurrent connections between the cortical columns are given by the weight matrix T, i.e. cortical column j is connected to cortical column i with connection strength T ij . As in chapter 9, given an input vector q, the Since we have two input populations and N output neurons, W is a 2  N matrix where the first column represents the weights from the left-responsive LGN population, and the second column represents the weights from the left-responsive LGN population. We will denote these column vectors w L and w R . Note that we have assumed that the matrix I  T is invertible. Since its inverse will come up over and over again, we will give it a new name, B  1 so that r  BWq. The matrix entry B ij captures the net effective connectivity from cortical column j to i. We will assume that these connection strengths are distance dependant, i.e. the magnitude of B ij depends only on i  j. One common assumption is that nearby cortical columns excite each other, while columns at a further distance display mutual inhibition. To avoid the different patterns of activity displayed by the columns at the "edge" of our one dimensional line of columns, we assume circular boundary conditions, i.e. we assume column 1 is right next to column N . By joining the ends of our cortical line in this way, we are considering a so-called ring network i-j where C is the 2  2 correlation matrix qq T . How is equation related to our traditional linear dynamical system that comes in the form  Ax First of all, equation is a linear differential equation in the sense that is a linear function of W. The major difference is that instead of the elements of the vector w defining an N dimensional state space, the elements of the N  2 matrix W defines an 2N dimensional space of connection strengths.Like any linear differential equation, our task is to find eigendirections in this space, i.e. we must find find specific matrices W such that BWC  W. One might guess that the eigendirections for W might depend on the eigenvectors for each of the matrices B and C. In fact, suppose that u is an eigenvector of B with eigenvalue  and that v is an eigenvector of C T with eigenvalue . The following shows that the outer product W  uv T is one of the eigendirections that we are looking for, and it has eigenvalue : should lead to the development of weight matrices that are close to w 1 dif f 1, 1 T , where w 1 dif f is the principal eigenvector of the matrix B.We have assumed that the intracortical interactions given by the matrix B are distance dependent, i.e. the value B ij depends only on the difference i  j. In this case B is known as a circulant matrix. It can be shown that the collection of discrete Fourier vectors form an eigenbasis for all circulant matrices. For N dimensional vectors, the Fourier vectors are given by the formulas v j  cos or v j  sin k represents the frequency of the discrete vector, since it determines the number of times the argument 2kjN traces out a circle as j goes from 1 to N . MORE. PROBLEMS Therefore, by performing a discrete Fourier transform on the cortical interaction function, we can find the component B with the largest eigenvalue. This direction represents the dominant periodicity at which the features represented by the eigenvectors of C will modulate as one moves across the cortex. For example, the dominant frequency of the centersurround connectivity shown in In the simple examples considered so far, the input from the LGN had no spatial extent -we lumped all cells getting input from the same eye into one either a "left" or "right" population. Perhaps in a more realistic model of ocular dominance, a cortical cell would come to respond to inputs from one eye in one portion of the cells receptive field, while responding to inputs from the other eye in a different portion of the receptive field. At each location in the LGN, inputs would come from one eye, but over the whole receptive field the cell would be binocular. It is easy to extend the framework we have developed so far to consider an LGN with a spatial extent. We consider two one-dimensional "rings" of LGN cells, one for the left eye and one for the right. We will assume that each contains P populations. Input vectors will be 2P dimensional and we will make the convention that the first P elements of the input vector q will represent the activity q L in the P left-eye populations and the last P elements will represent the activity q R in the P right-eye populations, i.e.But then the correlation matrix C takes the following formwhere C LR  C RL  C opp represents the P  P matrix of correlations between right and left eye populations, and C LL  C RR  C same represents the P  P matrix of within-eye correlations. Now we use our usual trick of expressing the inputs in sum and difference coordinates. Letting q sum  q L  q R and q dif f  q L  q R , we see that the sum and difference are decorrelated:Therefore, in these new coordinates, C becomeswhere C sum  2 and C dif f  2. Again, the sum and difference components develop independently. Assuming that the sum of the weights is constrained, we will focus on W dif f , the N  P dimensional matrix that holds the difference in w L and w R from each of the P LGN locations in the ring to the N cortical locations. We will also assume that input correlations depend on distance, so that C dif f  C dif f. As we derived above, the eventual pattern of weights will be dominated by the the outer product principal eigenvector of the cortical interaction matrix B and the principal eigenvector of C dif f . But since we are assuming that correlations depend only on distance in the LGN, C dif f is a circulant matrix as well, and its eigenvectors are given by the discrete Fourier vectors. The Fourier vector with the largest eigenvalue will determine the receptive field for each of the cortical cells. Note that we are actually looking for receptive fields that are very boring, i.e. if cells are to be monocular, then the cell must receive input from a given eye over the entire receptive field. Therefore, monocular receptive fields require that the zero frequency component -the vector of ones time the average correlation between LGN populations -be dominant. One can show that this will be the case whenever the correlation function is strictly positive.Problem 13.3.1 Show that equation defines a linear dynamical system. Problem 13.3.2 Assume that B, W, and C are 2  2 dimensional matrices:Rewrite W as a 4 dimensional vector w, and then rewrite equation The exact same machinery that we developed to look at ocular dominance can be used to examine the development of orientation selectivity in the visual cortex. Cells tuned for the orientation of a including contrast edges are first seen at the level of the visual cortex.In the retina and the LGN, most cells are not tuned for orientation, but have so-called centersurround receptive fields, like those in the limulus eye. Some cells respond to light in the center and dark in the surround, and others to the opposite pattern.The dominant hypothesis for the construction of oriented cells was first proposed by Hubel and Wiesel in the early sixties. They proposed that cortical cells receive inputs aligned in such a way so that cortical cells have alternating ON and OFF subregions. Moreover, they also showed that the orientation selectivity arising from such a relationship was mapped in the cortex, i.e. nearby cortical cells had similar orientation preferences. Important features of the development of ON and OFF subregions can be modeled by simply substituting ON and OFF for left and right in the above derivation. Oriented cells will develop whenever the dominant eigenvector of the correlation matrix C dif f is anything other than the zero frequency component. In this case, cortical receptive fields will contain an oscillation of ON and OFF subregions, and will be orientation selective. As before, the intracortical interaction matrix B determines the periodicity of orientation selectivity, and hence the structure of the orientation map.We have shown that a simple framework yields insight into the possible mechanisms underlying the development of both ocular dominance and orientation maps in the visual cortex. The main prediction of these models is that for ocular dominance to develop, the principal eigenvector of matrix obtained by adding the within eye correlations and the negative of the between eye correlations should be the zero frequency vector. For orientated cells, the correlation of the corresponding ON-OFF correlation matrix should be dominated by a vector other than the zero frequency vector. To the degree that these have been measured accurately, experiments tend to support these hypotheses.However, there is one major flaw in the simple models presented so far: the connectivity that develops is not local, i.e. cortical cells can receive input from LGN responding to any visual location. Conversely, LGN cells can project to all visual cortical neurons. An easy way to remedy this nonbiological behavior is to alter the learning equations to account for the fact that it may be quite difficult for cells to develop the large axonal or dendritic arbors that would be necessary to support such global connectivity. One easy way to do this is to assume that each LGN cell has an arbor function, that makes it easiest for connections grow toward a specified region of visual cortex. SEE MILLER ARTICLE. Such a system will eventually settle into a state that is a minimum of the energy function. 2 Example 14.2.1 Friction will eventually slow a swinging pendulum until it settles into the state where the pendulum hangs straight downward. In this state there is no motion energy and the potential energy of gravity is lowest. This state is an attracting state for the system.In low dimensional systems, we can plot the energy as a function of the state. From this geometric point of view, the energy function is viewed as an energy landscape, in which trajectories of the system "flow downhill." In the example shown, there are three attractors each represented by the lowest energy state at the bottom of a "valley." The set of all states that flow toward a particular attractor is known as the basin of attraction for that attractor. Note that only one of these gives the lowest energy state of the entire system. The other two attractors are local minima, i.e. they are minima for the energy function in a small neighborhood of the attracting state, but not over the whole state space.To make use of an energy landscape to store memories within a network, we must have some rule for structuring a network so that the stored memories lay at local minima of some energy function. In 1982, John Hopfield combined a generalized form of the Hebb rule with a simple activation dynamics, and constructed an energy function in which the stored memory states were indeed local minima of the energy function. This paper was not only important for clarifying how Hebbian learning can lead to attractor memories, it also strengthened the bridge between memory networks and certain branches of statistical mechanics. This opened the field of neural computation to a number of physicists who began to apply sophisticated statistical techniques to understand the behavior of large networks of simplified neurons.  Hopfield networks are built to store L memories, where each memory vector v k is a random binary pattern of activity distributed over N neurons, with N  L. The simplest version of the network views the binary patterns as strings of 1s and -1s rather than 1s and 0s. Storage and recall are strictly separated in these networks. First, to store the given memories, a matrix of connections T is constructed according to a correlation-based outer product rule:Self-connection strengths T ii are then set to zero. Note that during the storage phase, activities are assumed to be fixed or clamped in the pattern of the memories to be stored, without being influenced by the storage of previous memories. During recall, the activation dynamics follow the usual equations:g is the sigmoid inputoutput function. In vector notation this becomes u  u  Tg flows toward one of the stored memory vectors v k .Network Aside. Dynamics in terms of output rather than state variables. MORE., and the total input vector. If the state is closest to the memory vector v 1 , then the input will biased most strongly in the direction of v 1 . Therefore, the input will drive the activity toward v 1 , thereby increasing the match to v 1 . This sets up a positive feedback system with the trajectory finally approaching the attracting state v 1 . We will re-examine this argument below.Hopfield networks have been analyzed from three points of view.The first is the energy function point of view presented above. In 1983, Cohen and Grossberg showed that the following was an energy function for the dynamics: 3 E   1 2 g T Tg  The most important impact of Hopfields 1982 paper was it clarified the connection between associative memory networks and the subfield of physics known as statistical mechanics, paving the way for the application of a number of sophisticated tools from physics to the analysis of these networks. Although we will not explore these issues in detail, we will give a brief introduction to the approach. First, we explore the "high gain" case where g  r i  1. Then a stable equilibrium for the dynamics will be found at states where each component of the total input i has the same sign as r i . This follows immediately from the dynamic equation, since if i  0 then u i approaches a positive value and hence r i remains positive. Suppose that we want to check whether the memory vector v 1 is indeed a stable attractor for the dynamics. From equation, we have that the total inputThe main key to the statistical mechanics approach is that since we are assuming that the memory vectors to be stored were random binary vectors. Therefore, the second term in equation can be seen as a "noise term" describing the random interference from other memory vectors. The term v 1 constitutes the "signal." Given this framework, one can then meaningfully speak of the probability that a typical memory vector is stable, where the average is taken over the range of particular memory networks constructed from random memory vectors. Looking at things in more detail we have that 1Since v k and v 1 are uncorrelated binary vectors, v k j v 1 j is a random number taking values 1 or -1. Therefore, 1 N v k  v 1 is a random number between 1 and -1 with mean value 0 and variance equal to 1N . If N is large, this implies that 1 N v k  v 1  0 with high probability. This is a special case of the general fact that random vectors in high dimensional spaces are nearly orthogonal. Adding the contribution from each of the L  1 memory vectors other than v 1 we have that the noise term in equation will have mean 0 and varianceN .Now we look at Hopfield dynamics from the geometric point of view that we have emphasized in the rest of these notes. For this analysis we will use the piecewise linear approximation to the sigmoid function As in all linear dynamics, we look for the eigenvectors of I gT. To begin with, we will assume that the memory vectors used to construct T are orthogonal. As we saw previously, this is a good approximation as long as the number of memories is small relative to the number of neurons. In this case, each of the memories are eigenvectors, with eigenvalueg  1. Any vector perpendicular to the subspace spanned by the memory vectors is also an eigenvector, but with eigenvalue -1. Since we are assuming thatg is large, the dynamics is exponentially expanding within the memory subspace, and exponentially decaying in directions perpendicular to this subspace. As activities grow large, the exponential expansion within the memory subspace is halted when the neurons begin to saturate. If we look at the dynamics of the output values r i these are prevented from growing beyond the values -1 and 1, i.e. the set of allowable output states is the set of all vectors r with 1  r i  1. Thus the state space can be viewed as a high-dimensional cube in output space, with the states growing exponentially until they reach the faces of this "box." In 19, James Anderson explored associative memory dynamics from this point of view, calling his model the brain-state-in-a-box model. This model gives a very different perspective on the nature of attractors in these kind of associative memory networks. In particular, within the memory subspace, any linear combination of memory vectors is growing just as fast as the memory vectors themselves. Thus, the argument given in section 14.3.1 is highly misleading. Storing memories using an outer product matrix in and of itself does not make the memories into attracting states. It only specifies a memory subspace. The creation of attractors relies crucially on the sigmoid shape of the inputoutput function. In particular, saturation of outputs not only serves as a mechanism for bounding the unstable positive feedback created by strong recurrent connectivity, it provides a constraint surface that greatly influences the attracting states of the network. From the brain-state-in-a-box point of view, trajectories within the memory subspace expand until they hit the sides of the box, but continue to expand until they reach the corners So far we have focused on conditions ensuring that the stored memory vectors are stable fixed points for the dynamics. Even if this is the case, the network does not necessarily perform perfectly. In particular, we have not addressed the very real possibility that other states besides the memory states may also be attractors. Such attractors are sometimes called spurious attractors or spurious memories.The most common type of spurious memories are those constructed from combinations of odd numbers of memories. 4 For example, consider a vector v mix formed by first adding the entries of three memory vectors v 1 , v 2 , and v 3 . This will yield a vector whose entries are -3, -1, 1, or 3. v mix is then formed by setting negative entries to -1 and positive entries to 1. On average, an entry of v mix will match the entry for any of its component vectors 34 of the time. To see this, suppose v 1 j  1. Then v mix j  1 only if both v 2 j  1 and v 3 j  1, something that should happen 14 of the time. If we look at the total input when the network is in the state v mix , we have thatIf we consider large networks, then the last "noise" term is small, the sign of Tv mix will be determined by v 1  v 2  v 3 , and so v mix will be a stable fixed point. However, because of the factor of 34, v mix is not as stable as v 1 , v 2 , or v 3 . At the entries where all three memories do not agree, it would take smaller values of the noise term to destabilize v mix . From the energy function perspective, v mix is a local minimum of the energy function, but the energy is not as low as for the memory states v 1 , v 2 , and v 3 . This is shown in figure 14.6 using a dimensional general energy schematic as well as a two dimensional "corner" schematic.  One can think about spurious from the brain-state-in-a-box point of view as well, but this takes some imagination. The key is to find some way of picturing how a low dimensional plane is bounded by a high dimensional cube. One can think of the high dimensional cube as a "pointy ball" that bounds the overall size of the activity but allows some vectors -the "points" or "corners" -to be longer than others. As before there are two components to the dynamics, one that leads to outward expansion within the memory subspace and another that leads to decay toward the memory subspace. For points on the ball that are near memory subspace, the memory subspace expansion component will dominate and these points can be stable. Because v mix is nearly a linear combination of the memory vectors v 1 , v 2 , and v 3 , it lies near enough to the memory subspace to be stable.So far we have discussed the Hopfield network for units with a high gain sigmoid, i.e. where the sigmoid is a good approximation to the binary inputoutput function. What happens if we lower the gain From the statistical mechanics perspective, this has been shown to be similar to the effects of raising the "temperature" of a number of interacting particles. Although still tending toward the lowest energy state, particles have some probability to jump up from a lower to higher energy state. From the perspective of figure 14.6, high energy mixture state will become relatively unstable since particles can jump out of relatively shallow minima, but will remain in lower energy states. From the box perspective, lowering the gain "smoothes" the box constraint. The Hopfield model demonstrates that attracting states can arise from simple correlation-based learning rules applied to distributed patterns of activity in a recurrent network. However, the model suffers from a number of assumptions that are highly non-biological. Many of these can be addressed with only minor changes to the Hopfield framework. Others lead to networks with attracting states whose stability depends different dynamical mechanisms.The most common trick that we will use relies on the fact that in large networks, the average activity across all neurons is very close to the average of the distribution used to pick each element in the random memory vectors. This fact will allow us to effectively alter this mean value by adding or subtracting a constant value. Because this value is constant, it can be contributed by non-specific biological mechanisms that affect all neurons equally. To make things simple, we will assume that memory vectors are chosen so that for the various functionally important parameters, the value of that parameter is exactly equal to the mean value of the distribution of parameter values that would result if the memory vectors were chosen truly at random. For example, we will assume that the memory vectors are chosen so that the mean value of the elements in each memory vector is exactly equal to the mean of the distribution used to choose each element. For random binary 1 vectors, this assumption means that we assume an exactly equal number of 1s and -1s. For large networks, the mean over each memory vector will be very close to 0. To the degree that these differ, the more realistic networks outlined below will behave quite very similar to, but not exactly like, the original Hopfield model.The most glaring problem with the original Hopfield network is the use of positive and negative output values. Negative outputs obviously cannot be interpreted as corresponding to firing rate. This problem is easily addressed by simply shifting the range of the inputoutput function so that it ranges between 0 and 1, and then re-expressing the original dynamics and learning rule to compensate for this change of coordinates.In actuality, we can use essentially the same learning rule. For binary memory vectors constructed from random choices of -1 and 1, the mean activity level is 0. Therefore, the previous correlation learning ruleis equivalent to the covariance learning rule Another way in which the Hopfield network differs from biology is that single neurons in the network can have both excitatory and inhibitory effects on their postsynaptic targets. This apparent conflict may be one of interpretation. If we interpret the units in the Hopfield network not as single neurons, but as groups of neurons functioning as a single unit. Then an inhibitory weight can be interpreted as a strong connection from the excitatory neurons in one population to the inhibitory neurons in the other. While this interpretation can explain the existence of positive and negative weights, it also carries with it additional assumptions. In particular, the rule for plasticity between excitatory and inhibitory neurons must be such that the net connection strengths between populations follow a correlation-based rule.An alternative to having separate inhibitory neurons coupled to each group of excitatory cells is to posit the existence of a single population of inhibitory neurons that both receives from and projects to all excitatory neurons with equal strength. If this inhibitory population displays a linear response, its activity will be proportional to the total excitatory activity. Letting h denote this inhibitory activity, we have h  j W he gIf the inhibitory population is projects back to each excitatory cell with strength W eh , Using this model, a negative connection strength between excitatory neurons can be interpreted as indicating that the excitatory connection is weaker than the mutual inhibitory influence mediated by the inhibitory population. This interpretation has the benefit of not requiring new assumptions on the learning rule. The Hebb rule can be seen as increasing or decreasing a default positive excitation between neurons depending on whether the neurons are correlated or anti-correlated over the memory vectors. The feedback inhibition is assumed to counteract this positive excitation leaving the original Hopfield proscription to determine the net effective connectivity., where r j  g is the firing rate of neuron j. This raises the possibility that the transfer of presynaptic activation to postsynaptic current could saturate well-below the saturation level of the inputoutput function alone. This synaptic saturation will bound the total synaptic current received by neurons in the network, and hence could act to bound their firing rates at physiologically low values.These notes have focused on models where synaptic integration is modelled as a static linear operation. While this simplification is useful for thinking about processing within neural circuits, one must always remember that synapses and dendrites are complex and dynamic devices, and this complexity is likely to have a significant impact on the nature of computation within neural circuits.ProblemsAttractor NetworksOne of the most important ideas contributed by computational neuroscience is the idea of a memory being stored as an attracting state for the underlying dynamics in a neural network. The groundwork for this idea was part of Hebbs famous book, The Organization of Historical Aside. Hebbs ideas played an important role in returning the "mental" or "cognitive" aspects of behavior as legitimate questions for scientific study. In the earlier half of the century, behaviorist or stimulusresponse ideas were dominant in Psychology, especially in the United States. All behavior was assumed to be in response to external stimuli, and it was deemed impossible to "look inside" the brain to study "thinking." After all, the only things that were available for scientific study were the nature of the stimuli that an animal encountered and the subsequent behavioral response. In presenting a clear picture of how neural activity could be generated and sustained, even in the absence of external stimuli, Hebbs work made an important contribution to the increasing study of internal representation and the cognitive abilities of neural circuits.With the rise of biologically-inspired models of computation in the latter half of the century, the idea of the cell assembly found natural mathematical correlate in the notion of an attractor. An attractor is simply a state of a dynamical system such that trajectories that start at nearby states flow toward the attracting state. The simplest example of an attractor is a stable equilibrium One common way to demonstrate that a dynamical system has attractors is to construct an energy function or Lyapunov function for that system. 1 Consider a mapping that attaches a number to every location in state space. Such a function will be an energy function for a given dynamical system if the "energy" is decreasing along every trajectory of the dynamics. We use the term energy function in analogy with physical systems where the energy within a system does not increase, but can possibly decrease due to dissipative forces such as friction.