Based on the anatomical connections between visual areas, differences in electrophysiological response properties, and the effects of cortical lesions, a consensus has emerged that extrastriate cortical areas are organized into two largely separate systems that eventually feed information into cortical association areas in the temporal and parietal lobes. One system, called the ventral stream, includes area V4 and leads from the striate cortex into the inferior part of the temporal lobe. This system is thought to be responsible for high-resolution form vision and object recognition. The dorsal stream, which includes the middle temporal area, leads from striate cortex into the parietal lobe. This system is thought to be responsible for spatial aspects of vision, such as the analysis of motion, and positional relationships between objects in the visual scene The functional dichotomy between these two streams is supported by observations on the response properties of neurons and the effects of selective cortical lesions. Neurons in the ventral stream exhibit properties that are important for object recognition, such as selectivity for shape, color, and texture. At the highest levels in this pathway, neurons exhibit even greater selectivity, responding preferentially to faces and objects. In contrast, those in the dorsal stream are not tuned to these properties, but show selectivity for direction and speed of movement. Consistent with this interpretation, lesions of the parietal cortex severely impair an animals ability to distinguish objects on the basis of their position, while having little effect on its ability to perform object recognition tasks. In contrast, lesions of the inferotemporal cortex produce profound impairments in the ability to perform recognition tasks but no impairment in spatial tasks. These effects are remarkably similar to the syndromes associated with damage to the parietal and temporal lobe in humans.What, then, is the relationship between these "higher-order" extrastriate visual pathways and the magno-and parvocellular pathways that supply the primary visual cortex Not long ago, it seemed that these intracortical pathways were simply a continuation of the geniculostriate pathways-that is, the magnocellular pathway provided input to the dorsal stream and the parvocellular pathway provided input to the ventral stream. However, more recent work has indicated that the situation is more complicated. The temporal pathway clearly has access to the information conveyed by both the magno-and parvocellular streams and the parietal pathway, while dominated by inputs from the magnocellular stream, also receives inputs from the parvocellular stream. Thus, interaction and cooperation between the magno-and parvocellular streams appear to be the rule in complex visual perceptions.Distinct populations of retinal ganglion cells send their axons to a number of central visual structures that serve different functions. The most important projections are to the pretectum for mediating the pupillary light reflex, to the hypothalamus for the regulation of circadian rhythms, to the superior colliculus for the regulation of eye and head movements, and-most important of all-to the lateral geniculate nucleus for mediating vision and visual perception. The retinogeniculostriate projection is arranged topographically such that central visual structures contain an organized map of the contralateral visual field. Damage anywhere along the primary visual pathway, which includes the optic nerve, optic tract, lateral geniculate nucleus, optic radiation, and striate cortex, results in a loss of vision confined to a predictable region of visual space. Compared to retinal Temporal lobe ganglion cells, neurons at higher levels of the visual pathway become increasingly selective in their stimulus requirements. Thus, most neurons in the striate cortex respond to light-dark edges only if they are presented at a certain orientation some are selective for the length of the edge, and others to movement of the edge in a specific direction. Indeed, a point in visual space is related to a set of cortical neurons, each of which is specialized for processing a limited set of the attributes in the visual stimulus. The neural circuitry in the striate cortex also brings together information from the two eyes most cortical neurons have binocular responses. Binocular convergence is presumably essential for the detection of binocular disparity, an important component of depth perception. The primary visual pathway is composed of separate functional streams that convey information from different types of retinal ganglion cells to the initial stages of cortical processing. The magnocellular stream conveys information that is critical for the detection of rapidly changing stimuli, the parvocellular stream mediates high acuity vision and appears to share responsibility for color vision with the koniocellular stream. Finally, beyond striate cortex, parcellation of function continues in the ventral and dorsal streams that lead to the extrastriate and association areas in the temporal and parietal lobes, respectively. Areas in the inferotemporal cortex are especially important in object recognition, whereas areas in the parietal lobe are critical for understanding the spatial relations between objects in the visual field.In physical terms, sound refers to pressure waves generated by vibrating air molecules. Sound waves are much like the ripples that radiate outward when a rock is thrown in a pool of water. However, instead of occurring across a two-dimensional surface, sound waves propagate in three dimensions, creating spherical shells of alternating compression and rarefaction. Like all wave phenomena, sound waves have four major features: waveform, phase, amplitude, and frequency. For human listeners, the amplitude and frequency of a sound pressure change at the ear roughly correspond to loudness and pitch, respectively. The waveform of a sound stimulus is its amplitude plotted against time. It helps to begin by visualizing an acoustical waveform as a sine wave. At the same time, it must be kept in mind that sounds composed of single sine waves are extremely rare in nature most sounds in speech, for example, consist of acoustically complex waveforms. Interestingly, such complex waveforms can often be modeled as the sum of sinusoidal waves of varying amplitudes, frequencies, and phases. In engineering applications, an algorithm called the Fourier transform decomposes a complex signal into its sinusoidal components. In the auditory system, as will be apparent later in the chapter, the inner ear acts as a sort of acoustical prism, decomposing complex sounds into a myriad of constituent tones.The Auditory System Such regular, sinusoidal cycles of compression and rarefaction can be thought of as a form of circular motion, with one complete cycle equivalent to one full revolution. This point can be illustrated with two sinusoids of the same frequency projected onto a circle, a strategy that also makes it easier to understand the concept of phase The human ear is extraordinarily sensitive to sound pressure. At the threshold of hearing, air molecules are displaced an average of only 10 picometers, and the intensity of such a sound is about one-trillionth of a watt per square meter This means a listener on an otherwise noiseless planet could hear a 1-watt, 3-kHz sound source located over 450 km away. Even dangerously high sound pressure levels have power at the eardrum that is only in the milliwatt range.Humans can detect sounds in a frequency range from about 20 Hz to 20 kHz. Human infants can actually hear frequencies slightly higher than 20 kHz, but lose some high-frequency sensitivity as they mature the upper limit in average adults is closer to 15-17 kHz. Not all mammalian species are sensitive to the same range of frequencies. Most small mammals are sensitive to very high frequencies, but not to low frequencies. For instance, some species of bats are sensitive to tones as high as 200 kHz, but their lower limit is around 20 kHz-the upper limit for young people with normal hearing.One reason for these differences is that small objects, including the auditory structures of these small mammals, resonate at high frequencies, whereas large objects tend to resonate at low frequencies-which explains why the violin has a higher pitch than the cello. Different animal species tend to emphasize frequency bandwidths in both their vocalizations and their range of hearing. In general, vocalizations by virtue of their periodicity can be distinguished from the noise "barrier" created by environmental sounds, such as wind and rustling leaves. Animals that echolocate, such as bats and dolphins, rely on very high-frequency vocal sounds to maximally resolve spatial features of the target, while animals intent on avoiding predation have auditory systems "tuned" to the low frequency vibrations that approaching predators transmit through the substrate. These behavioral differences are mirrored by a wealth of anatomical and functional specializations throughout the auditory system. The auditory system transforms sound waves into distinct patterns of neural activity, which are then integrated with information from other sensory systems to guide behavior, including orienting movements to acoustical stimuli and intraspecies communication. The first stage of this transformation occurs at the external and middle ears, which collect sound waves and amplify their pressure, so that the sound energy in the air can be successfully transmitted to the fluid-filled cochlea of the inner ear. In the inner ear, a series of biomechanical processes occur that break up the signal into simpler, sinusoidal components, with the result that the frequency, amplitude, and phase of the original signal are all faithfully transduced by the sensory hair cells and encoded by the electrical activity of the auditory nerve fibers.One product of this process of acoustical decomposition is the systematic representation of sound frequency along the length of the cochlea, referred to as tonotopy, which is an important organizational feature preservedThe Auditor y System 285 Box AAcquired hearing loss is an increasingly common sensory deficit that can often lead to impaired oral communication and social isolation. Four major causes of acquired hearing loss are acoustical trauma, infection of the inner ear, ototoxic drugs, and presbyacusis. The exquisite sensitivity of the auditory periphery, combined with the direct mechanical linkage between the acoustical stimulus and the receptor cells, make the ear especially susceptible to acute or chronic acoustical trauma. Extremely loud, percussive sounds, such as those generated by explosives or gunfire, can rupture the eardrum and so severely distort the inner ear that the organ of Corti is torn. The resultant loss of hearing is abrupt and often quite severe. Less well appreciated is the fact that repeated exposure to less dramatic but nonetheless loud sounds, including those produced by industrial or household machinery or by amplified musical instruments, can also damage the inner ear. Although these sounds leave the eardrum intact, specific damage is done to the hair bundle itself the stereocilia of cochlear hair cells of animals exposed to loud sounds shear off at their pivot points with the hair cell body, or fuse together in a platelike fashion that impedes movement. In humans, the mechanical resonance of the ear to stimulus frequencies centered about 3 kHz means that exposure to loud, broadband noises results in especially pronounced deficits near this resonant frequency.Ototoxic drugs include aminoglycoside antibiotics, which directly affect hair cells, and ethacrynic acid, which poisons the potassium-extruding cells of the stria vascularis that generate the endocochlear potential. In the absence of these ion pumping cells, the endocochlear potential, which supplies the energy to drive the transduction process, is lost. Although still a matter of some debate, the relatively nonselective transduction channel apparently affords a means of entry for aminoglycoside antibiotics, which then poison hair cells by disrupting phosphoinositide metabolism. In particular, outer hair cells and those inner hair cells that transduce high-frequency stimuli are more affected, simply because of their greater energy requirements.Finally, presbyacusis, the hearing loss associated with aging, may in part stem from atherosclerotic damage to the especially fine microvasculature of the inner ear, as well as from genetic predispositions to hair cell damage. Recent advances in understanding the genetic transmission of acquired hearing loss in both humans and mice point to mutations in myosin isoforms unique to hair cells as a likely culprit.throughout the central auditory pathways. The earliest stage of central processing occurs at the cochlear nucleus, where the peripheral auditory information diverges into a number of parallel central pathways. Accordingly, the output of the cochlear nucleus has several targets. One of these is the superior olivary complex, the first place that information from the two ears interacts and the site of the initial processing of the cues that allow listeners to localize sound in space. The cochlear nucleus also projects to the inferior colliculus of the midbrain, a major integrative center and the first place where auditory information can interact with the motor system. The inferior colliculus is an obligatory relay for information traveling to the thalamus and cortex, where additional integrative aspects of sound especially germane to speech and music are processed Even though we all recognize it when we hear it, the concept of music is vague. The Oxford English Dictionary defines it as "The art or science of combining vocal or instrumental sounds with a view toward beauty or coherence of form and expression of emotion." In terms of the present chapter, music chiefly concerns the aspect of human audition that is experienced as tones. The stimuli that give rise to tonal percepts are periodic, meaning that they repeat systematically over time, as in the sine wave in Although we usually take the way tone-evoking stimuli are heard for granted, this aspect of audition presents some profoundly puzzling qualities. The most obvious of these is that humans perceive periodic stimuli whose fundamental frequencies have a 2:1 ratio as highly similar, and, for the most part, musically interchangeable. Thus in Western musical terminology, any two tones related by an interval of one or more octaves are given the same name, and are distinguished only by a qualifier that denotes relative ordinal position. As a result, music is framed in repeating intervals defined by these more or less interchangeable tones. A key question, then, is why periodic sound stimuli whose fundamentals have a 2:1 ratio are perceived as similar when there is no obvious physical or physiological basis for this phenomenon.A second puzzling feature is that most if not all musical traditions subdivide octaves into a relatively small set of intervals for composition and performance, each interval being defined by its relationship to the lowest tone of the set. Such sets are called musical scales. The scales predominantly employed in all cultures over the centuries have used some of the 12 tonal intervals that in Western musical terminology are referred to as the chromatic scale The external ear, which consists of the pinna, concha, and auditory meatus, gathers sound energy and focuses it on the eardrum, or tympanic membrane The Auditor y System 287 there is no principled explanation of these preferences among all the possible intervals within the octave.Perhaps the most fundamental question in music-and arguably the common denominator of all musical tonality-is why certain combinations of tones are perceived as relatively consonant or harmonious and others relatively dissonant or inharmonious. These perceived differences among the possible combinations of tones making up the chromatic scale are the basis for polytonal music, in which the perception of relative harmoniousness guides the composition of chords and melodic lines. The more compatible of these combinations are typically used to convey resolution at the end of a musical phrase or piece, whereas less compatible combinations are used to indicate a transition, a lack of resolution, or to introduce a sense of tension in a chord or melodic sequence. Like octaves and scales, the reason for this phenomenology remains a mystery.The classical approaches to rationalizing octaves, scales and consonance have been based on the fact that the musical intervals corresponding to octaves, fifths, and fourths are produced by physical sources whose relative proportions have ratios of 2:1, 3:2, or 4:3, respectively. This coincidence of numerical simplicity and perceptual effect has been so impressive over the centuries that attempts to rationalize phenomena such as consonance and scale structure in terms of mathematical relationships have tended to dominate the thinking about these issues. This conceptual framework, however, fails to account for many of the perceptual observations that have been made over the last century.Another way to consider the problem is in terms of the biological rationale for evolving a sense of tonality in the first place. A pertinent fact in this regard is that only a small minority of naturally occurring sound stimuli are periodic. Since the auditory system evolved in the world of natural sounds, this point is presumably critical for thinking about the biological purposes of tonality and music. Indeed, the majority of periodic sounds that humans would have been exposed to during evolution are those made by the human vocal tract in the process of communication, initially prelinguistic but more recently speech sounds. Thus developing a sense of tonality would enable listeners to respond not only the distinctions among the different speech sounds that are important for understanding spoken language, but to information about the probable sex, age, and emotional state of the speaker. It may thus be that music reflects the advantage of facilitating a listeners ability to glean the linguistic intent and biological state of fellow humans through vocal utterances. generated by heavy machinery or high explosives. The sensitivity to this frequency range in the human auditory system appears to be directly related to speech perception: although human speech is a broadband signal, the energy of the plosive consonants that distinguish different phonemes is concentrated around 3 kHz. Therefore, selective hearing loss in the 2-5 kHz range disproportionately degrades speech recognition. Most vocal communication occurs in the low-kHz range to overcome environmental noise as already noted, generation of higher frequencies is difficult for animals the size of humans.A second important function of the pinna and concha is to selectively filter different sound frequencies in order to provide cues about the elevation of the sound source. The vertically asymmetrical convolutions of the pinna are shaped so that the external ear transmits more high-frequency components from an elevated source than from the same source at ear level. This effect can be demonstrated by recording sounds from different elevations after they have passed through an "artificial" external ear when the recorded sounds are played back via earphones, so that the whole series is at the same elevation relative to the listener, the recordings from higher elevations are perceived as coming from positions higher in space than the recordings from lower elevations. Sounds impinging on the external ear are airborne however, the environment within the inner ear, where the sound-induced vibrations are converted to neural impulses, is aqueous. The major function of the middle ear is to match relatively low-impedance airborne sounds to the higher-impedance fluid of the inner ear. The term "impedance" in this context describes a mediums resistance to movement. Normally, when sound waves travel from a low-impedance medium like air to a much higher-impedance medium like water, almost all of the acoustical energy is reflected. The middle ear Two mechanical processes occur within the middle ear to achieve this large pressure gain. The first and major boost is achieved by focusing the force impinging on the relatively large-diameter tympanic membrane on to the much smaller-diameter oval window, the site where the bones of the middle ear contact the inner ear. A second and related process relies on the mechanical advantage gained by the lever action of the three small interconnected middle ear bones, or ossicles Bony and soft tissues, including those surrounding the inner ear, have impedances close to that of water. Therefore, even without an intact tympanic membrane or middle ear ossicles, acoustical vibrations can still be transferred directly through the bones and tissues of the head to the inner ear. In the clinic, bone conduction can be exploited using a simple test involving a tuning fork to determine whether hearing loss is due to conductive problems or is due to damage to the hair cells of the inner ear or to the auditory nerve itselfThe cochlea of the inner ear is arguably the most critical structure in the auditory pathway, for it is there that the energy from sonically generated pressure waves is transformed into neural impulses. The cochlea not only amplifies sound waves and converts them into neural signals, but it also acts as a mechanical frequency analyzer, decomposing complex acoustical waveforms into simpler elements. Many features of auditory perception derive from aspects of the physical properties of the cochlea hence, it is important to consider this structure in some detail.The cochlea is a small coiled structure, which, were it uncoiled, would form a tube about 35 mm long The same features that make the auditory periphery exquisitely sensitive to detecting airborne sounds also make it highly vulnerable to damage. By far the most common forms of hearing loss involve the peripheral auditory system, namely to those structures that transmit and transduce sounds into neural impulses. Monaural hearing deficits are the defining symptom of a peripheral hearing loss, because unilateral damage at or above the auditory brainstem results in a binaural deficit. Peripheral hearing insults can be further divided into conductive hearing losses, which involve damage to the outer or middle ear, and sensorineural hearing losses, which stem from damage to the inner ear, most typically the cochlear hair cells or the VIIIth nerve itself. Although both forms of peripheral hearing loss manifest themselves as a raised threshold for hearing on the affected side, their diagnoses and treatments differ.Conductive hearing loss can be due to occlusion of the ear canal by wax or foreign objects, rupture of the tympanic membrane itself, or arthritic ossification of the middle ear bones. In contrast, sensorineural hearing loss usually is due to congenital or environmental insults that lead to hair cell death or damage to the eighth nerve. As hair cells are relatively few in number and do not regenerate in humans, their depletion leads to a diminished ability to detect sounds. The Weber test, a simple test involving a tuning fork, can be used to distinguish between these two forms of hearing loss. If a resonating tuning fork is placed on the vertex, a patient with conductive hearing loss will report that the sound is louder in the affected ear. In the "plugged" state, sounds propagating through the skull do not dissipate so freely back out through the auditory meatus, and thus a greater amount of sound energy is transmitted to the cochlea on the blocked side. In contrast, a patient with a monaural sensorineural hearing loss will report that a Weber test sounds louder on the intact side, because even though the inner ear may vibrate equally on the two sides, the damaged side cannot transduce this vibration into a neural signal.Treatment also differs for these two types of deafness. An external hearing aid is used to boost sounds to compensate for the reduced efficiency of the conductive apparatus in conductive hearing losses. These miniature devices are inserted in the ear canal, and contain a microphone and speaker, as well as an amplifier. One limitation of hearing aids is that they often provide rather flat amplification curves, which can interfere with listening in noisy environments moreover, they do not achieve a high degree of directionality. The use of digital signal processing strategies partly overcomes these problems, and hearing aids obviously provide significant benefits to many people.The treatment of sensorineural hearing loss is more complicated and invasive conventional hearing aids are useless, because no amount of mechanical amplification can compensate for the inability to generate or convey a neural impulse from the cochlea. However, if the VIIIth nerve is intact, cochlear implants can be used to partially restore hearing. The cochlear implant consists of a peripherally mounted microphone and digital signal processor that transforms a sound into its spectral components, and additional electronics that use this information to activate different combinations of contacts on a threadlike multi-site stimulating electrode array. The electrode is inserted into the cochlea through the round window Cochlear implants can be remarkably effective in restoring hearing to people with hair cell damage, permitting them to engage in spoken communication. Despite such success in treating those who have lost their hearing after having cochlear partition. The cochlear partition does not extend all the way to the apical end of the cochlea instead there is an opening, known as the helicotrema, that joins the scala vestibuli to the scala tympani, allowing their fluid, known as perilymph, to mix. One consequence of this structural arrangement is that inward movement of the oval window displaces the fluid of the inner ear, causing the round window to bulge out slightly and deforming the cochlear partition.The manner in which the basilar membrane vibrates in response to sound is the key to understanding cochlear function. Measurements of the vibra- learned to speak, whether cochlear implants can enable development of spoken language in the congenitally deaf is still a matter of debate. Although cochlear implants cannot help patients with VIIIth nerve damage, brainstem implants are being developed that use a conceptually similar approach to stimulate the cochlear nuclei directly, bypassing the auditory periphery altogether.tion of different parts of the basilar membrane, as well as the discharge rates of individual auditory nerve fibers that terminate along its length, show that both these features are highly tuned that is, they respond most intensely to a sound of a specific frequency. Frequency tuning within the inner ear is attributable in part to the geometry of the basilar membrane, which is wider and more flexible at the apical end and narrower and stiffer at the basal end. One feature of such a system is that regardless of where energy is supplied to it, movement always begins at the stiff end, and then propagates to the more flexible end. Georg von Bksy, working at Harvard University, showed that a membrane that varies systematically in its width and flexibility vibrates maximally at different positions as a function of the stimulus frequency Von Bksys model of cochlear mechanics was a passive one, resting on the premise that the basilar membrane acts like a series of linked resonators, much as a concatenated set of tuning forks. Each point on the basilar membrane was postulated to have a characteristic frequency at which it vibrated most efficiently because it was physically linked to adjacent areas of the membrane, each point also vibrated at other frequencies, thus permitting propagation of the traveling wave. It is now clear, however, that the tuning of the auditory periphery, whether measured at the basilar membrane or recorded as the electrical activity of auditory nerve fibers, is too sharp to be explained by passive mechanics alone. At very low sound intensities, the basilar membrane vibrates one hundred-fold more than would be predicted by linear extrapolation from the motion measured at high intensities. Therefore, the ears sensitivity arises from an active biomechanical process, as well as from its passive resonant properties. The outer hair cells, which together with the inner hair cells comprise the Figure 12.5 Traveling waves along the cochlea. A traveling wave is shown at a given instant along the cochlea, which has been uncoiled for clarity. The graphs on the right profile the amplitude of the traveling wave along the basilar membrane for different frequencies and show that the position where the traveling wave reaches its maximum amplitude varies directly with the frequency of stimulation. The hair cell is an evolutionary triumph that solves the problem of transforming vibrational energy into an electrical signal. The scale at which theAs early as the first half of the eighteenth century, musical composers such as G. Tartini and W. A. Sorge discovered that upon playing pairs of tones, other tones not present in the original stimulus are also heard. These combination tones, fc, are mathematically related to the played tones f 1 and f 2 by the formula fc  mf 1  nf 2 where m and n are positive integers. Combination tones have been used for a variety of compositional effects, as they can strengthen the harmonic texture of a chord. Furthermore, organ builders sometimes use the difference tone created by two smaller organ pipes to produce the extremely low tones that would otherwise require building one especially large pipe.Modern experiments suggest that this distortion product is due at least in part to the nonlinear properties of the inner ear. M. Ruggero and his colleagues placed small glass beads on the basilar membrane of an anesthetized animal and then determined the velocity of the basilar membrane in response to different combinations of tones by measuring the Doppler shift of laser light reflected from the beads. When two tones were played into the ear, the basilar membrane vibrated not only at those two frequencies, but also at other frequencies predicted by the above formula.Related experiments on hair cells studied in vitro suggest that these nonlinearities result from the properties of the mechanical linkage of the transduction apparatus. By moving the hair bundle sinusoidally with a metal-coated glass fiber, A. J. Hudspeth and his coworkers found that the hair bundle exerts a force at the same frequency. However, when two sinusoids were applied simultaneously, the forces exerted by the hair bundle occurred not only at the primary frequencies, but at several combination frequencies as well. These distortion products are due to the transduction apparatus, since blocking the transduction channels causes the forces exerted at the combination frequencies to disappear, even though the forces at the primary frequencies remain unaffected. It seems that the tip links add a certain extra springiness to the hair bundle in the small range of motions over which the transduction channels are changing between closed and open states. If nonlinear distortions of basilar membrane vibrations arise from the properties of the hair bundle, then it is likely that hair cells can indeed influence basilar membrane motion, thereby accounting for the cochleas extreme sensitivity. When we hear difference tones, we may be paying the price in distortion for an exquisitely fast and sensitive transduction mechanism.   The hair cell is a flask-shaped epithelial cell named for the bundle of hairlike processes that protrude from its apical end into the scala media. Each hair bundle contains anywhere from 30 to a few hundred hexagonally arranged stereocilia, with one taller kinocilium  Receptor potentials generated by an individual hair cell in the cochlea in response to pure tones. Note that the hair cell potential faithfully follows the waveform of the stimulating sinusoids for low frequencies, but still responds with a DC offset to higher frequencies. The cochlear hair cells in humans consist of one row of inner hair cells and three rows of outer hair cells A clue to the significance of this efferent pathway was provided by the discovery that an active process within the cochlea, as mentioned, influences basilar membrane motion. First, it was found that the cochlea actually emits sound under certain conditions. These otoacoustical emissions can be detected by placing a sensitive microphone at the eardrum and monitoring the response after briefly presenting a tone or click, and provide a useful means to assess cochlear function in the newborn. Such emissions can also occur spontaneously, especially in certain pathological states, and are thus one potential source of tinnitus. These observations clearly indicate that a process within the cochlea is capable of producing sound. Second, stimulation of the crossed olivocochlear bundle, which supplies efferent input to the outer hair cells, can broaden VIIIth nerve tuning curves. Third, the high sensitivity notch of VIIIth nerve tuning curves is lost when the outer hair cells are selectively inactivated. Finally, isolated outer hair cells contract and expand in response to small electrical currents, thus providing a potential source of energy to drive an active process within the cochlea. Thus, it seems likely that the outer hair cells sharpen the frequency-resolving power of the cochlea by actively contracting and relaxing, thus changing the stiffness of the tectorial membrane at particular locations. This active process explains the nonlinear vibration of the basilar membrane at low sound intensities.The rapid response time of the transduction apparatus allows the membrane potential of the hair cell to follow deflections of the hair bundle up to moderately high frequencies of oscillation. In humans, the receptor potentials of certain hair cells and the action potentials of their associated auditory nerve fiber can follow stimuli of up to about 3 kHz in a one-to-one fashion. Such real-time encoding of stimulus frequency by the pattern of action potentials in the auditory nerve is known as the "volley theory" of auditory information transfer. Even these extraordinarily rapid processes, however, fail to follow frequencies above 3 kHz The other prominent feature of hair cells-their ability to follow the waveform of low-frequency sounds-is also important in other more subtle aspects of auditory coding. As mentioned earlier, hair cells have biphasic response properties. Because hair cells release transmitter only when depolarized, auditory nerve fibers fire only during the positive phases of low-frequency sounds A hallmark of the ascending auditory system is its parallel organization. This arrangement becomes evident as soon as the auditory nerve enters the brainstem, where it branches to innervate the three divisions of the cochlear nucleus. The auditory nerve comprises the central processes of the bipolar spiral ganglion cells in the cochlea Just as the auditory nerve branches to innervate several different targets in the cochlear nuclei, the neurons in these nuclei give rise to several different pathways The human ability to detect interaural time differences is remarkable. The longest interaural time differences, which are produced by sounds arising directly lateral to one ear, are on the order of only 700 microseconds. Psychophysical experiments show that humans can actually detect interaural time differences as small as 10 microseconds two sounds presented through earphones separated by such small interaural time differences are perceived as arising from the side of the leading ear. This sensitivity translates into accuracy for sound localization of about 1 degree.  How is timing in the 10 microseconds range accomplished by neural components that operate in the millisecond range The neural circuitry that computes such tiny interaural time differences consists of binaural inputs to the medial superior olive that arise from the right and left anteroventral cochlear nuclei Sound localization perceived on the basis of interaural time differences requires phase-locked information from the periphery, which, as already A given MSO neuron responds most strongly when the two inputs arrive simultaneously, as occurs when the contralateral and ipsilateral inputs precisely compensate for differences in the time of arrival of a sound at the two ears. The systematic variation in the delay lengths of the two inputs creates a map of sound location: In this model, E would be most sensitive to sounds located to the left, and A to sounds from the right C would respond best to sounds coming from directly in front of the listener. results in a net excitation of the LSO on the same side of the body as the sound source. For sounds arising directly lateral to the listener, firing rates will be highest in the LSO on that side in this circumstance, the excitation via the ipsilateral anteroventral cochlear nucleus will be maximal, and inhibition from the contralateral MNTB minimal. In contrast, sounds arising closer to the listeners midline will elicit lower firing rates in the ipsilateral LSO because of increased inhibition arising from the contralateral MNTB.For sounds arising at the midline, or from the other side, the increased inhibition arising from the MNTB is powerful enough to completely silence LSO activity. Note that each LSO only encodes sounds arising in the ipsilateral hemifield it therefore takes both LSOs to represent the full range of horizontal positions. In summary, there are two separate pathways-and two separate mechanisms-for localizing sound. Interaural time differences are processed in the medial superior olive, and interaural intensity differences are processed in the lateral superior olive. These two pathways are eventually merged in the midbrain auditory centers.The binaural pathways for sound localization are only part of the output of the cochlear nucleus. This fact is hardly surprising, given that auditory perception involves much more than locating the position of the sound source. A second major set of pathways from the cochlear nucleus bypasses the superior olive and terminates in the nuclei of the lateral lemniscus on the contralateral side of the brainstem Auditory pathways ascending via the olivary and lemniscal complexes, as well as other projections that arise directly from the cochlear nucleus, project to the midbrain auditory center, the inferior colliculus. In examining how integration occurs in the inferior colliculus, it is again instructive to turn to the most completely analyzed auditory mechanism, the binaural system for localizing sound. As already noted, space is not mapped on the auditory receptor surface thus the perception of auditory space must somehow be synthesized by circuitry in the lower brainstem and midbrain. Experiments in the barn owl, an extraordinarily proficient animal at localizing sounds, show that the convergence of binaural inputs in the midbrain produces something entirely new relative to the periphery-namely, a computed topographical representation of auditory space. Neurons within this auditory space map in the colliculus respond best to sounds originating in a specific region of space and thus have both a preferred elevation and a preferred horizontal location, or azimuth. Although comparable maps of auditory space have not yet been found in mammals, humans have a clear perception of both the elevational and azimuthal components of a sounds location, suggesting that we have a similar auditory space map.Another important property of the inferior colliculus is its ability to process sounds with complex temporal patterns. Many neurons in the inferior colliculus respond only to frequency-modulated sounds, while others respond only to sounds of specific durations. Such sounds are typical components of biologically relevant sounds, such as those made by predators, or intraspecific communication sounds, which in humans include speech.Despite the parallel pathways in the auditory stations of the brainstem and midbrain, the medial geniculate complex in the thalamus is an obligatory relay for all ascending auditory information destined for the cortex In some mammals, the strictly maintained tonotopy of the lower brainstem areas is exploited by convergence onto MGC neurons, generating specific responses to certain spectral combinations. The original evidence for this statement came from research on the response properties of cells in the MGC of echolocating bats. Some cells in the so-called belt areas of the bat MGC respond only to combinations of widely spaced frequencies that are specific components of the bats echolocation signal and of the echoes that are reflected from objects in the bats environment. In the mustached bat, where this phenomenon has been most thoroughly studied, the echolocation pulse has a changing frequency component that includes a fundamental frequency and one or more harmonics. The fundamental frequency has low intensity and sweeps from 30 kHz to 20 kHz. The second harmonic is the most intense component and sweeps from 60 kHz to 40 kHz. Note that these frequencies do not overlap. Most of the echoes are from the intense FM 2 sound, and virtually none arise from the weak FM 1 , even though the emitted FM 1 is loud enough for the bat to hear. Apparently, the bat assesses the distance to an object by measuring the delay between the FM 1 emission and the FM 2 echo. Certain MGC neurons respond when FM 2 follows FM 1 by a specific delay, providing a mechanism for sensing such frequency combinations. Because each neuron responds best to a particular delay, the population of MGC neurons encodes a range of distances.Bat sonar illustrates two important points about the function of the auditory thalamus. First, the MGC is the first station in the auditory pathway where selectivity for combinations of frequencies is found. The mechanism responsible for this selectivity is presumably the ultimate convergence of inputs from cochlear areas with different spectral sensitivities. Second, cells in the MGC are selective not only for frequency combinations, but also for specific time intervals between the two frequencies. The principle is the same as that described for binaural neurons in the medial superior olive, but in this instance, two monaural signals with different frequency sensitivity coincide, and the time difference is in the millisecond rather than the microsecond range.In summary, neurons in the medial geniculate complex receive convergent inputs from spectrally and temporally separate pathways. This complex, by virtue of its convergent inputs, mediates the detection of specific spectral and temporal combinations of sounds. In many species, including humans, varying spectral and temporal cues are especially important features of communication sounds. It is not known whether cells in the human medial geniculate are selective to combinations of sounds, but the processing of speech certainly requires both spectral and temporal combination sensitivity.The ultimate target of afferent auditory information is the auditory cortex. Although the auditory cortex has a number of subdivisions, a broad distinction can be made between a primary area and peripheral, or belt, areas. The primary auditory cortex is located on the superior temporal gyrus in the temporal lobe and receives point-to-point input from the ventral division of the medial geniculate complex thus, it contains a precise tonotopic map. The belt areas of the auditory cortex receive more diffuse input from the belt areas of the medial geniculate complex and therefore are less precise in their tonotopic organization.The primary auditory cortex has a topographical map of the cochlea Most natural sounds are complex, meaning that they differ from the pure tones or clicks that are frequently used in neurophysiological studies of the auditory system. Rather, natural sounds are tonal: they have a fundamental frequency that largely determines the "pitch" of the sound, and one or more harmonics of different intensities that contribute to the quality or "timbre" of a sound. The frequency of a harmonic is, by definition, a multiple of the fundamental frequency, and both may be modulated over time.Such frequency-modulated sweeps can rise or fall in frequency, or change in a sinusoidal or some other fashion. Occasionally, multiple nonharmonic frequencies may be simultaneously present in some communication or musical sounds. In some sounds, a level of spectral splatter or "broadband noise" is embedded within tonal or frequency modulated sounds. The variations in the sound spectrum are typically accompanied by a modulation of the amplitude envelop of the complex sound as well. All of these features can be visualized by performing a spectrographic analysis. How does the brain represent such complex natural sounds Cognitive studies of complex sound perception provide some understanding of how a large but limited number of neurons in the brain can dynamically represent an infinite variety of natural stimuli in the sensory environment of humans and other animals. In bats, specializations for processing complex sounds are apparent. Studies in echolocating bats show that both communication and echolocation sounds Asymmetrical representation is another common principle of complex sound processing that results in lateralized representations of natural stimuli. Thus, speech sounds that are important for communication are lateralized to the left in the belt regions of the auditory cortex, whereas environmental sounds that are important for reacting to and recogniz- The auditory cortex obviously does much more than provide a tonotopic map and respond differentially to ipsi-and contralateral stimulation. Although the sorts of sensory processing that occur in the auditory cortex are not well understood, they are likely to be important to higher-order processing of natural sounds, especially those used for communication. One clue about such processing comes from work in marmosets, a small neotropical primate with a complex vocal repertoire. The primary auditory cortex and belt areas of these animals are indeed organized tonotopically, but also contain neurons that are strongly responsive to spectral combinations that characterize certain vocalizations. The responsesThe Auditor y System 311 ing aspects of the auditory environment are represented in each hemisphere Many of the dually specialized neurons are categorized as "combinationsensitive" neurons, i.e., neurons that show a nonlinear increase in their response magnitude when presented with a combination of tones andor noise bands in comparison to the total magnitude of the response elicited by presenting each sound element separately. Combination-sensitive neurons are tuned to more than one frequency and are specialized to recognize complex species-specific sounds and extract information that is critical for survival. This sensitivity to combinations of simple sound elements appears to be a universal property of neurons for the perception of complex sounds by many animal species, such as frogs, birds bats and nonhuman primates. Therefore, combination-sensitive neurons most likely partake in the recognition of complex sounds in the human auditory cortex as well.Sounds that are especially important for intraspecific communication often have a highly ordered temporal structure. In humans, the best example of such time-varying signals is speech, where different phonetic sequences are perceived as distinct syllables and words. Behavioral studies in cats and monkeys show that the auditory cortex is especially important for processing temporal sequences of sound. If the auditory cortex is ablated in these animals, they lose the ability to discriminate between two complex sounds that have the same frequency components but which differ in temporal sequence. Thus, without the auditory cortex, monkeys cannot discriminate one conspecific communication sound from another. The physiological basis of such temporal sensitivity likely requires neurons that are sensitive to time-varying cues in communication sounds. Indeed, electrophysiological recordings from the primary auditory cortex of both marmosets and bats show that some neurons that respond to intraspecific communication sounds do not respond as strongly when the sounds are played in reverse, indicating sensitivity to the sounds temporal features. Studies of human patients with bilateral damage to the auditory cortex also reveal severe problems in processing the temporal order of sounds. It seems likely, therefore, that specific regions of the human auditory cortex are specialized for processing elementary speech sounds, as well as other temporally complex acoustical signals, such as music. Thus, Wernickes area, which is critical to the comprehension of human language, lies within the secondary auditory area Sound waves are transmitted via the external and middle ear to the cochlea of the inner ear, which exhibits a traveling wave when stimulated. For highfrequency sounds, the amplitude of the traveling wave reaches a maximum at the base of the cochlea for low-frequency sounds, the traveling wave reaches a maximum at the apical end. The associated motions of the basilar membrane are transduced primarily by the inner hair cells, while the basilar membrane motion is itself actively modulated by the outer hair cells. Damage to the outer or middle ear results in conductive hearing loss, while hair cell damage results in a sensorineural hearing deficit. The tonotopic organization of the cochlea is retained at all levels of the central auditory system. Projections from the cochlea travel via the eighth nerve to the three main divisions of the cochlear nucleus. The targets of the cochlear nucleus neurons include the superior olivary complex and nuclei of the lateral lemniscus, where the binaural cues for sound localization are processed. The inferior colliculus is the target of nearly all of the auditory pathways in the lower brainstem and carries out important integrative functions, such as processing sound frequencies and integrating the cues for localizing sound in space. The primary auditory cortex, which is also organized tonotopically, is essential for basic auditory functions, such as frequency discrimination and sound localization, and also plays an important role in processing of intraspecific communication sounds. The belt areas of the auditory cortex have a less strict tonotopic organization and also process complex sounds, such as those that mediate communication. In the human brain, the major speech comprehension areas are located in the zone immediately adjacent to the auditory cortex.The Auditor y System 313The vestibular system has important sensory functions, contributing to the perception of self-motion, head position, and spatial orientation relative to gravity. It also serves important motor functions, helping to stabilize gaze, head, and posture. The peripheral portion of the vestibular system includes inner ear structures that function as miniaturized accelerometers and inertial guidance devices, continually reporting information about the motions and position of the head and body to integrative centers in the brainstem, cerebellum, and somatic sensory cortices. The central portion of the system includes the vestibular nuclei, which make extensive connections with brainstem and cerebellar structures. The vestibular nuclei also directly innervate motor neurons controlling extraocular, cervical, and postural muscles. This motor output is especially important to stabilization of gaze, head orientation, and posture during movement. Although we are normally unaware of its functioning, the vestibular system is a key component in postural reflexes and eye movements. Balance, gaze stabilization during head movement, and sense of orientation in space are all adversely affected if the system is damaged. These manifestations of vestibular damage are especially important in the evaluation of brainstem injury. Because the circuitry of the vestibular system extends through a large part of the brainstem, simple clinical tests of vestibular function can be performed to determine brainstem involvement, even on comatose patients.The main peripheral component of the vestibular system is an elaborate set of interconnected chambers-the labyrinth-that has much in common, and is in fact continuous with, the cochlea. Like the cochlea, the labyrinth is derived from the otic placode of the embryo, and it uses the same specialized set of sensory cells-hair cells-to transduce physical motion into neural impulses. In the cochlea, the motion is due to airborne sounds in the labyrinth, the motions transduced arise from head movements, inertial effects due to gravity, and ground-borne vibrations.The labyrinth is buried deep in the temporal bone and consists of the two otolith organs and three semicircular canals The intimate relationship between the cochlea and the labyrinth goes beyond their common embryonic origin. Indeed, the cochlear and vestibular spaces are actually joined The vestibular hair cells are located in the utricle and saccule and in three juglike swellings called ampullae, located at the base of the semicircular canals next to the utricle. Within each ampulla, the vestibular hair cells extend their hair bundles into the endolymph of the membranous labyrinth. As in the cochlea, tight junctions seal the apical surfaces of the vestibular hair cells, ensuring that endolymph selectively bathes the hair cell bundle while remaining separate from the perilymph surrounding the basal portion of the hair cell.The vestibular hair cells, which like cochlear hair cells transduce minute displacements into behaviorally relevant receptor potentials, provide the basis for vestibular function. Vestibular and auditory hair cells are quite similar a detailed description of hair cell structure and function has already been given in Chapter 12. As in the case of auditory hair cells, movement of the stereocilia toward the kinocilium in the vestibular end organs opens mechanically gated transduction channels located at the tips of the stereocilia, depolarizing the hair cell and causing neurotransmitter release onto the vestibular nerve fibers. Movement of the stereocilia in the direction away from the kinocilium closes the channels, hyperpolarizing the hair cell and thus reducing vestibular nerve activity. The biphasic nature of the receptor potential means that some transduction channels are open in the absence of stimulation, with the result that hair cells tonically release transmitter, thereby generating considerable spontaneous activity in vestibular nerve fibers Displacements and linear accelerations of the head, such as those induced by tilting or translational movements, are detected by the two otolith organs: the saccule and the utricle. Both of these organs contain a sensory epithelium, the macula, which consists of hair cells and associated supporting cells. Overlying the hair cells and their hair bundles is a gelatinous layer above this layer is a fibrous structure, the otolithic membrane, in which are embedded crystals of calcium carbonate called otoconia As already mentioned, the orientation of the hair cell bundles is organized relative to the striola, which demarcates the overlying layer of otoco- The function of the vestibular system can be simplified by remembering some basic terminology of classical mechanics. All bodies moving in a three-dimensional framework have six degrees of freedom: three of these are translational and three are rotational. The translational elements refer to linear movements in the x, y, and z axes. Translational motion in these planes is the primary concern of the otolith organs. The three degrees of rotational freedom refer to a bodys rotation relative to the x, y, and z axes and are commonly referred to as roll, pitch, and yaw. The semicircular canals are primarily responsible for sensing rotational accelerations around these three axes.  The minuscule movement of the hair bundle at sensory threshold has been compared to the displacement of the top of the Eiffel Tower by a thumbs breadth Despite its great sensitivity, the hair cell can adapt quickly and continuously to static displacements of the hair bundle caused by large movements. Such adjustments are especially useful in the otolith organs, where adaptation permits hair cells to maintain sensitivity to small linear and angular accelerations of the head despite the constant input from gravitational forces that are over a million times greater. In other receptor cells, such as photoreceptors, adaptation is accomplished by regulating the second messenger cascade induced by the initial transduction event. The hair cell has to depend on a different strategy, however, because there is no second messenger system between the initial transduction event and the subsequent receptor potential. Adaptation occurs in both directions in which the hair bundle displacement generates a receptor potential, albeit at different rates for each direction. When the hair bundle is pushed toward the kinocilium, tension is initially increased in the gating spring. During adaptation, tension decreases back to the resting level, perhaps because one end of the gating spring repositions itself along the shank of the stereocilium. When the hair bundle is displaced in the opposite direction, away from the kinocilium, tension in the spring initially decreases adaptation then involves an increase in spring tension. One theory is that a calcium-regulated motor such as a myosin ATPase climbs along actin filaments in the stereocilium and actively resets the tension in the transduction spring. During sustained depolarization, some Ca 2 enters through the transduction channel, along with K  . Ca 2 then causes the motor to spend a greater fraction of its time unbound from the actin, resulting in slippage of the spring down the side of the stereocilium. During sustained hyperpolarization Although mechanical tuning plays an important role in generating frequency selectivity in the cochlea, there are other mechanisms that contribute to this process in vestibular and auditory nerve cells. These other tuning mechanisms are especially important in the otolith organs, where, unlike the cochlea, there are no  Although a hair cell responds to hair bundle movement over a wide range of frequencies, the resultant receptor potential is largest at the frequency of electrical resonance. The resonance frequency represents the characteristic frequency of the hair cell, and transduction at that frequency will be most efficient. This electrical resonance has important implications for structures like the utricle and sacculus, which may encode a range of characteristic frequencies based on the different resonance frequencies of their constituent hair cells. Thus, electrical tuning in the otolith organs can generate enhanced tuning to biologically relevant frequencies of stimulation, even in the absence of macromechanical resonances within these structures. The structure of the otolith organs enables them to sense both static displacements, as would be caused by tilting the head relative to the gravitational axis, and transient displacements caused by translational movements of the head. The mass of the otolithic membrane relative to the surrounding endolymph, as well as the otolithic membranes physical uncoupling from the underlying macula, means that hair bundle displacement will occur transiently in response to linear accelerations and tonically in response to tilting of the head. Therefore, both tonic and transient information can be conveyed by these sense organs. For each of the positions and accelerations due to translational movements, some set of hair cells will be maximally excited, whereas another set will be maximally inhibited. Note that head tilts produce displacements similar to certain accelerations. steady and relatively high firing rate when the head is upright. The change in firing rate in response to a given movement can be either sustained or transient, thereby signaling either absolute head position or linear acceleration. An example of the sustained response of a vestibular nerve fiber innervating the utricle is shown in Whereas the otolith organs are primarily concerned with head translations and orientation with respect to gravity, the semicircular canals sense head rotations, arising either from self-induced movements or from angular accelerations of the head imparted by external forces. Each of the three semicircular canals has at its base a bulbous expansion called the ampulla Each semicircular canal works in concert with the partner located on the other side of the head that has its hair cells aligned oppositely. There are three such pairs: the two pairs of horizontal canals, and the superior canal on each side working with the posterior canal on the other side For example, when the head accelerates to the left, the cupula is pushed toward the kinocilium in the left horizontal canal, and the firing rate of the relevant axons in the left vestibular nerve increases. In contrast, the cupula in the right horizontal canal is pushed away from the kinocilium, with a concomitant decrease in the firing rate of the related neurons. If the head movement is to the right, the result is just the opposite. This push-pull arrangement operates for all three pairs of canals the pair whose activity is modulated is in the plane of the rotation, and the member of the pair whose activity is increased is on the side toward which the head is turning. The net result is a system that provides information about the rotation of the head in any direction.Like axons that innervate the otolith organs, the vestibular fibers that innervate the semicircular canals exhibit a high level of spontaneous activity. As a result, they can transmit information by either increasing or decreasing their firing rate, thus more effectively encoding head movements. The bidirectional responses of fibers innervating the hair cells of the semicircular canal have been studied by recording the axonal firing rates in a monkeys Testing the integrity of the vestibular system can indicate much about the condition of the brainstem, particularly in comatose patients.Normally, when the head is not being rotated, the output of the nerves from the right and left sides are equal thus, no eye movements occur. When the head is rotated in the horizontal plane, the vestibular afferent fibers on the side toward the turning motion increase their firing rate, while the afferents on the opposite side decrease their firing rate Pathological nystagmus can occur if there is unilateral damage to the vestibular system. In this case, the silencing of the spontaneous output from the damaged side results in an unphysiological difference in firing rate because the spontaneous discharge from the intact side remains Responses to vestibular stimulation are thus useful in assessing the integrity of the brainstem in unconscious patients. If the individual is placed on his or her back and the head is elevated to about 30above horizontal, the horizontal semicircular canals lie in an almost vertical orientation. Irrigating one ear with cold water will then lead to spontaneous eye movements because convection currents in the canal mimic rotatory head movements away from the irrigated ear  Brainstem intact vestibular nerve. Seated in a chair, the monkey was rotated continuously in one direction during three phases: an initial period of acceleration, then a periord of several seconds at constant velocity, and finally a period of sudden deceleration to a stop The vestibular end organs communicate via the vestibular branch of cranial nerve VIII with targets in the brainstem and the cerebellum that process much of the information necessary to compute head position and motion. As with the cochlear nerve, the vestibular nerves arise from a population of bipolar neurons, the cell bodies of which in this instance reside in the vestibular nerve ganglion The central projections of the vestibular system participate in three major classes of reflexes: helping to maintain equilibrium and gaze during movement, maintaining posture, and maintaining muscle tone. The first of these reflexes helps coordinate head and eye movements to keep gaze fixated on objects of interest during movements. The vestibulo-ocular reflex in particular is a mechanism for producing eye movements that counter head movements, thus permitting the gaze to remain fixed on a particular point. For example, activity in the left horizontal canal induced by leftward rotary acceleration of the head excites neurons in the left vestibular nucleus and results in compensatory eye movements to the right. This effect is due to excitatory projections from the vestibular nucleus to the contralateral nucleus abducens that, along with the oculomotor nucleus, help execute conjugate eye movements.For instance, horizontal movement of the two eyes toward the right requires contraction of the left medial and right lateral rectus muscles. Vestibular nerve fibers originating in the left horizontal semicircular canal project to the medial and lateral vestibular nuclei the linear vertical head oscillations that accompany locomotion, and in response to vertical angular accelerations of the head, as can occur when riding on a swing. The rostrocaudal set of cranial nerve nuclei involved in the VOR, as well as the VORs persistence in the unconscious state, make this reflex especially useful for detecting brainstem damage in the comatose patient.Loss of the VOR can have severe consequences. A patient with vestibular damage finds it difficult or impossible to fixate on visual targets while the head is moving, a condition called oscillopsia Descending projections from the vestibular nuclei are essential for postural adjustments of the head, mediated by the vestibulo-cervical reflex, and body, mediated by the vestibulo-spinal reflex. As with the VOR, these postural reflexes are extremely fast, in part due to the small number of synapses interposed between the vestibular organ and the relevant motor neurons. Like the VOR, the VCR and the VSR are both compromised in patients with bilateral vestibular damage. Such patients exhibit diminished head and postural stability, resulting in gait deviations they also have difficulty balancing. These balance defects become more pronounced in low light or while walking on uneven surfaces, indicating that balance normally is the product of vestibular, visual, and proprioceptive inputs.The anatomical substrate for the VCR involves the medial vestibular nucleus, axons from which descend in the medial longitudinal fasciculus to reach the upper cervical levels of the spinal cord The VSR is mediated by a combination of pathways, including the lateral and medial vestibulospinal tracts and the reticulospinal tract. The inputs from the otolith organs project mainly to the lateral vestibular nucleus, which in turn sends axons in the lateral vestibulospinal tract to the spinal cord Decerebrate rigidity, characterized by rigid extension of the limbs, arises when the brainstem is transected above the level of the vestibular nucleus. Decerebrate rigidity in experimental animals is relieved when the vestibular nuclei are lesioned, underscoring the importance of the vestibular system to the maintenance of muscle tone. The tonic activation of extensor muscles in